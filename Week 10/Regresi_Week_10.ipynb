{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "haSAPuYdWUZf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4CNoJf5YFhD",
        "outputId": "ec4e1f05-4b3c-4281-e51e-1a2b56c73052"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/student-mat.csv', sep=';')\n",
        "\n",
        "# Preprocessing: Select numerical columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=categorical_columns)  # One-hot encoding for categorical columns\n",
        "\n",
        "# Feature-target split\n",
        "X = df.drop(columns='age')\n",
        "y = df['age']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n"
      ],
      "metadata": {
        "id": "2odz16JlYaTu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers, activation_function):\n",
        "        super(MLPRegressor, self).__init__()\n",
        "\n",
        "        # Create layers dynamically based on hidden_layers\n",
        "        layers = []\n",
        "        for i, neurons in enumerate(hidden_layers):\n",
        "            if i == 0:\n",
        "                layers.append(nn.Linear(input_dim, neurons))\n",
        "            else:\n",
        "                layers.append(nn.Linear(hidden_layers[i - 1], neurons))\n",
        "\n",
        "            # Add activation function\n",
        "            if activation_function == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation_function == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "            elif activation_function == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            elif activation_function == 'softmax':\n",
        "                layers.append(nn.Softmax(dim=1))\n",
        "            elif activation_function == 'linear':\n",
        "                pass  # Linear activation, no additional layer needed\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(hidden_layers[-1], 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "adYabN1CY_22"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, criterion, train_loader, test_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on test set every epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(X_test)\n",
        "            test_loss = criterion(y_pred, y_test)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "-MOeJ1JbZXwh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_layers_options = [[4], [8, 4], [16, 8, 4]]  # Example: 1, 2, 3 layers with varying neurons\n",
        "activation_functions = ['relu', 'sigmoid', 'tanh', 'softmax', 'linear']\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
        "epochs_options = [1, 10, 25, 50, 100, 250]\n",
        "\n",
        "# DataLoader preparation\n",
        "def create_data_loaders(X, y, batch_size):\n",
        "    dataset = torch.utils.data.TensorDataset(X, y)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Grid search over hyperparameters\n",
        "results = []\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "for hidden_layers, activation_function, lr, batch_size, epochs in itertools.product(\n",
        "    hidden_layers_options, activation_functions, learning_rates, batch_sizes, epochs_options\n",
        "):\n",
        "    print(f\"Training with {hidden_layers}, {activation_function}, lr={lr}, batch_size={batch_size}, epochs={epochs}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = create_data_loaders(X_train, y_train, batch_size)\n",
        "    test_loader = create_data_loaders(X_test, y_test, batch_size)\n",
        "\n",
        "    # Model, loss function, optimizer\n",
        "    model = MLPRegressor(input_dim, hidden_layers, activation_function)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(model, optimizer, criterion, train_loader, test_loader, epochs)\n",
        "\n",
        "    # Evaluate final test performance\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test).numpy()\n",
        "        mse = mean_squared_error(y_test.numpy(), y_pred)\n",
        "\n",
        "    results.append((hidden_layers, activation_function, lr, batch_size, epochs, mse))\n",
        "    print(f\"Final MSE: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV9hktfDZa88",
        "outputId": "f39edd28-8f55-4146-bfa3-6f345fb124af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 180/250, Loss: 1.7129100561141968, Test Loss: 1.0140324831008911\n",
            "Epoch 181/250, Loss: 1.2564829587936401, Test Loss: 0.9146613478660583\n",
            "Epoch 182/250, Loss: 2.1281630992889404, Test Loss: 1.0767916440963745\n",
            "Epoch 183/250, Loss: 3.0795631408691406, Test Loss: 0.9209849834442139\n",
            "Epoch 184/250, Loss: 0.9669082164764404, Test Loss: 1.1145832538604736\n",
            "Epoch 185/250, Loss: 1.436966061592102, Test Loss: 0.9538756012916565\n",
            "Epoch 186/250, Loss: 1.1974660158157349, Test Loss: 1.1362926959991455\n",
            "Epoch 187/250, Loss: 1.277437686920166, Test Loss: 1.0127644538879395\n",
            "Epoch 188/250, Loss: 2.083723783493042, Test Loss: 0.9554153680801392\n",
            "Epoch 189/250, Loss: 2.2175049781799316, Test Loss: 0.9436478614807129\n",
            "Epoch 190/250, Loss: 1.4610862731933594, Test Loss: 0.9570813775062561\n",
            "Epoch 191/250, Loss: 0.94575434923172, Test Loss: 1.06064772605896\n",
            "Epoch 192/250, Loss: 1.7930493354797363, Test Loss: 0.9444460272789001\n",
            "Epoch 193/250, Loss: 0.9834725856781006, Test Loss: 1.0269451141357422\n",
            "Epoch 194/250, Loss: 0.9124584794044495, Test Loss: 0.9052855968475342\n",
            "Epoch 195/250, Loss: 1.9013205766677856, Test Loss: 1.007524847984314\n",
            "Epoch 196/250, Loss: 0.806742250919342, Test Loss: 0.9020679593086243\n",
            "Epoch 197/250, Loss: 0.7186029553413391, Test Loss: 0.9771795868873596\n",
            "Epoch 198/250, Loss: 0.9178571105003357, Test Loss: 1.0306298732757568\n",
            "Epoch 199/250, Loss: 1.5866867303848267, Test Loss: 0.9437503218650818\n",
            "Epoch 200/250, Loss: 1.4388293027877808, Test Loss: 0.8987350463867188\n",
            "Epoch 201/250, Loss: 1.1803098917007446, Test Loss: 0.9942263960838318\n",
            "Epoch 202/250, Loss: 1.3367033004760742, Test Loss: 0.916758120059967\n",
            "Epoch 203/250, Loss: 0.3957550525665283, Test Loss: 0.9124759435653687\n",
            "Epoch 204/250, Loss: 1.2634743452072144, Test Loss: 1.01789391040802\n",
            "Epoch 205/250, Loss: 0.3942948281764984, Test Loss: 0.8796939253807068\n",
            "Epoch 206/250, Loss: 1.9183372259140015, Test Loss: 0.9235138893127441\n",
            "Epoch 207/250, Loss: 0.7518496513366699, Test Loss: 0.9740132093429565\n",
            "Epoch 208/250, Loss: 0.8376875519752502, Test Loss: 0.9023529291152954\n",
            "Epoch 209/250, Loss: 1.343632698059082, Test Loss: 1.0365629196166992\n",
            "Epoch 210/250, Loss: 1.3275790214538574, Test Loss: 0.9358136057853699\n",
            "Epoch 211/250, Loss: 0.5993558168411255, Test Loss: 0.9619400501251221\n",
            "Epoch 212/250, Loss: 0.7717125415802002, Test Loss: 0.9559798240661621\n",
            "Epoch 213/250, Loss: 2.0985324382781982, Test Loss: 0.9239274263381958\n",
            "Epoch 214/250, Loss: 1.5012507438659668, Test Loss: 0.9733155369758606\n",
            "Epoch 215/250, Loss: 0.8548347353935242, Test Loss: 0.9882628917694092\n",
            "Epoch 216/250, Loss: 1.9674859046936035, Test Loss: 0.9527905583381653\n",
            "Epoch 217/250, Loss: 0.9073153138160706, Test Loss: 0.9667498469352722\n",
            "Epoch 218/250, Loss: 1.2457354068756104, Test Loss: 0.9658578634262085\n",
            "Epoch 219/250, Loss: 1.7623621225357056, Test Loss: 0.9697248339653015\n",
            "Epoch 220/250, Loss: 0.6446542143821716, Test Loss: 0.9064209461212158\n",
            "Epoch 221/250, Loss: 1.7426623106002808, Test Loss: 0.9441747665405273\n",
            "Epoch 222/250, Loss: 1.5100880861282349, Test Loss: 0.9742451906204224\n",
            "Epoch 223/250, Loss: 1.241048812866211, Test Loss: 0.9900020956993103\n",
            "Epoch 224/250, Loss: 0.46472811698913574, Test Loss: 0.9935294985771179\n",
            "Epoch 225/250, Loss: 0.6409453749656677, Test Loss: 0.9207542538642883\n",
            "Epoch 226/250, Loss: 0.714918315410614, Test Loss: 1.014731764793396\n",
            "Epoch 227/250, Loss: 0.6376972794532776, Test Loss: 0.8947118520736694\n",
            "Epoch 228/250, Loss: 0.7825806736946106, Test Loss: 1.0653728246688843\n",
            "Epoch 229/250, Loss: 1.0218106508255005, Test Loss: 0.9015060663223267\n",
            "Epoch 230/250, Loss: 1.3029061555862427, Test Loss: 0.9601960182189941\n",
            "Epoch 231/250, Loss: 1.752597689628601, Test Loss: 0.9575989246368408\n",
            "Epoch 232/250, Loss: 1.2256262302398682, Test Loss: 1.0014418363571167\n",
            "Epoch 233/250, Loss: 0.8638433814048767, Test Loss: 1.0066553354263306\n",
            "Epoch 234/250, Loss: 1.053534746170044, Test Loss: 0.9489802122116089\n",
            "Epoch 235/250, Loss: 0.9016475677490234, Test Loss: 0.9775169491767883\n",
            "Epoch 236/250, Loss: 1.756911277770996, Test Loss: 0.8984537124633789\n",
            "Epoch 237/250, Loss: 1.00576913356781, Test Loss: 1.0573716163635254\n",
            "Epoch 238/250, Loss: 1.0107102394104004, Test Loss: 0.9210228323936462\n",
            "Epoch 239/250, Loss: 1.2908411026000977, Test Loss: 0.9376195669174194\n",
            "Epoch 240/250, Loss: 1.4542862176895142, Test Loss: 0.9353496432304382\n",
            "Epoch 241/250, Loss: 1.1019648313522339, Test Loss: 0.9434444308280945\n",
            "Epoch 242/250, Loss: 1.3752776384353638, Test Loss: 0.9225035309791565\n",
            "Epoch 243/250, Loss: 0.5989182591438293, Test Loss: 0.9382110834121704\n",
            "Epoch 244/250, Loss: 1.1066880226135254, Test Loss: 0.9660333395004272\n",
            "Epoch 245/250, Loss: 2.0285799503326416, Test Loss: 0.9548009634017944\n",
            "Epoch 246/250, Loss: 1.1959130764007568, Test Loss: 0.9485722780227661\n",
            "Epoch 247/250, Loss: 1.6093682050704956, Test Loss: 1.0386590957641602\n",
            "Epoch 248/250, Loss: 0.8480589985847473, Test Loss: 0.9451054334640503\n",
            "Epoch 249/250, Loss: 0.8406113982200623, Test Loss: 0.9599690437316895\n",
            "Epoch 250/250, Loss: 1.2893661260604858, Test Loss: 0.9175333976745605\n",
            "Final MSE: 0.917533278465271\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=32, epochs=1\n",
            "Epoch 1/1, Loss: 279.0564270019531, Test Loss: 263.7034606933594\n",
            "Final MSE: 263.7033996582031\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=32, epochs=10\n",
            "Epoch 1/10, Loss: 275.4866638183594, Test Loss: 264.3370666503906\n",
            "Epoch 2/10, Loss: 275.21221923828125, Test Loss: 262.5796203613281\n",
            "Epoch 3/10, Loss: 265.2066345214844, Test Loss: 260.6223449707031\n",
            "Epoch 4/10, Loss: 256.45916748046875, Test Loss: 258.519775390625\n",
            "Epoch 5/10, Loss: 258.3548889160156, Test Loss: 256.1619873046875\n",
            "Epoch 6/10, Loss: 253.82188415527344, Test Loss: 253.4730987548828\n",
            "Epoch 7/10, Loss: 263.9403381347656, Test Loss: 250.46121215820312\n",
            "Epoch 8/10, Loss: 252.22476196289062, Test Loss: 246.82461547851562\n",
            "Epoch 9/10, Loss: 252.4298553466797, Test Loss: 242.62762451171875\n",
            "Epoch 10/10, Loss: 243.24658203125, Test Loss: 237.54164123535156\n",
            "Final MSE: 237.54161071777344\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=32, epochs=25\n",
            "Epoch 1/25, Loss: 278.57611083984375, Test Loss: 278.6201171875\n",
            "Epoch 2/25, Loss: 300.0173034667969, Test Loss: 277.0723571777344\n",
            "Epoch 3/25, Loss: 279.4122009277344, Test Loss: 275.58795166015625\n",
            "Epoch 4/25, Loss: 275.2680358886719, Test Loss: 274.0325622558594\n",
            "Epoch 5/25, Loss: 269.2702331542969, Test Loss: 272.2801513671875\n",
            "Epoch 6/25, Loss: 256.4242248535156, Test Loss: 270.2939453125\n",
            "Epoch 7/25, Loss: 252.580810546875, Test Loss: 267.8580627441406\n",
            "Epoch 8/25, Loss: 274.5979309082031, Test Loss: 265.03997802734375\n",
            "Epoch 9/25, Loss: 259.99652099609375, Test Loss: 261.4422607421875\n",
            "Epoch 10/25, Loss: 261.907470703125, Test Loss: 256.7750244140625\n",
            "Epoch 11/25, Loss: 236.3011932373047, Test Loss: 250.7711181640625\n",
            "Epoch 12/25, Loss: 220.95166015625, Test Loss: 243.4646759033203\n",
            "Epoch 13/25, Loss: 190.36990356445312, Test Loss: 233.6340789794922\n",
            "Epoch 14/25, Loss: 178.17308044433594, Test Loss: 222.34310913085938\n",
            "Epoch 15/25, Loss: 198.3872528076172, Test Loss: 207.7586212158203\n",
            "Epoch 16/25, Loss: 138.59019470214844, Test Loss: 188.42848205566406\n",
            "Epoch 17/25, Loss: 122.36186981201172, Test Loss: 165.61135864257812\n",
            "Epoch 18/25, Loss: 120.49089813232422, Test Loss: 139.89710998535156\n",
            "Epoch 19/25, Loss: 89.31437683105469, Test Loss: 110.91252899169922\n",
            "Epoch 20/25, Loss: 57.635005950927734, Test Loss: 80.9373550415039\n",
            "Epoch 21/25, Loss: 25.598079681396484, Test Loss: 53.044525146484375\n",
            "Epoch 22/25, Loss: 17.184648513793945, Test Loss: 30.45208740234375\n",
            "Epoch 23/25, Loss: 8.951031684875488, Test Loss: 15.153402328491211\n",
            "Epoch 24/25, Loss: 4.751678466796875, Test Loss: 6.595449924468994\n",
            "Epoch 25/25, Loss: 2.569232225418091, Test Loss: 2.943885087966919\n",
            "Final MSE: 2.94388484954834\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=32, epochs=50\n",
            "Epoch 1/50, Loss: 297.1593933105469, Test Loss: 286.7199401855469\n",
            "Epoch 2/50, Loss: 289.3351135253906, Test Loss: 285.2920227050781\n",
            "Epoch 3/50, Loss: 293.59014892578125, Test Loss: 283.873779296875\n",
            "Epoch 4/50, Loss: 291.38531494140625, Test Loss: 282.38201904296875\n",
            "Epoch 5/50, Loss: 295.72515869140625, Test Loss: 280.7675476074219\n",
            "Epoch 6/50, Loss: 292.8258972167969, Test Loss: 278.8680725097656\n",
            "Epoch 7/50, Loss: 286.4512023925781, Test Loss: 276.6256408691406\n",
            "Epoch 8/50, Loss: 283.4748229980469, Test Loss: 273.9790344238281\n",
            "Epoch 9/50, Loss: 285.19085693359375, Test Loss: 270.77557373046875\n",
            "Epoch 10/50, Loss: 274.0433349609375, Test Loss: 266.8948974609375\n",
            "Epoch 11/50, Loss: 258.2051696777344, Test Loss: 262.1327209472656\n",
            "Epoch 12/50, Loss: 262.09136962890625, Test Loss: 256.2686767578125\n",
            "Epoch 13/50, Loss: 236.439208984375, Test Loss: 248.78863525390625\n",
            "Epoch 14/50, Loss: 220.1154327392578, Test Loss: 239.58094787597656\n",
            "Epoch 15/50, Loss: 221.7701416015625, Test Loss: 228.63169860839844\n",
            "Epoch 16/50, Loss: 190.8533477783203, Test Loss: 215.0373077392578\n",
            "Epoch 17/50, Loss: 164.96890258789062, Test Loss: 197.6130828857422\n",
            "Epoch 18/50, Loss: 178.40524291992188, Test Loss: 177.61143493652344\n",
            "Epoch 19/50, Loss: 141.84359741210938, Test Loss: 153.3341064453125\n",
            "Epoch 20/50, Loss: 113.11101531982422, Test Loss: 124.63716125488281\n",
            "Epoch 21/50, Loss: 66.77672576904297, Test Loss: 93.35232543945312\n",
            "Epoch 22/50, Loss: 37.9775276184082, Test Loss: 63.43102264404297\n",
            "Epoch 23/50, Loss: 21.647686004638672, Test Loss: 38.15970993041992\n",
            "Epoch 24/50, Loss: 8.444976806640625, Test Loss: 19.883176803588867\n",
            "Epoch 25/50, Loss: 4.225640296936035, Test Loss: 9.330723762512207\n",
            "Epoch 26/50, Loss: 4.21315860748291, Test Loss: 4.033836841583252\n",
            "Epoch 27/50, Loss: 1.7475157976150513, Test Loss: 2.045578956604004\n",
            "Epoch 28/50, Loss: 2.3058526515960693, Test Loss: 1.5510804653167725\n",
            "Epoch 29/50, Loss: 0.8455910086631775, Test Loss: 1.388264536857605\n",
            "Epoch 30/50, Loss: 0.773307204246521, Test Loss: 1.3190163373947144\n",
            "Epoch 31/50, Loss: 1.3844112157821655, Test Loss: 1.224363923072815\n",
            "Epoch 32/50, Loss: 0.7306613922119141, Test Loss: 1.1910570859909058\n",
            "Epoch 33/50, Loss: 0.8062030673027039, Test Loss: 1.1435147523880005\n",
            "Epoch 34/50, Loss: 0.9630122780799866, Test Loss: 1.1212685108184814\n",
            "Epoch 35/50, Loss: 0.9229297041893005, Test Loss: 1.0848448276519775\n",
            "Epoch 36/50, Loss: 0.7790487408638, Test Loss: 1.0490747690200806\n",
            "Epoch 37/50, Loss: 0.9024385213851929, Test Loss: 1.01546311378479\n",
            "Epoch 38/50, Loss: 1.022992491722107, Test Loss: 1.0484566688537598\n",
            "Epoch 39/50, Loss: 0.7557732462882996, Test Loss: 1.0020148754119873\n",
            "Epoch 40/50, Loss: 1.1904958486557007, Test Loss: 1.035563349723816\n",
            "Epoch 41/50, Loss: 0.9926553964614868, Test Loss: 1.0020467042922974\n",
            "Epoch 42/50, Loss: 0.7956199645996094, Test Loss: 0.9989097714424133\n",
            "Epoch 43/50, Loss: 0.7300933599472046, Test Loss: 0.97324538230896\n",
            "Epoch 44/50, Loss: 0.8073619604110718, Test Loss: 0.9750314950942993\n",
            "Epoch 45/50, Loss: 0.8715687394142151, Test Loss: 0.9551522731781006\n",
            "Epoch 46/50, Loss: 1.3254106044769287, Test Loss: 0.9347512722015381\n",
            "Epoch 47/50, Loss: 0.5789201855659485, Test Loss: 0.9666648507118225\n",
            "Epoch 48/50, Loss: 0.7149521708488464, Test Loss: 0.9595819711685181\n",
            "Epoch 49/50, Loss: 0.7550698518753052, Test Loss: 0.943250834941864\n",
            "Epoch 50/50, Loss: 1.0777767896652222, Test Loss: 0.9576965570449829\n",
            "Final MSE: 0.9576966762542725\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=32, epochs=100\n",
            "Epoch 1/100, Loss: 269.3793029785156, Test Loss: 274.77166748046875\n",
            "Epoch 2/100, Loss: 283.65631103515625, Test Loss: 273.0163269042969\n",
            "Epoch 3/100, Loss: 280.95269775390625, Test Loss: 271.07568359375\n",
            "Epoch 4/100, Loss: 280.1058044433594, Test Loss: 268.8574523925781\n",
            "Epoch 5/100, Loss: 275.0728759765625, Test Loss: 266.2530517578125\n",
            "Epoch 6/100, Loss: 268.27044677734375, Test Loss: 263.0719909667969\n",
            "Epoch 7/100, Loss: 243.01316833496094, Test Loss: 259.1065979003906\n",
            "Epoch 8/100, Loss: 256.8506164550781, Test Loss: 254.36058044433594\n",
            "Epoch 9/100, Loss: 263.5994567871094, Test Loss: 248.2915802001953\n",
            "Epoch 10/100, Loss: 230.90538024902344, Test Loss: 241.0701904296875\n",
            "Epoch 11/100, Loss: 229.92066955566406, Test Loss: 232.28407287597656\n",
            "Epoch 12/100, Loss: 217.39236450195312, Test Loss: 221.36253356933594\n",
            "Epoch 13/100, Loss: 195.35035705566406, Test Loss: 208.17770385742188\n",
            "Epoch 14/100, Loss: 193.35818481445312, Test Loss: 192.8275909423828\n",
            "Epoch 15/100, Loss: 157.45204162597656, Test Loss: 174.92713928222656\n",
            "Epoch 16/100, Loss: 134.49864196777344, Test Loss: 153.04029846191406\n",
            "Epoch 17/100, Loss: 106.35388946533203, Test Loss: 128.31341552734375\n",
            "Epoch 18/100, Loss: 83.986572265625, Test Loss: 102.26911163330078\n",
            "Epoch 19/100, Loss: 47.83500289916992, Test Loss: 75.35730743408203\n",
            "Epoch 20/100, Loss: 27.27105140686035, Test Loss: 49.45102310180664\n",
            "Epoch 21/100, Loss: 20.362102508544922, Test Loss: 29.669078826904297\n",
            "Epoch 22/100, Loss: 5.978476524353027, Test Loss: 14.666157722473145\n",
            "Epoch 23/100, Loss: 4.076534271240234, Test Loss: 6.55766487121582\n",
            "Epoch 24/100, Loss: 3.437110185623169, Test Loss: 2.919389247894287\n",
            "Epoch 25/100, Loss: 1.8066799640655518, Test Loss: 1.781806468963623\n",
            "Epoch 26/100, Loss: 1.211409568786621, Test Loss: 1.3106207847595215\n",
            "Epoch 27/100, Loss: 0.8026337027549744, Test Loss: 1.152417540550232\n",
            "Epoch 28/100, Loss: 0.9754745364189148, Test Loss: 1.0605603456497192\n",
            "Epoch 29/100, Loss: 1.4076350927352905, Test Loss: 1.0084422826766968\n",
            "Epoch 30/100, Loss: 1.647512674331665, Test Loss: 0.957552969455719\n",
            "Epoch 31/100, Loss: 1.0035064220428467, Test Loss: 0.9387043118476868\n",
            "Epoch 32/100, Loss: 0.84898442029953, Test Loss: 0.9169025421142578\n",
            "Epoch 33/100, Loss: 1.2466793060302734, Test Loss: 0.9642282128334045\n",
            "Epoch 34/100, Loss: 0.8924440741539001, Test Loss: 0.9105426669120789\n",
            "Epoch 35/100, Loss: 0.7753190994262695, Test Loss: 0.8963068723678589\n",
            "Epoch 36/100, Loss: 0.7679263353347778, Test Loss: 0.931644082069397\n",
            "Epoch 37/100, Loss: 1.3012093305587769, Test Loss: 0.9137851595878601\n",
            "Epoch 38/100, Loss: 0.8935036063194275, Test Loss: 0.9377565979957581\n",
            "Epoch 39/100, Loss: 0.9189367294311523, Test Loss: 0.8963617086410522\n",
            "Epoch 40/100, Loss: 0.9667666554450989, Test Loss: 0.9118511080741882\n",
            "Epoch 41/100, Loss: 0.6394271850585938, Test Loss: 0.948483407497406\n",
            "Epoch 42/100, Loss: 1.4639949798583984, Test Loss: 0.9103479385375977\n",
            "Epoch 43/100, Loss: 0.7222785353660583, Test Loss: 0.9134949445724487\n",
            "Epoch 44/100, Loss: 1.3278675079345703, Test Loss: 0.9065070748329163\n",
            "Epoch 45/100, Loss: 1.036178708076477, Test Loss: 0.9458135366439819\n",
            "Epoch 46/100, Loss: 1.7782784700393677, Test Loss: 0.93359375\n",
            "Epoch 47/100, Loss: 0.8974902033805847, Test Loss: 0.9310514330863953\n",
            "Epoch 48/100, Loss: 1.5281938314437866, Test Loss: 0.9667243361473083\n",
            "Epoch 49/100, Loss: 0.8944559693336487, Test Loss: 0.9108064770698547\n",
            "Epoch 50/100, Loss: 1.2093117237091064, Test Loss: 0.9374200105667114\n",
            "Epoch 51/100, Loss: 1.085164189338684, Test Loss: 0.9170183539390564\n",
            "Epoch 52/100, Loss: 1.564087986946106, Test Loss: 0.943368136882782\n",
            "Epoch 53/100, Loss: 1.1999911069869995, Test Loss: 0.9413699507713318\n",
            "Epoch 54/100, Loss: 1.2687731981277466, Test Loss: 0.9404192566871643\n",
            "Epoch 55/100, Loss: 0.8046665191650391, Test Loss: 0.9098687767982483\n",
            "Epoch 56/100, Loss: 1.3538376092910767, Test Loss: 0.9461948871612549\n",
            "Epoch 57/100, Loss: 0.5851148366928101, Test Loss: 0.918387770652771\n",
            "Epoch 58/100, Loss: 0.9954684972763062, Test Loss: 0.8984775543212891\n",
            "Epoch 59/100, Loss: 1.24321711063385, Test Loss: 0.9383482336997986\n",
            "Epoch 60/100, Loss: 0.9089413285255432, Test Loss: 0.9448719024658203\n",
            "Epoch 61/100, Loss: 1.0174938440322876, Test Loss: 0.9857481122016907\n",
            "Epoch 62/100, Loss: 0.7756484150886536, Test Loss: 0.9081469178199768\n",
            "Epoch 63/100, Loss: 1.703736662864685, Test Loss: 0.9318656325340271\n",
            "Epoch 64/100, Loss: 1.3447599411010742, Test Loss: 0.927830159664154\n",
            "Epoch 65/100, Loss: 0.8311357498168945, Test Loss: 0.8817387819290161\n",
            "Epoch 66/100, Loss: 0.6193801760673523, Test Loss: 0.9705499410629272\n",
            "Epoch 67/100, Loss: 1.1983129978179932, Test Loss: 0.9313055276870728\n",
            "Epoch 68/100, Loss: 2.014009714126587, Test Loss: 0.9309701323509216\n",
            "Epoch 69/100, Loss: 0.8048149943351746, Test Loss: 0.9178830981254578\n",
            "Epoch 70/100, Loss: 1.1686122417449951, Test Loss: 0.9456402659416199\n",
            "Epoch 71/100, Loss: 0.9874362945556641, Test Loss: 0.9346877336502075\n",
            "Epoch 72/100, Loss: 1.5089375972747803, Test Loss: 0.8867920637130737\n",
            "Epoch 73/100, Loss: 1.580123782157898, Test Loss: 0.9704532623291016\n",
            "Epoch 74/100, Loss: 0.8420976400375366, Test Loss: 0.9509811401367188\n",
            "Epoch 75/100, Loss: 1.1957134008407593, Test Loss: 0.9537803530693054\n",
            "Epoch 76/100, Loss: 0.7389211058616638, Test Loss: 0.9151807427406311\n",
            "Epoch 77/100, Loss: 0.912799060344696, Test Loss: 0.9632279872894287\n",
            "Epoch 78/100, Loss: 0.7716652154922485, Test Loss: 0.939663290977478\n",
            "Epoch 79/100, Loss: 1.3166391849517822, Test Loss: 0.8984904289245605\n",
            "Epoch 80/100, Loss: 1.2975492477416992, Test Loss: 0.9720442295074463\n",
            "Epoch 81/100, Loss: 0.7264599204063416, Test Loss: 0.9274226427078247\n",
            "Epoch 82/100, Loss: 0.9890901446342468, Test Loss: 0.9242238998413086\n",
            "Epoch 83/100, Loss: 1.3249790668487549, Test Loss: 0.924572229385376\n",
            "Epoch 84/100, Loss: 1.0508677959442139, Test Loss: 0.9310582876205444\n",
            "Epoch 85/100, Loss: 0.8631616830825806, Test Loss: 0.9520649909973145\n",
            "Epoch 86/100, Loss: 0.8702037930488586, Test Loss: 0.9370182752609253\n",
            "Epoch 87/100, Loss: 0.7633886337280273, Test Loss: 0.9418455958366394\n",
            "Epoch 88/100, Loss: 1.7868894338607788, Test Loss: 0.9407268762588501\n",
            "Epoch 89/100, Loss: 1.0026581287384033, Test Loss: 0.8935385346412659\n",
            "Epoch 90/100, Loss: 1.2733017206192017, Test Loss: 0.9720830917358398\n",
            "Epoch 91/100, Loss: 0.9850814938545227, Test Loss: 0.9402311444282532\n",
            "Epoch 92/100, Loss: 1.2231028079986572, Test Loss: 0.9310223460197449\n",
            "Epoch 93/100, Loss: 0.9193540811538696, Test Loss: 0.9202249050140381\n",
            "Epoch 94/100, Loss: 0.5886003375053406, Test Loss: 0.9467915296554565\n",
            "Epoch 95/100, Loss: 1.4730100631713867, Test Loss: 0.9337852597236633\n",
            "Epoch 96/100, Loss: 1.3719942569732666, Test Loss: 0.9526030421257019\n",
            "Epoch 97/100, Loss: 0.9227867722511292, Test Loss: 0.9730628132820129\n",
            "Epoch 98/100, Loss: 1.4793096780776978, Test Loss: 0.8933060169219971\n",
            "Epoch 99/100, Loss: 1.166955828666687, Test Loss: 0.9503399729728699\n",
            "Epoch 100/100, Loss: 0.8739887475967407, Test Loss: 0.9349958300590515\n",
            "Final MSE: 0.934995710849762\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=32, epochs=250\n",
            "Epoch 1/250, Loss: 293.940673828125, Test Loss: 283.45172119140625\n",
            "Epoch 2/250, Loss: 274.0213317871094, Test Loss: 281.1622009277344\n",
            "Epoch 3/250, Loss: 283.52374267578125, Test Loss: 278.8315734863281\n",
            "Epoch 4/250, Loss: 269.022216796875, Test Loss: 276.390869140625\n",
            "Epoch 5/250, Loss: 291.76544189453125, Test Loss: 273.5249328613281\n",
            "Epoch 6/250, Loss: 278.85003662109375, Test Loss: 270.1910095214844\n",
            "Epoch 7/250, Loss: 276.515869140625, Test Loss: 266.01904296875\n",
            "Epoch 8/250, Loss: 265.3689880371094, Test Loss: 261.1748046875\n",
            "Epoch 9/250, Loss: 250.6012420654297, Test Loss: 255.60997009277344\n",
            "Epoch 10/250, Loss: 251.8502655029297, Test Loss: 248.8266143798828\n",
            "Epoch 11/250, Loss: 241.46902465820312, Test Loss: 240.40744018554688\n",
            "Epoch 12/250, Loss: 220.16941833496094, Test Loss: 229.9827423095703\n",
            "Epoch 13/250, Loss: 200.74009704589844, Test Loss: 217.59693908691406\n",
            "Epoch 14/250, Loss: 192.1798553466797, Test Loss: 202.89984130859375\n",
            "Epoch 15/250, Loss: 163.22348022460938, Test Loss: 184.58538818359375\n",
            "Epoch 16/250, Loss: 122.71894836425781, Test Loss: 162.96226501464844\n",
            "Epoch 17/250, Loss: 111.96196746826172, Test Loss: 138.451904296875\n",
            "Epoch 18/250, Loss: 84.23865509033203, Test Loss: 111.5887222290039\n",
            "Epoch 19/250, Loss: 56.155967712402344, Test Loss: 83.20430755615234\n",
            "Epoch 20/250, Loss: 44.76442337036133, Test Loss: 57.78788757324219\n",
            "Epoch 21/250, Loss: 24.47007942199707, Test Loss: 35.080387115478516\n",
            "Epoch 22/250, Loss: 7.810397624969482, Test Loss: 18.033777236938477\n",
            "Epoch 23/250, Loss: 5.73331356048584, Test Loss: 8.691969871520996\n",
            "Epoch 24/250, Loss: 3.831601858139038, Test Loss: 3.9245033264160156\n",
            "Epoch 25/250, Loss: 1.729824185371399, Test Loss: 1.9765338897705078\n",
            "Epoch 26/250, Loss: 1.7240461111068726, Test Loss: 1.4315228462219238\n",
            "Epoch 27/250, Loss: 1.4441508054733276, Test Loss: 1.2341982126235962\n",
            "Epoch 28/250, Loss: 0.8855615854263306, Test Loss: 1.1216539144515991\n",
            "Epoch 29/250, Loss: 0.8389207720756531, Test Loss: 1.058829665184021\n",
            "Epoch 30/250, Loss: 1.02375328540802, Test Loss: 1.014981985092163\n",
            "Epoch 31/250, Loss: 1.4879655838012695, Test Loss: 0.9860660433769226\n",
            "Epoch 32/250, Loss: 1.051900863647461, Test Loss: 0.9661397933959961\n",
            "Epoch 33/250, Loss: 1.2142382860183716, Test Loss: 0.9624927639961243\n",
            "Epoch 34/250, Loss: 0.9930676817893982, Test Loss: 0.952499508857727\n",
            "Epoch 35/250, Loss: 1.3465420007705688, Test Loss: 0.9133660793304443\n",
            "Epoch 36/250, Loss: 1.4140796661376953, Test Loss: 0.9550908803939819\n",
            "Epoch 37/250, Loss: 0.9757100343704224, Test Loss: 0.919677734375\n",
            "Epoch 38/250, Loss: 0.767443060874939, Test Loss: 0.9236426949501038\n",
            "Epoch 39/250, Loss: 0.8597742319107056, Test Loss: 0.9274170994758606\n",
            "Epoch 40/250, Loss: 1.4785349369049072, Test Loss: 0.9367077946662903\n",
            "Epoch 41/250, Loss: 0.8902096748352051, Test Loss: 0.9302565455436707\n",
            "Epoch 42/250, Loss: 0.886143147945404, Test Loss: 0.9254000782966614\n",
            "Epoch 43/250, Loss: 1.6926164627075195, Test Loss: 0.9017643928527832\n",
            "Epoch 44/250, Loss: 1.001613974571228, Test Loss: 0.9097352027893066\n",
            "Epoch 45/250, Loss: 0.8431634306907654, Test Loss: 0.942679226398468\n",
            "Epoch 46/250, Loss: 1.2008241415023804, Test Loss: 0.9220436215400696\n",
            "Epoch 47/250, Loss: 0.7915372252464294, Test Loss: 0.9119030833244324\n",
            "Epoch 48/250, Loss: 1.2269293069839478, Test Loss: 0.9337981939315796\n",
            "Epoch 49/250, Loss: 1.0461660623550415, Test Loss: 0.9016737937927246\n",
            "Epoch 50/250, Loss: 0.739017128944397, Test Loss: 0.9129797220230103\n",
            "Epoch 51/250, Loss: 0.8629341125488281, Test Loss: 0.9521421790122986\n",
            "Epoch 52/250, Loss: 0.7227702736854553, Test Loss: 0.9296270608901978\n",
            "Epoch 53/250, Loss: 0.7262882590293884, Test Loss: 0.913131833076477\n",
            "Epoch 54/250, Loss: 0.7529377341270447, Test Loss: 0.928157389163971\n",
            "Epoch 55/250, Loss: 1.0890865325927734, Test Loss: 0.9222611784934998\n",
            "Epoch 56/250, Loss: 0.7964103817939758, Test Loss: 0.9068059921264648\n",
            "Epoch 57/250, Loss: 0.9873090386390686, Test Loss: 0.9428137540817261\n",
            "Epoch 58/250, Loss: 0.8616203665733337, Test Loss: 0.9344373345375061\n",
            "Epoch 59/250, Loss: 1.1308255195617676, Test Loss: 0.9126545190811157\n",
            "Epoch 60/250, Loss: 0.4639751613140106, Test Loss: 0.950960099697113\n",
            "Epoch 61/250, Loss: 1.1915754079818726, Test Loss: 0.9311606287956238\n",
            "Epoch 62/250, Loss: 1.8298799991607666, Test Loss: 0.9772161245346069\n",
            "Epoch 63/250, Loss: 1.0700860023498535, Test Loss: 0.9109653234481812\n",
            "Epoch 64/250, Loss: 0.8501394987106323, Test Loss: 0.9284624457359314\n",
            "Epoch 65/250, Loss: 0.8737543225288391, Test Loss: 0.9222580194473267\n",
            "Epoch 66/250, Loss: 1.142060399055481, Test Loss: 0.9612249135971069\n",
            "Epoch 67/250, Loss: 0.8952134251594543, Test Loss: 0.9187150001525879\n",
            "Epoch 68/250, Loss: 0.7961097359657288, Test Loss: 0.9467198848724365\n",
            "Epoch 69/250, Loss: 1.728081464767456, Test Loss: 0.9347385168075562\n",
            "Epoch 70/250, Loss: 1.0282840728759766, Test Loss: 0.9360151886940002\n",
            "Epoch 71/250, Loss: 0.931864321231842, Test Loss: 0.9234452247619629\n",
            "Epoch 72/250, Loss: 0.8443459272384644, Test Loss: 0.9334248304367065\n",
            "Epoch 73/250, Loss: 1.7665481567382812, Test Loss: 0.9505330324172974\n",
            "Epoch 74/250, Loss: 0.7646248936653137, Test Loss: 0.9059261083602905\n",
            "Epoch 75/250, Loss: 1.2816139459609985, Test Loss: 0.9484602808952332\n",
            "Epoch 76/250, Loss: 1.1037625074386597, Test Loss: 0.9152582883834839\n",
            "Epoch 77/250, Loss: 0.9419741630554199, Test Loss: 0.9437877535820007\n",
            "Epoch 78/250, Loss: 0.9535975456237793, Test Loss: 0.9379568696022034\n",
            "Epoch 79/250, Loss: 1.041474461555481, Test Loss: 0.9123644232749939\n",
            "Epoch 80/250, Loss: 1.5325167179107666, Test Loss: 0.9353029131889343\n",
            "Epoch 81/250, Loss: 1.0687066316604614, Test Loss: 0.9582704901695251\n",
            "Epoch 82/250, Loss: 0.9700984358787537, Test Loss: 0.9267218112945557\n",
            "Epoch 83/250, Loss: 0.8815617561340332, Test Loss: 0.9728819727897644\n",
            "Epoch 84/250, Loss: 0.6076522469520569, Test Loss: 0.9243901968002319\n",
            "Epoch 85/250, Loss: 1.3726540803909302, Test Loss: 0.9214092493057251\n",
            "Epoch 86/250, Loss: 1.0378848314285278, Test Loss: 0.9397811889648438\n",
            "Epoch 87/250, Loss: 1.0046671628952026, Test Loss: 0.9356057047843933\n",
            "Epoch 88/250, Loss: 0.8873239755630493, Test Loss: 0.9068737030029297\n",
            "Epoch 89/250, Loss: 0.6813668608665466, Test Loss: 0.952707052230835\n",
            "Epoch 90/250, Loss: 0.9623705744743347, Test Loss: 0.9285566806793213\n",
            "Epoch 91/250, Loss: 1.1576653718948364, Test Loss: 0.9354217052459717\n",
            "Epoch 92/250, Loss: 1.0477612018585205, Test Loss: 0.9645871520042419\n",
            "Epoch 93/250, Loss: 1.0881106853485107, Test Loss: 0.9555639028549194\n",
            "Epoch 94/250, Loss: 1.1213775873184204, Test Loss: 0.908318817615509\n",
            "Epoch 95/250, Loss: 1.6396267414093018, Test Loss: 0.9062210321426392\n",
            "Epoch 96/250, Loss: 1.0232203006744385, Test Loss: 0.9824958443641663\n",
            "Epoch 97/250, Loss: 0.6578271985054016, Test Loss: 0.9268887639045715\n",
            "Epoch 98/250, Loss: 0.8752997517585754, Test Loss: 0.91937255859375\n",
            "Epoch 99/250, Loss: 0.7313739061355591, Test Loss: 0.943351149559021\n",
            "Epoch 100/250, Loss: 1.0986818075180054, Test Loss: 0.95339435338974\n",
            "Epoch 101/250, Loss: 1.0845582485198975, Test Loss: 0.9213096499443054\n",
            "Epoch 102/250, Loss: 1.3396825790405273, Test Loss: 0.9864950776100159\n",
            "Epoch 103/250, Loss: 1.3449944257736206, Test Loss: 0.895828127861023\n",
            "Epoch 104/250, Loss: 0.7292974591255188, Test Loss: 1.0056408643722534\n",
            "Epoch 105/250, Loss: 1.0652662515640259, Test Loss: 0.9073136448860168\n",
            "Epoch 106/250, Loss: 0.7969484329223633, Test Loss: 0.9278715252876282\n",
            "Epoch 107/250, Loss: 0.9633251428604126, Test Loss: 0.9373634457588196\n",
            "Epoch 108/250, Loss: 0.7468417882919312, Test Loss: 0.946251392364502\n",
            "Epoch 109/250, Loss: 1.3744975328445435, Test Loss: 0.9309771656990051\n",
            "Epoch 110/250, Loss: 1.2420321702957153, Test Loss: 0.8845269680023193\n",
            "Epoch 111/250, Loss: 1.4659727811813354, Test Loss: 0.9363465905189514\n",
            "Epoch 112/250, Loss: 0.9651650786399841, Test Loss: 0.9372808933258057\n",
            "Epoch 113/250, Loss: 1.4906580448150635, Test Loss: 0.9852710366249084\n",
            "Epoch 114/250, Loss: 2.579556703567505, Test Loss: 0.9215695261955261\n",
            "Epoch 115/250, Loss: 1.134749174118042, Test Loss: 0.9526928067207336\n",
            "Epoch 116/250, Loss: 0.6743598580360413, Test Loss: 0.9371951222419739\n",
            "Epoch 117/250, Loss: 1.0603039264678955, Test Loss: 0.9303483366966248\n",
            "Epoch 118/250, Loss: 1.0551574230194092, Test Loss: 0.947414755821228\n",
            "Epoch 119/250, Loss: 0.8822861909866333, Test Loss: 0.9326559901237488\n",
            "Epoch 120/250, Loss: 0.7897371649742126, Test Loss: 0.9184396266937256\n",
            "Epoch 121/250, Loss: 1.2357147932052612, Test Loss: 0.951932966709137\n",
            "Epoch 122/250, Loss: 1.377163290977478, Test Loss: 0.9013670682907104\n",
            "Epoch 123/250, Loss: 1.2595447301864624, Test Loss: 0.962056040763855\n",
            "Epoch 124/250, Loss: 0.9725944399833679, Test Loss: 0.9828808903694153\n",
            "Epoch 125/250, Loss: 0.6235795617103577, Test Loss: 0.9001575708389282\n",
            "Epoch 126/250, Loss: 1.0489399433135986, Test Loss: 0.9881072044372559\n",
            "Epoch 127/250, Loss: 1.104880690574646, Test Loss: 0.9607889652252197\n",
            "Epoch 128/250, Loss: 0.9713758230209351, Test Loss: 0.9759531021118164\n",
            "Epoch 129/250, Loss: 1.4370386600494385, Test Loss: 0.9144255518913269\n",
            "Epoch 130/250, Loss: 1.3530378341674805, Test Loss: 0.9520168304443359\n",
            "Epoch 131/250, Loss: 1.269152045249939, Test Loss: 0.9170275330543518\n",
            "Epoch 132/250, Loss: 1.2915910482406616, Test Loss: 0.9623796343803406\n",
            "Epoch 133/250, Loss: 1.3745567798614502, Test Loss: 0.9875166416168213\n",
            "Epoch 134/250, Loss: 0.6665759682655334, Test Loss: 0.9662100076675415\n",
            "Epoch 135/250, Loss: 0.9126189351081848, Test Loss: 0.9161314964294434\n",
            "Epoch 136/250, Loss: 1.4483394622802734, Test Loss: 0.9077410101890564\n",
            "Epoch 137/250, Loss: 0.7623274922370911, Test Loss: 1.0066499710083008\n",
            "Epoch 138/250, Loss: 1.3236643075942993, Test Loss: 0.970455527305603\n",
            "Epoch 139/250, Loss: 0.8946112394332886, Test Loss: 0.9645121097564697\n",
            "Epoch 140/250, Loss: 1.345504879951477, Test Loss: 0.9640430212020874\n",
            "Epoch 141/250, Loss: 1.5029608011245728, Test Loss: 0.9668257236480713\n",
            "Epoch 142/250, Loss: 1.2737696170806885, Test Loss: 0.9551944136619568\n",
            "Epoch 143/250, Loss: 0.683337390422821, Test Loss: 0.9908019304275513\n",
            "Epoch 144/250, Loss: 0.745406448841095, Test Loss: 0.93874192237854\n",
            "Epoch 145/250, Loss: 0.9704704880714417, Test Loss: 0.9170116782188416\n",
            "Epoch 146/250, Loss: 0.7489588856697083, Test Loss: 0.9065998792648315\n",
            "Epoch 147/250, Loss: 0.9257852435112, Test Loss: 0.952407717704773\n",
            "Epoch 148/250, Loss: 0.8409604430198669, Test Loss: 1.0216385126113892\n",
            "Epoch 149/250, Loss: 1.8137656450271606, Test Loss: 0.9062747359275818\n",
            "Epoch 150/250, Loss: 0.8471066355705261, Test Loss: 0.8969239592552185\n",
            "Epoch 151/250, Loss: 0.6488197445869446, Test Loss: 0.9967970252037048\n",
            "Epoch 152/250, Loss: 0.8673956990242004, Test Loss: 0.9572609066963196\n",
            "Epoch 153/250, Loss: 1.433884859085083, Test Loss: 0.9156092405319214\n",
            "Epoch 154/250, Loss: 1.3879375457763672, Test Loss: 0.9661091566085815\n",
            "Epoch 155/250, Loss: 1.486441969871521, Test Loss: 0.9361020922660828\n",
            "Epoch 156/250, Loss: 0.9338169097900391, Test Loss: 0.9314766526222229\n",
            "Epoch 157/250, Loss: 1.1208328008651733, Test Loss: 0.9008741974830627\n",
            "Epoch 158/250, Loss: 1.5808906555175781, Test Loss: 1.0197268724441528\n",
            "Epoch 159/250, Loss: 0.9451280236244202, Test Loss: 0.9305000901222229\n",
            "Epoch 160/250, Loss: 1.3842045068740845, Test Loss: 0.9485664963722229\n",
            "Epoch 161/250, Loss: 0.7291852831840515, Test Loss: 0.9713091254234314\n",
            "Epoch 162/250, Loss: 1.0158098936080933, Test Loss: 0.9044793248176575\n",
            "Epoch 163/250, Loss: 1.8033411502838135, Test Loss: 0.9425570368766785\n",
            "Epoch 164/250, Loss: 1.4365469217300415, Test Loss: 1.0170845985412598\n",
            "Epoch 165/250, Loss: 0.9551214575767517, Test Loss: 0.9411076307296753\n",
            "Epoch 166/250, Loss: 1.5120781660079956, Test Loss: 0.9535232782363892\n",
            "Epoch 167/250, Loss: 2.131859064102173, Test Loss: 0.9297021627426147\n",
            "Epoch 168/250, Loss: 1.3045400381088257, Test Loss: 0.9050272703170776\n",
            "Epoch 169/250, Loss: 1.3148220777511597, Test Loss: 1.0011357069015503\n",
            "Epoch 170/250, Loss: 1.0452479124069214, Test Loss: 0.9302145838737488\n",
            "Epoch 171/250, Loss: 1.5627943277359009, Test Loss: 0.9540092349052429\n",
            "Epoch 172/250, Loss: 0.5753692388534546, Test Loss: 0.9389672875404358\n",
            "Epoch 173/250, Loss: 0.9984347224235535, Test Loss: 0.9565656781196594\n",
            "Epoch 174/250, Loss: 2.132032632827759, Test Loss: 0.9663292765617371\n",
            "Epoch 175/250, Loss: 1.221996545791626, Test Loss: 0.9632231593132019\n",
            "Epoch 176/250, Loss: 0.7271996736526489, Test Loss: 0.9640191197395325\n",
            "Epoch 177/250, Loss: 0.720338761806488, Test Loss: 0.9141919016838074\n",
            "Epoch 178/250, Loss: 1.1826609373092651, Test Loss: 0.971696674823761\n",
            "Epoch 179/250, Loss: 0.8097759485244751, Test Loss: 0.9407227039337158\n",
            "Epoch 180/250, Loss: 1.2463642358779907, Test Loss: 0.951989471912384\n",
            "Epoch 181/250, Loss: 1.0257742404937744, Test Loss: 0.9869375228881836\n",
            "Epoch 182/250, Loss: 0.9607094526290894, Test Loss: 0.9792081713676453\n",
            "Epoch 183/250, Loss: 0.9179906845092773, Test Loss: 0.9003795981407166\n",
            "Epoch 184/250, Loss: 0.9936032891273499, Test Loss: 0.9501176476478577\n",
            "Epoch 185/250, Loss: 0.9959319233894348, Test Loss: 0.9500614404678345\n",
            "Epoch 186/250, Loss: 0.898045003414154, Test Loss: 0.9626675248146057\n",
            "Epoch 187/250, Loss: 1.2292581796646118, Test Loss: 0.9056572318077087\n",
            "Epoch 188/250, Loss: 1.3137357234954834, Test Loss: 0.9840881824493408\n",
            "Epoch 189/250, Loss: 1.1119543313980103, Test Loss: 0.9290441274642944\n",
            "Epoch 190/250, Loss: 0.9504515528678894, Test Loss: 0.937757670879364\n",
            "Epoch 191/250, Loss: 0.6768350601196289, Test Loss: 0.9336390495300293\n",
            "Epoch 192/250, Loss: 1.6176568269729614, Test Loss: 0.9332261681556702\n",
            "Epoch 193/250, Loss: 1.3266834020614624, Test Loss: 1.027980923652649\n",
            "Epoch 194/250, Loss: 0.5618025660514832, Test Loss: 0.8950470685958862\n",
            "Epoch 195/250, Loss: 1.362163782119751, Test Loss: 0.916214108467102\n",
            "Epoch 196/250, Loss: 0.8345891237258911, Test Loss: 0.9745573997497559\n",
            "Epoch 197/250, Loss: 0.9104315042495728, Test Loss: 0.9357296228408813\n",
            "Epoch 198/250, Loss: 1.3681840896606445, Test Loss: 0.951701283454895\n",
            "Epoch 199/250, Loss: 1.7473455667495728, Test Loss: 0.9646480083465576\n",
            "Epoch 200/250, Loss: 0.9538231492042542, Test Loss: 0.9275861382484436\n",
            "Epoch 201/250, Loss: 1.8981351852416992, Test Loss: 0.9764783978462219\n",
            "Epoch 202/250, Loss: 1.3955066204071045, Test Loss: 0.9182706475257874\n",
            "Epoch 203/250, Loss: 1.1133779287338257, Test Loss: 1.0193829536437988\n",
            "Epoch 204/250, Loss: 1.4438927173614502, Test Loss: 0.933497965335846\n",
            "Epoch 205/250, Loss: 1.1100742816925049, Test Loss: 0.9429727792739868\n",
            "Epoch 206/250, Loss: 1.2691504955291748, Test Loss: 0.9462552666664124\n",
            "Epoch 207/250, Loss: 1.1530059576034546, Test Loss: 0.9838331341743469\n",
            "Epoch 208/250, Loss: 1.619904637336731, Test Loss: 0.9600996375083923\n",
            "Epoch 209/250, Loss: 1.1232472658157349, Test Loss: 0.9129881858825684\n",
            "Epoch 210/250, Loss: 1.1579347848892212, Test Loss: 0.9348635077476501\n",
            "Epoch 211/250, Loss: 1.0425022840499878, Test Loss: 0.9391177892684937\n",
            "Epoch 212/250, Loss: 0.555559515953064, Test Loss: 0.962046205997467\n",
            "Epoch 213/250, Loss: 1.5802257061004639, Test Loss: 0.9336851835250854\n",
            "Epoch 214/250, Loss: 1.6420341730117798, Test Loss: 0.9211567640304565\n",
            "Epoch 215/250, Loss: 0.8652194142341614, Test Loss: 0.9198776483535767\n",
            "Epoch 216/250, Loss: 1.1252410411834717, Test Loss: 0.9685803055763245\n",
            "Epoch 217/250, Loss: 0.9816555380821228, Test Loss: 0.927849292755127\n",
            "Epoch 218/250, Loss: 0.6142219305038452, Test Loss: 0.9574353098869324\n",
            "Epoch 219/250, Loss: 1.9105050563812256, Test Loss: 0.9215078353881836\n",
            "Epoch 220/250, Loss: 0.7835593819618225, Test Loss: 0.9804090857505798\n",
            "Epoch 221/250, Loss: 1.2585937976837158, Test Loss: 0.9185346961021423\n",
            "Epoch 222/250, Loss: 1.4127663373947144, Test Loss: 0.9991052150726318\n",
            "Epoch 223/250, Loss: 0.7907696962356567, Test Loss: 0.8926494121551514\n",
            "Epoch 224/250, Loss: 1.1574839353561401, Test Loss: 0.9331454634666443\n",
            "Epoch 225/250, Loss: 1.002619743347168, Test Loss: 1.0152089595794678\n",
            "Epoch 226/250, Loss: 1.9087096452713013, Test Loss: 0.934331476688385\n",
            "Epoch 227/250, Loss: 1.615314245223999, Test Loss: 0.9266077280044556\n",
            "Epoch 228/250, Loss: 1.1557018756866455, Test Loss: 0.9787408113479614\n",
            "Epoch 229/250, Loss: 1.096853494644165, Test Loss: 0.9156294465065002\n",
            "Epoch 230/250, Loss: 0.8647357225418091, Test Loss: 0.9769265651702881\n",
            "Epoch 231/250, Loss: 1.0717467069625854, Test Loss: 0.9298349618911743\n",
            "Epoch 232/250, Loss: 0.899945080280304, Test Loss: 0.9764412045478821\n",
            "Epoch 233/250, Loss: 1.3813133239746094, Test Loss: 0.9036239385604858\n",
            "Epoch 234/250, Loss: 1.4586849212646484, Test Loss: 0.9707037806510925\n",
            "Epoch 235/250, Loss: 0.6933068037033081, Test Loss: 0.916494607925415\n",
            "Epoch 236/250, Loss: 1.0471776723861694, Test Loss: 0.9439665675163269\n",
            "Epoch 237/250, Loss: 1.125943899154663, Test Loss: 0.9448268413543701\n",
            "Epoch 238/250, Loss: 1.419531226158142, Test Loss: 0.9594929218292236\n",
            "Epoch 239/250, Loss: 0.988185465335846, Test Loss: 0.9233331084251404\n",
            "Epoch 240/250, Loss: 1.0841753482818604, Test Loss: 0.9371219277381897\n",
            "Epoch 241/250, Loss: 1.3802152872085571, Test Loss: 0.8686622977256775\n",
            "Epoch 242/250, Loss: 0.9803882241249084, Test Loss: 1.0536792278289795\n",
            "Epoch 243/250, Loss: 1.0903692245483398, Test Loss: 0.9175544381141663\n",
            "Epoch 244/250, Loss: 1.5370092391967773, Test Loss: 0.9538628458976746\n",
            "Epoch 245/250, Loss: 1.5539757013320923, Test Loss: 0.9409100413322449\n",
            "Epoch 246/250, Loss: 1.2724250555038452, Test Loss: 0.9618452787399292\n",
            "Epoch 247/250, Loss: 0.6825847029685974, Test Loss: 0.8976581692695618\n",
            "Epoch 248/250, Loss: 1.4877387285232544, Test Loss: 0.9888073801994324\n",
            "Epoch 249/250, Loss: 0.652633011341095, Test Loss: 0.9252252578735352\n",
            "Epoch 250/250, Loss: 0.8711298108100891, Test Loss: 0.938399612903595\n",
            "Final MSE: 0.938399612903595\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=64, epochs=1\n",
            "Epoch 1/1, Loss: 293.8022155761719, Test Loss: 288.3233337402344\n",
            "Final MSE: 288.3233337402344\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=64, epochs=10\n",
            "Epoch 1/10, Loss: 278.6282043457031, Test Loss: 270.3864440917969\n",
            "Epoch 2/10, Loss: 285.36297607421875, Test Loss: 269.3691711425781\n",
            "Epoch 3/10, Loss: 277.1108703613281, Test Loss: 268.3347473144531\n",
            "Epoch 4/10, Loss: 273.0425109863281, Test Loss: 267.281005859375\n",
            "Epoch 5/10, Loss: 269.5855712890625, Test Loss: 266.1948547363281\n",
            "Epoch 6/10, Loss: 267.22430419921875, Test Loss: 265.06927490234375\n",
            "Epoch 7/10, Loss: 266.5711364746094, Test Loss: 263.9151306152344\n",
            "Epoch 8/10, Loss: 267.65625, Test Loss: 262.73419189453125\n",
            "Epoch 9/10, Loss: 274.2295837402344, Test Loss: 261.48834228515625\n",
            "Epoch 10/10, Loss: 258.51678466796875, Test Loss: 260.19580078125\n",
            "Final MSE: 260.19580078125\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=64, epochs=25\n",
            "Epoch 1/25, Loss: 261.9007873535156, Test Loss: 258.027587890625\n",
            "Epoch 2/25, Loss: 270.53497314453125, Test Loss: 257.1551513671875\n",
            "Epoch 3/25, Loss: 263.7082824707031, Test Loss: 256.2248229980469\n",
            "Epoch 4/25, Loss: 259.0283508300781, Test Loss: 255.2733917236328\n",
            "Epoch 5/25, Loss: 263.00885009765625, Test Loss: 254.28811645507812\n",
            "Epoch 6/25, Loss: 259.35028076171875, Test Loss: 253.23350524902344\n",
            "Epoch 7/25, Loss: 251.34962463378906, Test Loss: 252.14955139160156\n",
            "Epoch 8/25, Loss: 254.38829040527344, Test Loss: 251.00303649902344\n",
            "Epoch 9/25, Loss: 250.39913940429688, Test Loss: 249.76817321777344\n",
            "Epoch 10/25, Loss: 237.44345092773438, Test Loss: 248.4841766357422\n",
            "Epoch 11/25, Loss: 254.24517822265625, Test Loss: 247.11508178710938\n",
            "Epoch 12/25, Loss: 248.4829864501953, Test Loss: 245.64071655273438\n",
            "Epoch 13/25, Loss: 240.68453979492188, Test Loss: 244.0545196533203\n",
            "Epoch 14/25, Loss: 239.3687286376953, Test Loss: 242.3421173095703\n",
            "Epoch 15/25, Loss: 243.03643798828125, Test Loss: 240.58514404296875\n",
            "Epoch 16/25, Loss: 240.27415466308594, Test Loss: 238.61599731445312\n",
            "Epoch 17/25, Loss: 237.1457977294922, Test Loss: 236.4356231689453\n",
            "Epoch 18/25, Loss: 228.80174255371094, Test Loss: 234.05731201171875\n",
            "Epoch 19/25, Loss: 227.83616638183594, Test Loss: 231.5348663330078\n",
            "Epoch 20/25, Loss: 216.95614624023438, Test Loss: 228.61480712890625\n",
            "Epoch 21/25, Loss: 218.1380157470703, Test Loss: 225.43722534179688\n",
            "Epoch 22/25, Loss: 210.65650939941406, Test Loss: 222.13528442382812\n",
            "Epoch 23/25, Loss: 208.93385314941406, Test Loss: 218.24453735351562\n",
            "Epoch 24/25, Loss: 189.9636993408203, Test Loss: 213.9484100341797\n",
            "Epoch 25/25, Loss: 191.45175170898438, Test Loss: 209.0468292236328\n",
            "Final MSE: 209.0468292236328\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=64, epochs=50\n",
            "Epoch 1/50, Loss: 305.9820251464844, Test Loss: 287.3334655761719\n",
            "Epoch 2/50, Loss: 289.99456787109375, Test Loss: 286.5812072753906\n",
            "Epoch 3/50, Loss: 293.9706726074219, Test Loss: 285.8467102050781\n",
            "Epoch 4/50, Loss: 292.55023193359375, Test Loss: 285.11712646484375\n",
            "Epoch 5/50, Loss: 284.8773498535156, Test Loss: 284.3738708496094\n",
            "Epoch 6/50, Loss: 293.6142578125, Test Loss: 283.6531982421875\n",
            "Epoch 7/50, Loss: 293.7528991699219, Test Loss: 282.87603759765625\n",
            "Epoch 8/50, Loss: 288.6552734375, Test Loss: 282.1029357910156\n",
            "Epoch 9/50, Loss: 292.4455871582031, Test Loss: 281.2957458496094\n",
            "Epoch 10/50, Loss: 274.2856750488281, Test Loss: 280.4475402832031\n",
            "Epoch 11/50, Loss: 289.7244567871094, Test Loss: 279.59039306640625\n",
            "Epoch 12/50, Loss: 279.4754943847656, Test Loss: 278.6759033203125\n",
            "Epoch 13/50, Loss: 290.0088806152344, Test Loss: 277.7074890136719\n",
            "Epoch 14/50, Loss: 284.3486022949219, Test Loss: 276.6364440917969\n",
            "Epoch 15/50, Loss: 270.6414794921875, Test Loss: 275.45849609375\n",
            "Epoch 16/50, Loss: 275.1629333496094, Test Loss: 274.20867919921875\n",
            "Epoch 17/50, Loss: 274.2554016113281, Test Loss: 272.755126953125\n",
            "Epoch 18/50, Loss: 274.9899597167969, Test Loss: 271.25091552734375\n",
            "Epoch 19/50, Loss: 255.8036651611328, Test Loss: 269.4344482421875\n",
            "Epoch 20/50, Loss: 249.66610717773438, Test Loss: 267.407958984375\n",
            "Epoch 21/50, Loss: 260.7227478027344, Test Loss: 265.34814453125\n",
            "Epoch 22/50, Loss: 249.33872985839844, Test Loss: 262.90802001953125\n",
            "Epoch 23/50, Loss: 237.25852966308594, Test Loss: 260.07891845703125\n",
            "Epoch 24/50, Loss: 225.0166473388672, Test Loss: 257.0254821777344\n",
            "Epoch 25/50, Loss: 230.1597442626953, Test Loss: 253.5384979248047\n",
            "Epoch 26/50, Loss: 214.51123046875, Test Loss: 249.42868041992188\n",
            "Epoch 27/50, Loss: 209.06874084472656, Test Loss: 244.494140625\n",
            "Epoch 28/50, Loss: 200.37832641601562, Test Loss: 239.05819702148438\n",
            "Epoch 29/50, Loss: 193.95730590820312, Test Loss: 232.4536895751953\n",
            "Epoch 30/50, Loss: 190.4015655517578, Test Loss: 225.17262268066406\n",
            "Epoch 31/50, Loss: 184.92776489257812, Test Loss: 216.48890686035156\n",
            "Epoch 32/50, Loss: 154.8691864013672, Test Loss: 206.4090118408203\n",
            "Epoch 33/50, Loss: 143.89242553710938, Test Loss: 195.43519592285156\n",
            "Epoch 34/50, Loss: 138.6229705810547, Test Loss: 183.1439666748047\n",
            "Epoch 35/50, Loss: 127.82404327392578, Test Loss: 169.18975830078125\n",
            "Epoch 36/50, Loss: 106.70511627197266, Test Loss: 154.52334594726562\n",
            "Epoch 37/50, Loss: 95.46757507324219, Test Loss: 138.35598754882812\n",
            "Epoch 38/50, Loss: 78.04899597167969, Test Loss: 121.75045776367188\n",
            "Epoch 39/50, Loss: 66.86782836914062, Test Loss: 104.96354675292969\n",
            "Epoch 40/50, Loss: 54.65703201293945, Test Loss: 88.33258819580078\n",
            "Epoch 41/50, Loss: 46.98943328857422, Test Loss: 72.27906036376953\n",
            "Epoch 42/50, Loss: 32.70415115356445, Test Loss: 57.11901092529297\n",
            "Epoch 43/50, Loss: 22.358009338378906, Test Loss: 43.508262634277344\n",
            "Epoch 44/50, Loss: 19.304311752319336, Test Loss: 31.972286224365234\n",
            "Epoch 45/50, Loss: 13.646377563476562, Test Loss: 22.39324951171875\n",
            "Epoch 46/50, Loss: 10.648660659790039, Test Loss: 14.907000541687012\n",
            "Epoch 47/50, Loss: 4.582852363586426, Test Loss: 9.635658264160156\n",
            "Epoch 48/50, Loss: 4.709381103515625, Test Loss: 6.164916038513184\n",
            "Epoch 49/50, Loss: 2.968966007232666, Test Loss: 4.027232646942139\n",
            "Epoch 50/50, Loss: 1.6773473024368286, Test Loss: 2.7664694786071777\n",
            "Final MSE: 2.7664694786071777\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=64, epochs=100\n",
            "Epoch 1/100, Loss: 291.26556396484375, Test Loss: 290.1510925292969\n",
            "Epoch 2/100, Loss: 291.0682067871094, Test Loss: 289.1456298828125\n",
            "Epoch 3/100, Loss: 296.5727233886719, Test Loss: 288.1146545410156\n",
            "Epoch 4/100, Loss: 291.3017578125, Test Loss: 287.07318115234375\n",
            "Epoch 5/100, Loss: 296.13262939453125, Test Loss: 286.03924560546875\n",
            "Epoch 6/100, Loss: 287.9787902832031, Test Loss: 284.95697021484375\n",
            "Epoch 7/100, Loss: 290.4076232910156, Test Loss: 283.8804931640625\n",
            "Epoch 8/100, Loss: 294.2770690917969, Test Loss: 282.75177001953125\n",
            "Epoch 9/100, Loss: 296.3890380859375, Test Loss: 281.55633544921875\n",
            "Epoch 10/100, Loss: 288.1919250488281, Test Loss: 280.3561706542969\n",
            "Epoch 11/100, Loss: 286.3625793457031, Test Loss: 279.05523681640625\n",
            "Epoch 12/100, Loss: 281.2093200683594, Test Loss: 277.7072448730469\n",
            "Epoch 13/100, Loss: 276.56427001953125, Test Loss: 276.2506408691406\n",
            "Epoch 14/100, Loss: 272.5927429199219, Test Loss: 274.68243408203125\n",
            "Epoch 15/100, Loss: 269.5207214355469, Test Loss: 272.9267578125\n",
            "Epoch 16/100, Loss: 264.2487487792969, Test Loss: 271.10205078125\n",
            "Epoch 17/100, Loss: 269.8062744140625, Test Loss: 269.0614013671875\n",
            "Epoch 18/100, Loss: 257.58953857421875, Test Loss: 266.8957824707031\n",
            "Epoch 19/100, Loss: 260.07574462890625, Test Loss: 264.63165283203125\n",
            "Epoch 20/100, Loss: 254.1130828857422, Test Loss: 262.148681640625\n",
            "Epoch 21/100, Loss: 249.4177703857422, Test Loss: 259.3677673339844\n",
            "Epoch 22/100, Loss: 234.22262573242188, Test Loss: 256.2450866699219\n",
            "Epoch 23/100, Loss: 233.21697998046875, Test Loss: 252.84840393066406\n",
            "Epoch 24/100, Loss: 238.74749755859375, Test Loss: 248.884765625\n",
            "Epoch 25/100, Loss: 226.44517517089844, Test Loss: 244.57855224609375\n",
            "Epoch 26/100, Loss: 202.539306640625, Test Loss: 239.48353576660156\n",
            "Epoch 27/100, Loss: 198.21096801757812, Test Loss: 233.3831329345703\n",
            "Epoch 28/100, Loss: 200.86851501464844, Test Loss: 226.84884643554688\n",
            "Epoch 29/100, Loss: 183.54478454589844, Test Loss: 219.52378845214844\n",
            "Epoch 30/100, Loss: 161.42710876464844, Test Loss: 211.1069793701172\n",
            "Epoch 31/100, Loss: 159.0379180908203, Test Loss: 201.62655639648438\n",
            "Epoch 32/100, Loss: 140.074951171875, Test Loss: 190.5413055419922\n",
            "Epoch 33/100, Loss: 139.60446166992188, Test Loss: 178.6554718017578\n",
            "Epoch 34/100, Loss: 117.96964263916016, Test Loss: 164.82489013671875\n",
            "Epoch 35/100, Loss: 112.45690155029297, Test Loss: 150.65431213378906\n",
            "Epoch 36/100, Loss: 102.95368957519531, Test Loss: 135.76600646972656\n",
            "Epoch 37/100, Loss: 79.08615112304688, Test Loss: 119.17689514160156\n",
            "Epoch 38/100, Loss: 68.47406768798828, Test Loss: 102.74501037597656\n",
            "Epoch 39/100, Loss: 62.89337158203125, Test Loss: 86.85265350341797\n",
            "Epoch 40/100, Loss: 42.89567184448242, Test Loss: 71.34326934814453\n",
            "Epoch 41/100, Loss: 41.56943130493164, Test Loss: 57.00422286987305\n",
            "Epoch 42/100, Loss: 27.902660369873047, Test Loss: 43.900367736816406\n",
            "Epoch 43/100, Loss: 16.171794891357422, Test Loss: 32.685638427734375\n",
            "Epoch 44/100, Loss: 13.570505142211914, Test Loss: 23.420269012451172\n",
            "Epoch 45/100, Loss: 9.073189735412598, Test Loss: 16.126338958740234\n",
            "Epoch 46/100, Loss: 7.7026753425598145, Test Loss: 10.8282470703125\n",
            "Epoch 47/100, Loss: 4.558161735534668, Test Loss: 7.198919773101807\n",
            "Epoch 48/100, Loss: 3.0187828540802, Test Loss: 4.954914569854736\n",
            "Epoch 49/100, Loss: 2.465080738067627, Test Loss: 3.5437891483306885\n",
            "Epoch 50/100, Loss: 2.000366687774658, Test Loss: 2.7693824768066406\n",
            "Epoch 51/100, Loss: 1.8638713359832764, Test Loss: 2.280388832092285\n",
            "Epoch 52/100, Loss: 1.4735279083251953, Test Loss: 1.9801616668701172\n",
            "Epoch 53/100, Loss: 1.9186291694641113, Test Loss: 1.7844465970993042\n",
            "Epoch 54/100, Loss: 1.1129635572433472, Test Loss: 1.6074261665344238\n",
            "Epoch 55/100, Loss: 1.6546076536178589, Test Loss: 1.4992719888687134\n",
            "Epoch 56/100, Loss: 1.134467601776123, Test Loss: 1.400091290473938\n",
            "Epoch 57/100, Loss: 1.0022175312042236, Test Loss: 1.3189160823822021\n",
            "Epoch 58/100, Loss: 1.0989123582839966, Test Loss: 1.2538131475448608\n",
            "Epoch 59/100, Loss: 1.0030657052993774, Test Loss: 1.1801058053970337\n",
            "Epoch 60/100, Loss: 0.7740722894668579, Test Loss: 1.147684931755066\n",
            "Epoch 61/100, Loss: 1.0063647031784058, Test Loss: 1.1193568706512451\n",
            "Epoch 62/100, Loss: 0.7910385727882385, Test Loss: 1.08919095993042\n",
            "Epoch 63/100, Loss: 1.125073790550232, Test Loss: 1.0528600215911865\n",
            "Epoch 64/100, Loss: 1.1881656646728516, Test Loss: 1.0155134201049805\n",
            "Epoch 65/100, Loss: 1.060608148574829, Test Loss: 1.0209732055664062\n",
            "Epoch 66/100, Loss: 1.0528966188430786, Test Loss: 1.0025478601455688\n",
            "Epoch 67/100, Loss: 1.249585747718811, Test Loss: 0.986323893070221\n",
            "Epoch 68/100, Loss: 0.884905993938446, Test Loss: 0.9874818921089172\n",
            "Epoch 69/100, Loss: 0.7061347961425781, Test Loss: 0.9719962477684021\n",
            "Epoch 70/100, Loss: 0.6900269985198975, Test Loss: 0.9672291278839111\n",
            "Epoch 71/100, Loss: 0.9388455152511597, Test Loss: 0.9763851761817932\n",
            "Epoch 72/100, Loss: 0.8167948126792908, Test Loss: 0.9678161144256592\n",
            "Epoch 73/100, Loss: 1.0839523077011108, Test Loss: 0.948298990726471\n",
            "Epoch 74/100, Loss: 1.1003905534744263, Test Loss: 0.9345972537994385\n",
            "Epoch 75/100, Loss: 1.116068720817566, Test Loss: 0.9225497841835022\n",
            "Epoch 76/100, Loss: 0.7071378231048584, Test Loss: 0.9335459470748901\n",
            "Epoch 77/100, Loss: 1.0823607444763184, Test Loss: 0.9484928846359253\n",
            "Epoch 78/100, Loss: 0.8984695672988892, Test Loss: 0.9462589025497437\n",
            "Epoch 79/100, Loss: 1.0296494960784912, Test Loss: 0.946870744228363\n",
            "Epoch 80/100, Loss: 0.824572741985321, Test Loss: 0.9390982985496521\n",
            "Epoch 81/100, Loss: 0.900158703327179, Test Loss: 0.9368147850036621\n",
            "Epoch 82/100, Loss: 1.3885999917984009, Test Loss: 0.9275520443916321\n",
            "Epoch 83/100, Loss: 0.7778797149658203, Test Loss: 0.9270323514938354\n",
            "Epoch 84/100, Loss: 0.746538519859314, Test Loss: 0.9131430387496948\n",
            "Epoch 85/100, Loss: 0.6912896633148193, Test Loss: 0.9315775632858276\n",
            "Epoch 86/100, Loss: 0.856235146522522, Test Loss: 0.9393090605735779\n",
            "Epoch 87/100, Loss: 0.6762519478797913, Test Loss: 0.9456213712692261\n",
            "Epoch 88/100, Loss: 0.8011552095413208, Test Loss: 0.9224531054496765\n",
            "Epoch 89/100, Loss: 0.8645962476730347, Test Loss: 0.938492476940155\n",
            "Epoch 90/100, Loss: 0.786499559879303, Test Loss: 0.9302178621292114\n",
            "Epoch 91/100, Loss: 0.8243857622146606, Test Loss: 0.9231902360916138\n",
            "Epoch 92/100, Loss: 0.7255379557609558, Test Loss: 0.9389032125473022\n",
            "Epoch 93/100, Loss: 1.0505064725875854, Test Loss: 0.9470099806785583\n",
            "Epoch 94/100, Loss: 0.795724630355835, Test Loss: 0.9122408032417297\n",
            "Epoch 95/100, Loss: 0.8296367526054382, Test Loss: 0.9138683676719666\n",
            "Epoch 96/100, Loss: 0.9540215134620667, Test Loss: 0.9478060603141785\n",
            "Epoch 97/100, Loss: 0.7374647855758667, Test Loss: 0.9384304285049438\n",
            "Epoch 98/100, Loss: 0.6189193725585938, Test Loss: 0.9094715118408203\n",
            "Epoch 99/100, Loss: 0.9712523818016052, Test Loss: 0.898871898651123\n",
            "Epoch 100/100, Loss: 0.8464834690093994, Test Loss: 0.9260783195495605\n",
            "Final MSE: 0.9260783195495605\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=64, epochs=250\n",
            "Epoch 1/250, Loss: 300.2438049316406, Test Loss: 282.6269836425781\n",
            "Epoch 2/250, Loss: 294.3478698730469, Test Loss: 281.4251403808594\n",
            "Epoch 3/250, Loss: 294.8859558105469, Test Loss: 280.1461486816406\n",
            "Epoch 4/250, Loss: 288.2550964355469, Test Loss: 278.8160095214844\n",
            "Epoch 5/250, Loss: 291.23553466796875, Test Loss: 277.4800720214844\n",
            "Epoch 6/250, Loss: 275.4578857421875, Test Loss: 276.0531005859375\n",
            "Epoch 7/250, Loss: 279.9709167480469, Test Loss: 274.5580749511719\n",
            "Epoch 8/250, Loss: 283.32073974609375, Test Loss: 272.9753723144531\n",
            "Epoch 9/250, Loss: 281.6938171386719, Test Loss: 271.3235778808594\n",
            "Epoch 10/250, Loss: 268.8326110839844, Test Loss: 269.59539794921875\n",
            "Epoch 11/250, Loss: 275.314453125, Test Loss: 267.75567626953125\n",
            "Epoch 12/250, Loss: 266.0343322753906, Test Loss: 265.8029479980469\n",
            "Epoch 13/250, Loss: 271.9710388183594, Test Loss: 263.7457580566406\n",
            "Epoch 14/250, Loss: 266.9026794433594, Test Loss: 261.52288818359375\n",
            "Epoch 15/250, Loss: 264.6187744140625, Test Loss: 259.19329833984375\n",
            "Epoch 16/250, Loss: 253.73548889160156, Test Loss: 256.6872253417969\n",
            "Epoch 17/250, Loss: 246.9987030029297, Test Loss: 253.87405395507812\n",
            "Epoch 18/250, Loss: 241.31138610839844, Test Loss: 250.93251037597656\n",
            "Epoch 19/250, Loss: 235.35865783691406, Test Loss: 247.75074768066406\n",
            "Epoch 20/250, Loss: 229.75399780273438, Test Loss: 244.24087524414062\n",
            "Epoch 21/250, Loss: 235.84231567382812, Test Loss: 240.70220947265625\n",
            "Epoch 22/250, Loss: 216.70095825195312, Test Loss: 236.46421813964844\n",
            "Epoch 23/250, Loss: 207.22390747070312, Test Loss: 232.09873962402344\n",
            "Epoch 24/250, Loss: 200.84490966796875, Test Loss: 227.13026428222656\n",
            "Epoch 25/250, Loss: 202.08567810058594, Test Loss: 221.71435546875\n",
            "Epoch 26/250, Loss: 184.35256958007812, Test Loss: 215.28961181640625\n",
            "Epoch 27/250, Loss: 176.69775390625, Test Loss: 208.5393524169922\n",
            "Epoch 28/250, Loss: 172.7716522216797, Test Loss: 200.69097900390625\n",
            "Epoch 29/250, Loss: 152.13380432128906, Test Loss: 192.0236053466797\n",
            "Epoch 30/250, Loss: 157.20184326171875, Test Loss: 182.5485382080078\n",
            "Epoch 31/250, Loss: 137.2073516845703, Test Loss: 171.64341735839844\n",
            "Epoch 32/250, Loss: 126.39198303222656, Test Loss: 159.6495361328125\n",
            "Epoch 33/250, Loss: 110.86341857910156, Test Loss: 146.8508758544922\n",
            "Epoch 34/250, Loss: 91.7582015991211, Test Loss: 132.4273681640625\n",
            "Epoch 35/250, Loss: 79.3985366821289, Test Loss: 117.33572387695312\n",
            "Epoch 36/250, Loss: 66.46968078613281, Test Loss: 102.50552368164062\n",
            "Epoch 37/250, Loss: 58.70370101928711, Test Loss: 87.65135955810547\n",
            "Epoch 38/250, Loss: 55.99378967285156, Test Loss: 73.02765655517578\n",
            "Epoch 39/250, Loss: 41.02634811401367, Test Loss: 59.089881896972656\n",
            "Epoch 40/250, Loss: 30.64482307434082, Test Loss: 45.90992736816406\n",
            "Epoch 41/250, Loss: 27.648006439208984, Test Loss: 34.409080505371094\n",
            "Epoch 42/250, Loss: 14.99065113067627, Test Loss: 24.525705337524414\n",
            "Epoch 43/250, Loss: 12.149762153625488, Test Loss: 16.96193504333496\n",
            "Epoch 44/250, Loss: 7.933272838592529, Test Loss: 11.02278995513916\n",
            "Epoch 45/250, Loss: 4.596083164215088, Test Loss: 6.901429176330566\n",
            "Epoch 46/250, Loss: 3.1306118965148926, Test Loss: 4.33651876449585\n",
            "Epoch 47/250, Loss: 1.580531358718872, Test Loss: 2.7833569049835205\n",
            "Epoch 48/250, Loss: 1.2227356433868408, Test Loss: 2.0114357471466064\n",
            "Epoch 49/250, Loss: 1.0779821872711182, Test Loss: 1.6508007049560547\n",
            "Epoch 50/250, Loss: 1.2548972368240356, Test Loss: 1.4807331562042236\n",
            "Epoch 51/250, Loss: 1.438007116317749, Test Loss: 1.3761589527130127\n",
            "Epoch 52/250, Loss: 1.1211012601852417, Test Loss: 1.306531548500061\n",
            "Epoch 53/250, Loss: 1.2349364757537842, Test Loss: 1.2471834421157837\n",
            "Epoch 54/250, Loss: 0.7943123579025269, Test Loss: 1.1933788061141968\n",
            "Epoch 55/250, Loss: 1.279191493988037, Test Loss: 1.14250910282135\n",
            "Epoch 56/250, Loss: 1.419600248336792, Test Loss: 1.1267061233520508\n",
            "Epoch 57/250, Loss: 1.3958123922348022, Test Loss: 1.0731201171875\n",
            "Epoch 58/250, Loss: 0.7305006384849548, Test Loss: 1.048150897026062\n",
            "Epoch 59/250, Loss: 0.8483577370643616, Test Loss: 1.0128304958343506\n",
            "Epoch 60/250, Loss: 0.6292668581008911, Test Loss: 1.0032844543457031\n",
            "Epoch 61/250, Loss: 0.8520497679710388, Test Loss: 0.9996671080589294\n",
            "Epoch 62/250, Loss: 0.9133276343345642, Test Loss: 0.9823375940322876\n",
            "Epoch 63/250, Loss: 1.2008801698684692, Test Loss: 0.9896216988563538\n",
            "Epoch 64/250, Loss: 0.9630025029182434, Test Loss: 0.9740002751350403\n",
            "Epoch 65/250, Loss: 0.9234055876731873, Test Loss: 0.9496192336082458\n",
            "Epoch 66/250, Loss: 0.704062283039093, Test Loss: 0.9399862289428711\n",
            "Epoch 67/250, Loss: 0.9021369814872742, Test Loss: 0.9653503894805908\n",
            "Epoch 68/250, Loss: 1.2487739324569702, Test Loss: 0.9547354578971863\n",
            "Epoch 69/250, Loss: 1.1845600605010986, Test Loss: 0.9434798955917358\n",
            "Epoch 70/250, Loss: 1.070729374885559, Test Loss: 0.9574328064918518\n",
            "Epoch 71/250, Loss: 0.9459822177886963, Test Loss: 0.9487096071243286\n",
            "Epoch 72/250, Loss: 1.1137185096740723, Test Loss: 0.9336727261543274\n",
            "Epoch 73/250, Loss: 1.1167407035827637, Test Loss: 0.9308090209960938\n",
            "Epoch 74/250, Loss: 1.0685192346572876, Test Loss: 0.9306060075759888\n",
            "Epoch 75/250, Loss: 0.8466189503669739, Test Loss: 0.942527174949646\n",
            "Epoch 76/250, Loss: 0.7308033108711243, Test Loss: 0.9604739546775818\n",
            "Epoch 77/250, Loss: 0.7808536887168884, Test Loss: 0.9355281591415405\n",
            "Epoch 78/250, Loss: 0.9494063258171082, Test Loss: 0.937581479549408\n",
            "Epoch 79/250, Loss: 0.8407185673713684, Test Loss: 0.935617983341217\n",
            "Epoch 80/250, Loss: 1.1055200099945068, Test Loss: 0.9327206015586853\n",
            "Epoch 81/250, Loss: 0.6740290522575378, Test Loss: 0.9320563077926636\n",
            "Epoch 82/250, Loss: 0.9373260736465454, Test Loss: 0.9366663694381714\n",
            "Epoch 83/250, Loss: 0.7214608788490295, Test Loss: 0.9434955716133118\n",
            "Epoch 84/250, Loss: 1.0625799894332886, Test Loss: 0.9164204597473145\n",
            "Epoch 85/250, Loss: 0.9981234669685364, Test Loss: 0.9178283214569092\n",
            "Epoch 86/250, Loss: 1.0573867559432983, Test Loss: 0.9427063465118408\n",
            "Epoch 87/250, Loss: 0.8096868991851807, Test Loss: 0.9453761577606201\n",
            "Epoch 88/250, Loss: 0.8489006757736206, Test Loss: 0.9183383584022522\n",
            "Epoch 89/250, Loss: 1.2931183576583862, Test Loss: 0.9331653714179993\n",
            "Epoch 90/250, Loss: 0.7592352032661438, Test Loss: 0.9192643761634827\n",
            "Epoch 91/250, Loss: 0.6607896685600281, Test Loss: 0.9310961365699768\n",
            "Epoch 92/250, Loss: 0.7438591718673706, Test Loss: 0.9232324361801147\n",
            "Epoch 93/250, Loss: 0.9309160113334656, Test Loss: 0.9055395722389221\n",
            "Epoch 94/250, Loss: 0.8072322010993958, Test Loss: 0.9251917600631714\n",
            "Epoch 95/250, Loss: 0.628212034702301, Test Loss: 0.9497420191764832\n",
            "Epoch 96/250, Loss: 0.8561360239982605, Test Loss: 0.9320225715637207\n",
            "Epoch 97/250, Loss: 0.8534637689590454, Test Loss: 0.9076372981071472\n",
            "Epoch 98/250, Loss: 0.8902524709701538, Test Loss: 0.9088385105133057\n",
            "Epoch 99/250, Loss: 1.0370285511016846, Test Loss: 0.9425654411315918\n",
            "Epoch 100/250, Loss: 0.9348794221878052, Test Loss: 0.9298804402351379\n",
            "Epoch 101/250, Loss: 0.7800413370132446, Test Loss: 0.9255877137184143\n",
            "Epoch 102/250, Loss: 0.9093273282051086, Test Loss: 0.9341951012611389\n",
            "Epoch 103/250, Loss: 1.2618942260742188, Test Loss: 0.9407507181167603\n",
            "Epoch 104/250, Loss: 0.8731745481491089, Test Loss: 0.9160409569740295\n",
            "Epoch 105/250, Loss: 0.91221022605896, Test Loss: 0.912609338760376\n",
            "Epoch 106/250, Loss: 0.8765465021133423, Test Loss: 0.9224174618721008\n",
            "Epoch 107/250, Loss: 1.0409444570541382, Test Loss: 0.945807933807373\n",
            "Epoch 108/250, Loss: 1.1282219886779785, Test Loss: 0.9199194312095642\n",
            "Epoch 109/250, Loss: 0.8905932903289795, Test Loss: 0.8997049331665039\n",
            "Epoch 110/250, Loss: 0.8922885060310364, Test Loss: 0.9386140704154968\n",
            "Epoch 111/250, Loss: 1.007149577140808, Test Loss: 0.9392027854919434\n",
            "Epoch 112/250, Loss: 0.9738196730613708, Test Loss: 0.9057161211967468\n",
            "Epoch 113/250, Loss: 1.0725364685058594, Test Loss: 0.91054368019104\n",
            "Epoch 114/250, Loss: 0.7031128406524658, Test Loss: 0.9356362819671631\n",
            "Epoch 115/250, Loss: 1.0003267526626587, Test Loss: 0.9374520778656006\n",
            "Epoch 116/250, Loss: 0.877724826335907, Test Loss: 0.9313129186630249\n",
            "Epoch 117/250, Loss: 1.163201093673706, Test Loss: 0.9000510573387146\n",
            "Epoch 118/250, Loss: 0.9886003732681274, Test Loss: 0.9294673204421997\n",
            "Epoch 119/250, Loss: 0.7495462894439697, Test Loss: 0.9386202692985535\n",
            "Epoch 120/250, Loss: 0.9785324335098267, Test Loss: 0.9291447401046753\n",
            "Epoch 121/250, Loss: 1.0672527551651, Test Loss: 0.9245680570602417\n",
            "Epoch 122/250, Loss: 0.7531315088272095, Test Loss: 0.9199816584587097\n",
            "Epoch 123/250, Loss: 0.950289785861969, Test Loss: 0.931830883026123\n",
            "Epoch 124/250, Loss: 0.7971300482749939, Test Loss: 0.9229915738105774\n",
            "Epoch 125/250, Loss: 0.6263722777366638, Test Loss: 0.9344559907913208\n",
            "Epoch 126/250, Loss: 1.0630779266357422, Test Loss: 0.9161105751991272\n",
            "Epoch 127/250, Loss: 1.0062990188598633, Test Loss: 0.9230400919914246\n",
            "Epoch 128/250, Loss: 1.0580413341522217, Test Loss: 0.9078672528266907\n",
            "Epoch 129/250, Loss: 1.1804697513580322, Test Loss: 0.9206198453903198\n",
            "Epoch 130/250, Loss: 0.7512499094009399, Test Loss: 0.932562530040741\n",
            "Epoch 131/250, Loss: 0.8908241391181946, Test Loss: 0.9232913851737976\n",
            "Epoch 132/250, Loss: 0.8782145977020264, Test Loss: 0.9324211478233337\n",
            "Epoch 133/250, Loss: 0.6487024426460266, Test Loss: 0.9301868081092834\n",
            "Epoch 134/250, Loss: 0.8095510005950928, Test Loss: 0.9240990877151489\n",
            "Epoch 135/250, Loss: 0.7702932357788086, Test Loss: 0.9079250693321228\n",
            "Epoch 136/250, Loss: 1.123043417930603, Test Loss: 0.9099503755569458\n",
            "Epoch 137/250, Loss: 0.9240929484367371, Test Loss: 0.9481023550033569\n",
            "Epoch 138/250, Loss: 0.8259360790252686, Test Loss: 0.9249464869499207\n",
            "Epoch 139/250, Loss: 0.8654528260231018, Test Loss: 0.9173789620399475\n",
            "Epoch 140/250, Loss: 1.0634775161743164, Test Loss: 0.926018238067627\n",
            "Epoch 141/250, Loss: 1.2144578695297241, Test Loss: 0.9265616536140442\n",
            "Epoch 142/250, Loss: 0.8975475430488586, Test Loss: 0.9211240410804749\n",
            "Epoch 143/250, Loss: 0.9219513535499573, Test Loss: 0.9254779815673828\n",
            "Epoch 144/250, Loss: 0.7568405866622925, Test Loss: 0.9230213165283203\n",
            "Epoch 145/250, Loss: 1.166385293006897, Test Loss: 0.9385150074958801\n",
            "Epoch 146/250, Loss: 0.9602517485618591, Test Loss: 0.9302321076393127\n",
            "Epoch 147/250, Loss: 0.7168683409690857, Test Loss: 0.9308755397796631\n",
            "Epoch 148/250, Loss: 1.22880220413208, Test Loss: 0.9162957072257996\n",
            "Epoch 149/250, Loss: 0.9810798764228821, Test Loss: 0.9221588373184204\n",
            "Epoch 150/250, Loss: 1.007407784461975, Test Loss: 0.8971670866012573\n",
            "Epoch 151/250, Loss: 1.138129472732544, Test Loss: 0.9285387396812439\n",
            "Epoch 152/250, Loss: 1.1143580675125122, Test Loss: 0.9194158911705017\n",
            "Epoch 153/250, Loss: 0.7756739258766174, Test Loss: 0.9302667379379272\n",
            "Epoch 154/250, Loss: 0.9634735584259033, Test Loss: 0.9196107387542725\n",
            "Epoch 155/250, Loss: 0.7056366205215454, Test Loss: 0.927927553653717\n",
            "Epoch 156/250, Loss: 1.2123478651046753, Test Loss: 0.9291151762008667\n",
            "Epoch 157/250, Loss: 0.8961737751960754, Test Loss: 0.9075773358345032\n",
            "Epoch 158/250, Loss: 0.7892602682113647, Test Loss: 0.9158916473388672\n",
            "Epoch 159/250, Loss: 1.1596273183822632, Test Loss: 0.9240157008171082\n",
            "Epoch 160/250, Loss: 0.9796348214149475, Test Loss: 0.9542262554168701\n",
            "Epoch 161/250, Loss: 0.7955251336097717, Test Loss: 0.9347198009490967\n",
            "Epoch 162/250, Loss: 1.180874228477478, Test Loss: 0.915827214717865\n",
            "Epoch 163/250, Loss: 1.1086103916168213, Test Loss: 0.9150186777114868\n",
            "Epoch 164/250, Loss: 0.9497899413108826, Test Loss: 0.936316967010498\n",
            "Epoch 165/250, Loss: 1.1086736917495728, Test Loss: 0.9339752197265625\n",
            "Epoch 166/250, Loss: 1.0803112983703613, Test Loss: 0.9074065685272217\n",
            "Epoch 167/250, Loss: 1.106627106666565, Test Loss: 0.9181274175643921\n",
            "Epoch 168/250, Loss: 0.9689247012138367, Test Loss: 0.9515905380249023\n",
            "Epoch 169/250, Loss: 0.9531792998313904, Test Loss: 0.9521081447601318\n",
            "Epoch 170/250, Loss: 0.9315704107284546, Test Loss: 0.9063790440559387\n",
            "Epoch 171/250, Loss: 1.0591824054718018, Test Loss: 0.897194504737854\n",
            "Epoch 172/250, Loss: 1.1522719860076904, Test Loss: 0.9546815156936646\n",
            "Epoch 173/250, Loss: 0.9000142812728882, Test Loss: 0.9241988658905029\n",
            "Epoch 174/250, Loss: 1.3369086980819702, Test Loss: 0.924136757850647\n",
            "Epoch 175/250, Loss: 1.2533729076385498, Test Loss: 0.9634424448013306\n",
            "Epoch 176/250, Loss: 0.7758849859237671, Test Loss: 0.928466796875\n",
            "Epoch 177/250, Loss: 0.8978517651557922, Test Loss: 0.9041597247123718\n",
            "Epoch 178/250, Loss: 0.987317681312561, Test Loss: 0.9245255589485168\n",
            "Epoch 179/250, Loss: 0.7747008800506592, Test Loss: 0.920983612537384\n",
            "Epoch 180/250, Loss: 0.8027124404907227, Test Loss: 0.9150694012641907\n",
            "Epoch 181/250, Loss: 0.9442103505134583, Test Loss: 0.9401901960372925\n",
            "Epoch 182/250, Loss: 1.1419836282730103, Test Loss: 0.9383289813995361\n",
            "Epoch 183/250, Loss: 0.6953557133674622, Test Loss: 0.9294016361236572\n",
            "Epoch 184/250, Loss: 1.2556736469268799, Test Loss: 0.9245004057884216\n",
            "Epoch 185/250, Loss: 0.7709282040596008, Test Loss: 0.9173082709312439\n",
            "Epoch 186/250, Loss: 0.9806368350982666, Test Loss: 0.9160776138305664\n",
            "Epoch 187/250, Loss: 0.9699601531028748, Test Loss: 0.9064713716506958\n",
            "Epoch 188/250, Loss: 1.4656153917312622, Test Loss: 0.9156739711761475\n",
            "Epoch 189/250, Loss: 1.005004644393921, Test Loss: 0.9385975003242493\n",
            "Epoch 190/250, Loss: 1.0559189319610596, Test Loss: 0.9350936412811279\n",
            "Epoch 191/250, Loss: 1.0463000535964966, Test Loss: 0.9515038728713989\n",
            "Epoch 192/250, Loss: 0.881161630153656, Test Loss: 0.9571835398674011\n",
            "Epoch 193/250, Loss: 0.8177655935287476, Test Loss: 0.8947330117225647\n",
            "Epoch 194/250, Loss: 0.9380075335502625, Test Loss: 0.920254647731781\n",
            "Epoch 195/250, Loss: 0.9195008873939514, Test Loss: 0.9458243250846863\n",
            "Epoch 196/250, Loss: 0.6802904009819031, Test Loss: 0.9146368503570557\n",
            "Epoch 197/250, Loss: 1.1205251216888428, Test Loss: 0.9306126832962036\n",
            "Epoch 198/250, Loss: 0.7484799027442932, Test Loss: 0.9103964567184448\n",
            "Epoch 199/250, Loss: 0.7252367734909058, Test Loss: 0.9430909156799316\n",
            "Epoch 200/250, Loss: 0.8649471402168274, Test Loss: 0.9249417185783386\n",
            "Epoch 201/250, Loss: 1.068865418434143, Test Loss: 0.9034856557846069\n",
            "Epoch 202/250, Loss: 0.7295485734939575, Test Loss: 0.928346574306488\n",
            "Epoch 203/250, Loss: 1.2096699476242065, Test Loss: 0.9666593670845032\n",
            "Epoch 204/250, Loss: 0.6466065049171448, Test Loss: 0.9249963760375977\n",
            "Epoch 205/250, Loss: 1.4068584442138672, Test Loss: 0.9160704016685486\n",
            "Epoch 206/250, Loss: 1.1634635925292969, Test Loss: 0.9411178827285767\n",
            "Epoch 207/250, Loss: 0.8680688142776489, Test Loss: 0.9180775880813599\n",
            "Epoch 208/250, Loss: 1.2766773700714111, Test Loss: 0.9073813557624817\n",
            "Epoch 209/250, Loss: 1.5815216302871704, Test Loss: 0.9477140307426453\n",
            "Epoch 210/250, Loss: 1.3482215404510498, Test Loss: 0.9205290675163269\n",
            "Epoch 211/250, Loss: 1.2104853391647339, Test Loss: 0.9167326092720032\n",
            "Epoch 212/250, Loss: 1.0550931692123413, Test Loss: 0.9291921854019165\n",
            "Epoch 213/250, Loss: 1.4100754261016846, Test Loss: 0.9323685169219971\n",
            "Epoch 214/250, Loss: 0.7285255789756775, Test Loss: 0.9275827407836914\n",
            "Epoch 215/250, Loss: 0.910305380821228, Test Loss: 0.9218134880065918\n",
            "Epoch 216/250, Loss: 0.8178519010543823, Test Loss: 0.931094765663147\n",
            "Epoch 217/250, Loss: 1.0532418489456177, Test Loss: 0.948869526386261\n",
            "Epoch 218/250, Loss: 0.8894799947738647, Test Loss: 0.9281154870986938\n",
            "Epoch 219/250, Loss: 1.0150468349456787, Test Loss: 0.9061412811279297\n",
            "Epoch 220/250, Loss: 1.2784239053726196, Test Loss: 0.944783627986908\n",
            "Epoch 221/250, Loss: 0.8337389826774597, Test Loss: 0.937362015247345\n",
            "Epoch 222/250, Loss: 0.7694224119186401, Test Loss: 0.9167869687080383\n",
            "Epoch 223/250, Loss: 0.6221747994422913, Test Loss: 0.9046975374221802\n",
            "Epoch 224/250, Loss: 1.0010589361190796, Test Loss: 0.9262987971305847\n",
            "Epoch 225/250, Loss: 0.8296151757240295, Test Loss: 0.9092814922332764\n",
            "Epoch 226/250, Loss: 0.7155802845954895, Test Loss: 0.9405813217163086\n",
            "Epoch 227/250, Loss: 1.2828633785247803, Test Loss: 0.9356558918952942\n",
            "Epoch 228/250, Loss: 1.070349097251892, Test Loss: 0.9020392298698425\n",
            "Epoch 229/250, Loss: 0.8141080737113953, Test Loss: 0.9382266402244568\n",
            "Epoch 230/250, Loss: 1.160427451133728, Test Loss: 0.9398552775382996\n",
            "Epoch 231/250, Loss: 1.0010710954666138, Test Loss: 0.9128477573394775\n",
            "Epoch 232/250, Loss: 0.9618815183639526, Test Loss: 0.9345656037330627\n",
            "Epoch 233/250, Loss: 0.8587434887886047, Test Loss: 0.9338805079460144\n",
            "Epoch 234/250, Loss: 0.9442042112350464, Test Loss: 0.92160564661026\n",
            "Epoch 235/250, Loss: 0.8950158357620239, Test Loss: 0.9344609975814819\n",
            "Epoch 236/250, Loss: 0.7982008457183838, Test Loss: 0.9360519647598267\n",
            "Epoch 237/250, Loss: 0.7108151316642761, Test Loss: 0.911167562007904\n",
            "Epoch 238/250, Loss: 0.9373190999031067, Test Loss: 0.9157420992851257\n",
            "Epoch 239/250, Loss: 1.0096514225006104, Test Loss: 0.9241199493408203\n",
            "Epoch 240/250, Loss: 0.9410606026649475, Test Loss: 0.934144139289856\n",
            "Epoch 241/250, Loss: 1.170376181602478, Test Loss: 0.9340890049934387\n",
            "Epoch 242/250, Loss: 1.3224945068359375, Test Loss: 0.9397332072257996\n",
            "Epoch 243/250, Loss: 1.370008111000061, Test Loss: 0.909086287021637\n",
            "Epoch 244/250, Loss: 1.0968352556228638, Test Loss: 0.9233133792877197\n",
            "Epoch 245/250, Loss: 1.1405518054962158, Test Loss: 0.9251705408096313\n",
            "Epoch 246/250, Loss: 0.9649150371551514, Test Loss: 0.9247459769248962\n",
            "Epoch 247/250, Loss: 0.9143856167793274, Test Loss: 0.945970356464386\n",
            "Epoch 248/250, Loss: 0.9604276418685913, Test Loss: 0.9165955781936646\n",
            "Epoch 249/250, Loss: 0.6512691974639893, Test Loss: 0.9295014142990112\n",
            "Epoch 250/250, Loss: 1.0960805416107178, Test Loss: 0.9149224162101746\n",
            "Final MSE: 0.914922297000885\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=128, epochs=1\n",
            "Epoch 1/1, Loss: 279.0137023925781, Test Loss: 270.4563293457031\n",
            "Final MSE: 270.456298828125\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=128, epochs=10\n",
            "Epoch 1/10, Loss: 285.63214111328125, Test Loss: 281.47540283203125\n",
            "Epoch 2/10, Loss: 290.0264587402344, Test Loss: 281.0409851074219\n",
            "Epoch 3/10, Loss: 303.4563293457031, Test Loss: 280.59747314453125\n",
            "Epoch 4/10, Loss: 291.7842712402344, Test Loss: 280.15252685546875\n",
            "Epoch 5/10, Loss: 286.40020751953125, Test Loss: 279.7015380859375\n",
            "Epoch 6/10, Loss: 287.4455871582031, Test Loss: 279.2488098144531\n",
            "Epoch 7/10, Loss: 288.91217041015625, Test Loss: 278.7838134765625\n",
            "Epoch 8/10, Loss: 285.6635437011719, Test Loss: 278.3082580566406\n",
            "Epoch 9/10, Loss: 278.0438232421875, Test Loss: 277.82159423828125\n",
            "Epoch 10/10, Loss: 278.36187744140625, Test Loss: 277.3235778808594\n",
            "Final MSE: 277.3235168457031\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=128, epochs=25\n",
            "Epoch 1/25, Loss: 279.9508972167969, Test Loss: 276.9655456542969\n",
            "Epoch 2/25, Loss: 284.7391357421875, Test Loss: 276.5620422363281\n",
            "Epoch 3/25, Loss: 288.75885009765625, Test Loss: 276.15191650390625\n",
            "Epoch 4/25, Loss: 284.72210693359375, Test Loss: 275.73931884765625\n",
            "Epoch 5/25, Loss: 283.1583251953125, Test Loss: 275.31866455078125\n",
            "Epoch 6/25, Loss: 280.80780029296875, Test Loss: 274.8979187011719\n",
            "Epoch 7/25, Loss: 282.5404357910156, Test Loss: 274.47393798828125\n",
            "Epoch 8/25, Loss: 272.6338806152344, Test Loss: 274.0356750488281\n",
            "Epoch 9/25, Loss: 282.58929443359375, Test Loss: 273.59844970703125\n",
            "Epoch 10/25, Loss: 282.9898376464844, Test Loss: 273.1562194824219\n",
            "Epoch 11/25, Loss: 274.0085144042969, Test Loss: 272.6916198730469\n",
            "Epoch 12/25, Loss: 284.90240478515625, Test Loss: 272.2139587402344\n",
            "Epoch 13/25, Loss: 290.4132995605469, Test Loss: 271.7206115722656\n",
            "Epoch 14/25, Loss: 270.2539978027344, Test Loss: 271.2195739746094\n",
            "Epoch 15/25, Loss: 288.0409851074219, Test Loss: 270.695556640625\n",
            "Epoch 16/25, Loss: 273.5576477050781, Test Loss: 270.1513671875\n",
            "Epoch 17/25, Loss: 280.6094055175781, Test Loss: 269.5969543457031\n",
            "Epoch 18/25, Loss: 267.6779479980469, Test Loss: 269.02532958984375\n",
            "Epoch 19/25, Loss: 271.9707336425781, Test Loss: 268.4291687011719\n",
            "Epoch 20/25, Loss: 263.1497497558594, Test Loss: 267.81982421875\n",
            "Epoch 21/25, Loss: 275.5594482421875, Test Loss: 267.1735534667969\n",
            "Epoch 22/25, Loss: 267.10260009765625, Test Loss: 266.5164794921875\n",
            "Epoch 23/25, Loss: 267.6869812011719, Test Loss: 265.8509216308594\n",
            "Epoch 24/25, Loss: 265.9747314453125, Test Loss: 265.1411437988281\n",
            "Epoch 25/25, Loss: 260.1695556640625, Test Loss: 264.3744201660156\n",
            "Final MSE: 264.3744201660156\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=128, epochs=50\n",
            "Epoch 1/50, Loss: 294.87908935546875, Test Loss: 282.1886901855469\n",
            "Epoch 2/50, Loss: 290.44830322265625, Test Loss: 281.5931701660156\n",
            "Epoch 3/50, Loss: 284.4721984863281, Test Loss: 281.01251220703125\n",
            "Epoch 4/50, Loss: 285.97186279296875, Test Loss: 280.42962646484375\n",
            "Epoch 5/50, Loss: 282.0589294433594, Test Loss: 279.8659973144531\n",
            "Epoch 6/50, Loss: 283.8758239746094, Test Loss: 279.2828674316406\n",
            "Epoch 7/50, Loss: 280.8456115722656, Test Loss: 278.7110595703125\n",
            "Epoch 8/50, Loss: 295.89227294921875, Test Loss: 278.13995361328125\n",
            "Epoch 9/50, Loss: 280.69415283203125, Test Loss: 277.5678405761719\n",
            "Epoch 10/50, Loss: 275.1221923828125, Test Loss: 276.9989013671875\n",
            "Epoch 11/50, Loss: 273.7745056152344, Test Loss: 276.4319763183594\n",
            "Epoch 12/50, Loss: 289.19268798828125, Test Loss: 275.87664794921875\n",
            "Epoch 13/50, Loss: 278.1354675292969, Test Loss: 275.336181640625\n",
            "Epoch 14/50, Loss: 277.3347473144531, Test Loss: 274.80462646484375\n",
            "Epoch 15/50, Loss: 275.4955749511719, Test Loss: 274.28302001953125\n",
            "Epoch 16/50, Loss: 285.35528564453125, Test Loss: 273.7483825683594\n",
            "Epoch 17/50, Loss: 272.3401184082031, Test Loss: 273.228271484375\n",
            "Epoch 18/50, Loss: 275.0242004394531, Test Loss: 272.7068176269531\n",
            "Epoch 19/50, Loss: 273.26873779296875, Test Loss: 272.1939392089844\n",
            "Epoch 20/50, Loss: 270.40948486328125, Test Loss: 271.664794921875\n",
            "Epoch 21/50, Loss: 275.0844421386719, Test Loss: 271.1207275390625\n",
            "Epoch 22/50, Loss: 273.65740966796875, Test Loss: 270.5942687988281\n",
            "Epoch 23/50, Loss: 277.5447692871094, Test Loss: 270.05255126953125\n",
            "Epoch 24/50, Loss: 273.7380065917969, Test Loss: 269.5016174316406\n",
            "Epoch 25/50, Loss: 267.65673828125, Test Loss: 268.95556640625\n",
            "Epoch 26/50, Loss: 274.47955322265625, Test Loss: 268.3801574707031\n",
            "Epoch 27/50, Loss: 270.72515869140625, Test Loss: 267.77520751953125\n",
            "Epoch 28/50, Loss: 257.92462158203125, Test Loss: 267.1368408203125\n",
            "Epoch 29/50, Loss: 258.78765869140625, Test Loss: 266.5206298828125\n",
            "Epoch 30/50, Loss: 258.7828063964844, Test Loss: 265.9122314453125\n",
            "Epoch 31/50, Loss: 246.59738159179688, Test Loss: 265.2483825683594\n",
            "Epoch 32/50, Loss: 255.75999450683594, Test Loss: 264.61541748046875\n",
            "Epoch 33/50, Loss: 254.95596313476562, Test Loss: 263.89471435546875\n",
            "Epoch 34/50, Loss: 248.85035705566406, Test Loss: 263.10516357421875\n",
            "Epoch 35/50, Loss: 241.63137817382812, Test Loss: 262.2015075683594\n",
            "Epoch 36/50, Loss: 237.89117431640625, Test Loss: 261.3374328613281\n",
            "Epoch 37/50, Loss: 229.66224670410156, Test Loss: 260.53533935546875\n",
            "Epoch 38/50, Loss: 239.60023498535156, Test Loss: 259.68572998046875\n",
            "Epoch 39/50, Loss: 239.42523193359375, Test Loss: 258.5989074707031\n",
            "Epoch 40/50, Loss: 229.55831909179688, Test Loss: 257.333984375\n",
            "Epoch 41/50, Loss: 227.55625915527344, Test Loss: 255.90341186523438\n",
            "Epoch 42/50, Loss: 215.75082397460938, Test Loss: 254.27821350097656\n",
            "Epoch 43/50, Loss: 222.93936157226562, Test Loss: 252.64230346679688\n",
            "Epoch 44/50, Loss: 222.4127655029297, Test Loss: 250.781494140625\n",
            "Epoch 45/50, Loss: 213.00759887695312, Test Loss: 248.58004760742188\n",
            "Epoch 46/50, Loss: 199.8087615966797, Test Loss: 246.2610321044922\n",
            "Epoch 47/50, Loss: 220.17771911621094, Test Loss: 243.6568145751953\n",
            "Epoch 48/50, Loss: 215.12510681152344, Test Loss: 240.29010009765625\n",
            "Epoch 49/50, Loss: 185.4404296875, Test Loss: 236.32725524902344\n",
            "Epoch 50/50, Loss: 186.11302185058594, Test Loss: 232.0298614501953\n",
            "Final MSE: 232.0298614501953\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=128, epochs=100\n",
            "Epoch 1/100, Loss: 263.88299560546875, Test Loss: 261.9331970214844\n",
            "Epoch 2/100, Loss: 273.9282531738281, Test Loss: 261.60748291015625\n",
            "Epoch 3/100, Loss: 272.9192199707031, Test Loss: 261.2766418457031\n",
            "Epoch 4/100, Loss: 266.60455322265625, Test Loss: 260.9469299316406\n",
            "Epoch 5/100, Loss: 266.7239074707031, Test Loss: 260.6217041015625\n",
            "Epoch 6/100, Loss: 272.77496337890625, Test Loss: 260.3001708984375\n",
            "Epoch 7/100, Loss: 264.6005859375, Test Loss: 259.978515625\n",
            "Epoch 8/100, Loss: 268.0834655761719, Test Loss: 259.6625671386719\n",
            "Epoch 9/100, Loss: 271.2375793457031, Test Loss: 259.3480224609375\n",
            "Epoch 10/100, Loss: 263.1656799316406, Test Loss: 259.03448486328125\n",
            "Epoch 11/100, Loss: 259.81036376953125, Test Loss: 258.72088623046875\n",
            "Epoch 12/100, Loss: 269.5649108886719, Test Loss: 258.405029296875\n",
            "Epoch 13/100, Loss: 271.2205505371094, Test Loss: 258.0849914550781\n",
            "Epoch 14/100, Loss: 262.11834716796875, Test Loss: 257.7633361816406\n",
            "Epoch 15/100, Loss: 266.9696960449219, Test Loss: 257.43536376953125\n",
            "Epoch 16/100, Loss: 255.987548828125, Test Loss: 257.0992736816406\n",
            "Epoch 17/100, Loss: 261.7789306640625, Test Loss: 256.752197265625\n",
            "Epoch 18/100, Loss: 268.58660888671875, Test Loss: 256.396240234375\n",
            "Epoch 19/100, Loss: 265.51422119140625, Test Loss: 256.0249328613281\n",
            "Epoch 20/100, Loss: 258.4867858886719, Test Loss: 255.63320922851562\n",
            "Epoch 21/100, Loss: 260.39068603515625, Test Loss: 255.22174072265625\n",
            "Epoch 22/100, Loss: 261.7254943847656, Test Loss: 254.79457092285156\n",
            "Epoch 23/100, Loss: 265.8023986816406, Test Loss: 254.33187866210938\n",
            "Epoch 24/100, Loss: 259.0808410644531, Test Loss: 253.84593200683594\n",
            "Epoch 25/100, Loss: 261.2945861816406, Test Loss: 253.32522583007812\n",
            "Epoch 26/100, Loss: 253.9905548095703, Test Loss: 252.7636260986328\n",
            "Epoch 27/100, Loss: 262.6854248046875, Test Loss: 252.15087890625\n",
            "Epoch 28/100, Loss: 253.8401641845703, Test Loss: 251.5211944580078\n",
            "Epoch 29/100, Loss: 258.2610168457031, Test Loss: 250.85739135742188\n",
            "Epoch 30/100, Loss: 255.81375122070312, Test Loss: 250.1835479736328\n",
            "Epoch 31/100, Loss: 241.00164794921875, Test Loss: 249.48226928710938\n",
            "Epoch 32/100, Loss: 246.4358367919922, Test Loss: 248.73875427246094\n",
            "Epoch 33/100, Loss: 250.66693115234375, Test Loss: 247.96563720703125\n",
            "Epoch 34/100, Loss: 242.1427764892578, Test Loss: 247.0860595703125\n",
            "Epoch 35/100, Loss: 246.4073944091797, Test Loss: 246.18812561035156\n",
            "Epoch 36/100, Loss: 234.6375274658203, Test Loss: 245.2008819580078\n",
            "Epoch 37/100, Loss: 235.59254455566406, Test Loss: 244.1555633544922\n",
            "Epoch 38/100, Loss: 231.84300231933594, Test Loss: 243.12747192382812\n",
            "Epoch 39/100, Loss: 242.23948669433594, Test Loss: 241.9611358642578\n",
            "Epoch 40/100, Loss: 230.3447723388672, Test Loss: 240.6580810546875\n",
            "Epoch 41/100, Loss: 218.06553649902344, Test Loss: 239.2572479248047\n",
            "Epoch 42/100, Loss: 216.1683807373047, Test Loss: 237.8594512939453\n",
            "Epoch 43/100, Loss: 212.10031127929688, Test Loss: 236.33047485351562\n",
            "Epoch 44/100, Loss: 206.9393768310547, Test Loss: 234.7028045654297\n",
            "Epoch 45/100, Loss: 216.93824768066406, Test Loss: 232.85733032226562\n",
            "Epoch 46/100, Loss: 195.67282104492188, Test Loss: 230.57557678222656\n",
            "Epoch 47/100, Loss: 202.62258911132812, Test Loss: 228.18772888183594\n",
            "Epoch 48/100, Loss: 193.917236328125, Test Loss: 225.36643981933594\n",
            "Epoch 49/100, Loss: 186.32484436035156, Test Loss: 222.22157287597656\n",
            "Epoch 50/100, Loss: 188.91436767578125, Test Loss: 218.7639923095703\n",
            "Epoch 51/100, Loss: 160.89865112304688, Test Loss: 214.52642822265625\n",
            "Epoch 52/100, Loss: 154.17129516601562, Test Loss: 209.97647094726562\n",
            "Epoch 53/100, Loss: 144.2957000732422, Test Loss: 204.9237060546875\n",
            "Epoch 54/100, Loss: 151.92922973632812, Test Loss: 199.76206970214844\n",
            "Epoch 55/100, Loss: 138.30072021484375, Test Loss: 193.95968627929688\n",
            "Epoch 56/100, Loss: 146.905517578125, Test Loss: 187.8385772705078\n",
            "Epoch 57/100, Loss: 119.2460708618164, Test Loss: 180.83572387695312\n",
            "Epoch 58/100, Loss: 129.11672973632812, Test Loss: 173.35049438476562\n",
            "Epoch 59/100, Loss: 117.65513610839844, Test Loss: 165.0999755859375\n",
            "Epoch 60/100, Loss: 126.25899505615234, Test Loss: 156.12393188476562\n",
            "Epoch 61/100, Loss: 108.26264953613281, Test Loss: 146.19593811035156\n",
            "Epoch 62/100, Loss: 81.85546875, Test Loss: 135.9026336669922\n",
            "Epoch 63/100, Loss: 87.48595428466797, Test Loss: 125.5939712524414\n",
            "Epoch 64/100, Loss: 67.22482299804688, Test Loss: 114.83101654052734\n",
            "Epoch 65/100, Loss: 71.89594268798828, Test Loss: 104.24192810058594\n",
            "Epoch 66/100, Loss: 69.85692596435547, Test Loss: 93.8436050415039\n",
            "Epoch 67/100, Loss: 58.307472229003906, Test Loss: 83.15987396240234\n",
            "Epoch 68/100, Loss: 47.04071807861328, Test Loss: 72.71419525146484\n",
            "Epoch 69/100, Loss: 46.55438232421875, Test Loss: 62.75603103637695\n",
            "Epoch 70/100, Loss: 33.22589111328125, Test Loss: 53.33370590209961\n",
            "Epoch 71/100, Loss: 27.703569412231445, Test Loss: 44.402591705322266\n",
            "Epoch 72/100, Loss: 20.538166046142578, Test Loss: 36.314823150634766\n",
            "Epoch 73/100, Loss: 18.66636085510254, Test Loss: 29.288545608520508\n",
            "Epoch 74/100, Loss: 16.658082962036133, Test Loss: 23.32376480102539\n",
            "Epoch 75/100, Loss: 12.488876342773438, Test Loss: 18.213171005249023\n",
            "Epoch 76/100, Loss: 9.733316421508789, Test Loss: 13.899909973144531\n",
            "Epoch 77/100, Loss: 7.336406230926514, Test Loss: 10.38080883026123\n",
            "Epoch 78/100, Loss: 5.293027400970459, Test Loss: 7.628941059112549\n",
            "Epoch 79/100, Loss: 3.1987147331237793, Test Loss: 5.5549163818359375\n",
            "Epoch 80/100, Loss: 2.557427406311035, Test Loss: 4.083625793457031\n",
            "Epoch 81/100, Loss: 1.9245818853378296, Test Loss: 3.082742214202881\n",
            "Epoch 82/100, Loss: 2.084028720855713, Test Loss: 2.4256973266601562\n",
            "Epoch 83/100, Loss: 1.7050071954727173, Test Loss: 1.9974879026412964\n",
            "Epoch 84/100, Loss: 1.2026264667510986, Test Loss: 1.7451815605163574\n",
            "Epoch 85/100, Loss: 1.16213059425354, Test Loss: 1.559121012687683\n",
            "Epoch 86/100, Loss: 1.451106309890747, Test Loss: 1.439501404762268\n",
            "Epoch 87/100, Loss: 1.1213639974594116, Test Loss: 1.3774161338806152\n",
            "Epoch 88/100, Loss: 1.2432414293289185, Test Loss: 1.3285690546035767\n",
            "Epoch 89/100, Loss: 0.9474483132362366, Test Loss: 1.2559936046600342\n",
            "Epoch 90/100, Loss: 1.1651535034179688, Test Loss: 1.1928513050079346\n",
            "Epoch 91/100, Loss: 1.2434883117675781, Test Loss: 1.1400541067123413\n",
            "Epoch 92/100, Loss: 1.1946171522140503, Test Loss: 1.115964412689209\n",
            "Epoch 93/100, Loss: 1.0804592370986938, Test Loss: 1.1032299995422363\n",
            "Epoch 94/100, Loss: 0.8384568691253662, Test Loss: 1.1001670360565186\n",
            "Epoch 95/100, Loss: 0.7966420650482178, Test Loss: 1.0804752111434937\n",
            "Epoch 96/100, Loss: 0.81545490026474, Test Loss: 1.0471596717834473\n",
            "Epoch 97/100, Loss: 0.8893644213676453, Test Loss: 1.03023362159729\n",
            "Epoch 98/100, Loss: 0.9326737523078918, Test Loss: 1.0195305347442627\n",
            "Epoch 99/100, Loss: 1.0099889039993286, Test Loss: 1.0091547966003418\n",
            "Epoch 100/100, Loss: 0.868663489818573, Test Loss: 0.9972700476646423\n",
            "Final MSE: 0.9972699284553528\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=128, epochs=250\n",
            "Epoch 1/250, Loss: 282.70587158203125, Test Loss: 275.9420471191406\n",
            "Epoch 2/250, Loss: 288.35357666015625, Test Loss: 275.5255126953125\n",
            "Epoch 3/250, Loss: 285.9380187988281, Test Loss: 275.096923828125\n",
            "Epoch 4/250, Loss: 284.6018371582031, Test Loss: 274.66925048828125\n",
            "Epoch 5/250, Loss: 286.2955017089844, Test Loss: 274.23272705078125\n",
            "Epoch 6/250, Loss: 279.2408752441406, Test Loss: 273.7984924316406\n",
            "Epoch 7/250, Loss: 270.38787841796875, Test Loss: 273.36334228515625\n",
            "Epoch 8/250, Loss: 270.9701232910156, Test Loss: 272.9222106933594\n",
            "Epoch 9/250, Loss: 280.1010437011719, Test Loss: 272.4814147949219\n",
            "Epoch 10/250, Loss: 291.2983703613281, Test Loss: 272.0317687988281\n",
            "Epoch 11/250, Loss: 274.97711181640625, Test Loss: 271.5721435546875\n",
            "Epoch 12/250, Loss: 270.8619384765625, Test Loss: 271.1083984375\n",
            "Epoch 13/250, Loss: 284.73553466796875, Test Loss: 270.6378173828125\n",
            "Epoch 14/250, Loss: 280.0179138183594, Test Loss: 270.15216064453125\n",
            "Epoch 15/250, Loss: 266.9256591796875, Test Loss: 269.6449890136719\n",
            "Epoch 16/250, Loss: 277.15325927734375, Test Loss: 269.1141662597656\n",
            "Epoch 17/250, Loss: 274.8120422363281, Test Loss: 268.55841064453125\n",
            "Epoch 18/250, Loss: 273.8790588378906, Test Loss: 267.96978759765625\n",
            "Epoch 19/250, Loss: 278.8908996582031, Test Loss: 267.3551940917969\n",
            "Epoch 20/250, Loss: 270.1919860839844, Test Loss: 266.7181091308594\n",
            "Epoch 21/250, Loss: 266.1359558105469, Test Loss: 266.0491638183594\n",
            "Epoch 22/250, Loss: 263.5910949707031, Test Loss: 265.3519592285156\n",
            "Epoch 23/250, Loss: 269.4366149902344, Test Loss: 264.6004638671875\n",
            "Epoch 24/250, Loss: 269.9529724121094, Test Loss: 263.8113708496094\n",
            "Epoch 25/250, Loss: 257.05755615234375, Test Loss: 262.9889831542969\n",
            "Epoch 26/250, Loss: 258.99932861328125, Test Loss: 262.1291198730469\n",
            "Epoch 27/250, Loss: 266.4060974121094, Test Loss: 261.22222900390625\n",
            "Epoch 28/250, Loss: 262.9330749511719, Test Loss: 260.2613830566406\n",
            "Epoch 29/250, Loss: 255.74862670898438, Test Loss: 259.2599792480469\n",
            "Epoch 30/250, Loss: 256.1343688964844, Test Loss: 258.26361083984375\n",
            "Epoch 31/250, Loss: 258.9309387207031, Test Loss: 257.2246398925781\n",
            "Epoch 32/250, Loss: 246.15499877929688, Test Loss: 256.06402587890625\n",
            "Epoch 33/250, Loss: 251.31472778320312, Test Loss: 254.87547302246094\n",
            "Epoch 34/250, Loss: 247.06382751464844, Test Loss: 253.5953826904297\n",
            "Epoch 35/250, Loss: 243.30201721191406, Test Loss: 252.28799438476562\n",
            "Epoch 36/250, Loss: 241.01336669921875, Test Loss: 250.91082763671875\n",
            "Epoch 37/250, Loss: 232.57273864746094, Test Loss: 249.44650268554688\n",
            "Epoch 38/250, Loss: 226.20489501953125, Test Loss: 247.87918090820312\n",
            "Epoch 39/250, Loss: 237.52590942382812, Test Loss: 246.2850341796875\n",
            "Epoch 40/250, Loss: 219.7812042236328, Test Loss: 244.49911499023438\n",
            "Epoch 41/250, Loss: 229.00732421875, Test Loss: 242.6204071044922\n",
            "Epoch 42/250, Loss: 225.15386962890625, Test Loss: 240.46563720703125\n",
            "Epoch 43/250, Loss: 210.35682678222656, Test Loss: 238.0660400390625\n",
            "Epoch 44/250, Loss: 213.9286651611328, Test Loss: 235.36038208007812\n",
            "Epoch 45/250, Loss: 199.15234375, Test Loss: 232.3289337158203\n",
            "Epoch 46/250, Loss: 190.73690795898438, Test Loss: 229.05780029296875\n",
            "Epoch 47/250, Loss: 184.1671600341797, Test Loss: 225.42698669433594\n",
            "Epoch 48/250, Loss: 189.17147827148438, Test Loss: 221.52667236328125\n",
            "Epoch 49/250, Loss: 182.00958251953125, Test Loss: 217.2834014892578\n",
            "Epoch 50/250, Loss: 178.7772979736328, Test Loss: 212.56976318359375\n",
            "Epoch 51/250, Loss: 181.37783813476562, Test Loss: 207.27720642089844\n",
            "Epoch 52/250, Loss: 175.1432342529297, Test Loss: 201.3256072998047\n",
            "Epoch 53/250, Loss: 154.51670837402344, Test Loss: 194.65231323242188\n",
            "Epoch 54/250, Loss: 153.91995239257812, Test Loss: 187.7243194580078\n",
            "Epoch 55/250, Loss: 132.57411193847656, Test Loss: 180.1940460205078\n",
            "Epoch 56/250, Loss: 141.6769561767578, Test Loss: 172.57391357421875\n",
            "Epoch 57/250, Loss: 118.21824645996094, Test Loss: 164.33131408691406\n",
            "Epoch 58/250, Loss: 109.13689422607422, Test Loss: 155.8031005859375\n",
            "Epoch 59/250, Loss: 105.40597534179688, Test Loss: 146.87704467773438\n",
            "Epoch 60/250, Loss: 109.04745483398438, Test Loss: 137.74365234375\n",
            "Epoch 61/250, Loss: 85.98599243164062, Test Loss: 128.1938018798828\n",
            "Epoch 62/250, Loss: 95.74764251708984, Test Loss: 118.37335968017578\n",
            "Epoch 63/250, Loss: 71.80106353759766, Test Loss: 108.28533172607422\n",
            "Epoch 64/250, Loss: 66.61735534667969, Test Loss: 98.43225860595703\n",
            "Epoch 65/250, Loss: 66.02928924560547, Test Loss: 88.55668640136719\n",
            "Epoch 66/250, Loss: 55.88731384277344, Test Loss: 78.4610366821289\n",
            "Epoch 67/250, Loss: 50.679447174072266, Test Loss: 68.70661163330078\n",
            "Epoch 68/250, Loss: 43.28929901123047, Test Loss: 59.38209533691406\n",
            "Epoch 69/250, Loss: 32.735721588134766, Test Loss: 50.55586624145508\n",
            "Epoch 70/250, Loss: 30.86676788330078, Test Loss: 42.49340057373047\n",
            "Epoch 71/250, Loss: 25.915271759033203, Test Loss: 35.21451187133789\n",
            "Epoch 72/250, Loss: 18.208436965942383, Test Loss: 28.607336044311523\n",
            "Epoch 73/250, Loss: 13.696573257446289, Test Loss: 22.794567108154297\n",
            "Epoch 74/250, Loss: 12.640776634216309, Test Loss: 17.752918243408203\n",
            "Epoch 75/250, Loss: 9.134602546691895, Test Loss: 13.489812850952148\n",
            "Epoch 76/250, Loss: 7.720076084136963, Test Loss: 9.975388526916504\n",
            "Epoch 77/250, Loss: 5.393594741821289, Test Loss: 7.261748313903809\n",
            "Epoch 78/250, Loss: 3.57307505607605, Test Loss: 5.2195258140563965\n",
            "Epoch 79/250, Loss: 2.739638566970825, Test Loss: 3.778667688369751\n",
            "Epoch 80/250, Loss: 1.8777114152908325, Test Loss: 2.7585339546203613\n",
            "Epoch 81/250, Loss: 1.6938660144805908, Test Loss: 2.076270818710327\n",
            "Epoch 82/250, Loss: 1.4346288442611694, Test Loss: 1.6396067142486572\n",
            "Epoch 83/250, Loss: 1.0943118333816528, Test Loss: 1.387045979499817\n",
            "Epoch 84/250, Loss: 1.0489394664764404, Test Loss: 1.2542976140975952\n",
            "Epoch 85/250, Loss: 1.0390326976776123, Test Loss: 1.189561128616333\n",
            "Epoch 86/250, Loss: 1.1856108903884888, Test Loss: 1.1707583665847778\n",
            "Epoch 87/250, Loss: 1.0492948293685913, Test Loss: 1.1554293632507324\n",
            "Epoch 88/250, Loss: 1.0587811470031738, Test Loss: 1.144378900527954\n",
            "Epoch 89/250, Loss: 0.5921803116798401, Test Loss: 1.1206164360046387\n",
            "Epoch 90/250, Loss: 1.4213095903396606, Test Loss: 1.099923014640808\n",
            "Epoch 91/250, Loss: 1.0677932500839233, Test Loss: 1.0643199682235718\n",
            "Epoch 92/250, Loss: 0.8080601096153259, Test Loss: 1.0408653020858765\n",
            "Epoch 93/250, Loss: 1.1412688493728638, Test Loss: 1.0288143157958984\n",
            "Epoch 94/250, Loss: 0.8805586695671082, Test Loss: 1.0305911302566528\n",
            "Epoch 95/250, Loss: 0.8432286381721497, Test Loss: 1.025109052658081\n",
            "Epoch 96/250, Loss: 0.9868370890617371, Test Loss: 1.0142823457717896\n",
            "Epoch 97/250, Loss: 0.9833023548126221, Test Loss: 0.9862476587295532\n",
            "Epoch 98/250, Loss: 0.8276507258415222, Test Loss: 0.9749495387077332\n",
            "Epoch 99/250, Loss: 0.9376925230026245, Test Loss: 0.9678897261619568\n",
            "Epoch 100/250, Loss: 0.958316445350647, Test Loss: 0.9601171016693115\n",
            "Epoch 101/250, Loss: 0.729854166507721, Test Loss: 0.9612550139427185\n",
            "Epoch 102/250, Loss: 0.9145128726959229, Test Loss: 0.9663457870483398\n",
            "Epoch 103/250, Loss: 1.433397889137268, Test Loss: 0.9672988653182983\n",
            "Epoch 104/250, Loss: 0.8353462815284729, Test Loss: 0.9613340497016907\n",
            "Epoch 105/250, Loss: 1.0357640981674194, Test Loss: 0.9526113867759705\n",
            "Epoch 106/250, Loss: 0.9517032504081726, Test Loss: 0.9454202651977539\n",
            "Epoch 107/250, Loss: 0.908987820148468, Test Loss: 0.9441883563995361\n",
            "Epoch 108/250, Loss: 0.8064295053482056, Test Loss: 0.9439546465873718\n",
            "Epoch 109/250, Loss: 0.9534160494804382, Test Loss: 0.9426001310348511\n",
            "Epoch 110/250, Loss: 0.93311607837677, Test Loss: 0.9378846287727356\n",
            "Epoch 111/250, Loss: 0.6072203516960144, Test Loss: 0.9335230588912964\n",
            "Epoch 112/250, Loss: 1.0207058191299438, Test Loss: 0.9301371574401855\n",
            "Epoch 113/250, Loss: 1.0020289421081543, Test Loss: 0.9273045063018799\n",
            "Epoch 114/250, Loss: 0.880594789981842, Test Loss: 0.9236466884613037\n",
            "Epoch 115/250, Loss: 0.8112006783485413, Test Loss: 0.920688271522522\n",
            "Epoch 116/250, Loss: 0.8583052754402161, Test Loss: 0.924897313117981\n",
            "Epoch 117/250, Loss: 0.8821799159049988, Test Loss: 0.9424971342086792\n",
            "Epoch 118/250, Loss: 0.7800461649894714, Test Loss: 0.9396781325340271\n",
            "Epoch 119/250, Loss: 0.9113374352455139, Test Loss: 0.9232404828071594\n",
            "Epoch 120/250, Loss: 0.8850729465484619, Test Loss: 0.9226433634757996\n",
            "Epoch 121/250, Loss: 0.852584719657898, Test Loss: 0.928099513053894\n",
            "Epoch 122/250, Loss: 0.793411374092102, Test Loss: 0.9335440993309021\n",
            "Epoch 123/250, Loss: 0.9016821384429932, Test Loss: 0.9380853176116943\n",
            "Epoch 124/250, Loss: 0.9535211324691772, Test Loss: 0.925782322883606\n",
            "Epoch 125/250, Loss: 0.771662712097168, Test Loss: 0.9164552092552185\n",
            "Epoch 126/250, Loss: 0.9146825671195984, Test Loss: 0.9104211330413818\n",
            "Epoch 127/250, Loss: 1.2898139953613281, Test Loss: 0.9198293685913086\n",
            "Epoch 128/250, Loss: 1.0244797468185425, Test Loss: 0.924740195274353\n",
            "Epoch 129/250, Loss: 0.6654497385025024, Test Loss: 0.9261696934700012\n",
            "Epoch 130/250, Loss: 0.9061696529388428, Test Loss: 0.9472583532333374\n",
            "Epoch 131/250, Loss: 0.943397581577301, Test Loss: 0.9571094512939453\n",
            "Epoch 132/250, Loss: 1.175372838973999, Test Loss: 0.9503304362297058\n",
            "Epoch 133/250, Loss: 0.9303886890411377, Test Loss: 0.9259928464889526\n",
            "Epoch 134/250, Loss: 0.8660906553268433, Test Loss: 0.9229454398155212\n",
            "Epoch 135/250, Loss: 0.8785685896873474, Test Loss: 0.9300037622451782\n",
            "Epoch 136/250, Loss: 0.770169198513031, Test Loss: 0.9445502161979675\n",
            "Epoch 137/250, Loss: 0.9396639466285706, Test Loss: 0.9459739327430725\n",
            "Epoch 138/250, Loss: 0.7400197386741638, Test Loss: 0.9366090893745422\n",
            "Epoch 139/250, Loss: 0.82899010181427, Test Loss: 0.9338525533676147\n",
            "Epoch 140/250, Loss: 0.9105426073074341, Test Loss: 0.9185245633125305\n",
            "Epoch 141/250, Loss: 0.7821770906448364, Test Loss: 0.9205306768417358\n",
            "Epoch 142/250, Loss: 1.112868070602417, Test Loss: 0.9398400187492371\n",
            "Epoch 143/250, Loss: 0.9262247085571289, Test Loss: 0.942264199256897\n",
            "Epoch 144/250, Loss: 1.0745105743408203, Test Loss: 0.9389528632164001\n",
            "Epoch 145/250, Loss: 1.055086374282837, Test Loss: 0.9241555333137512\n",
            "Epoch 146/250, Loss: 0.9772534966468811, Test Loss: 0.930352509021759\n",
            "Epoch 147/250, Loss: 0.9146229028701782, Test Loss: 0.9423535466194153\n",
            "Epoch 148/250, Loss: 1.1072776317596436, Test Loss: 0.9403170943260193\n",
            "Epoch 149/250, Loss: 0.9587013721466064, Test Loss: 0.9174261093139648\n",
            "Epoch 150/250, Loss: 0.9314728379249573, Test Loss: 0.9130313992500305\n",
            "Epoch 151/250, Loss: 0.815523087978363, Test Loss: 0.9086636900901794\n",
            "Epoch 152/250, Loss: 0.7882607579231262, Test Loss: 0.9184095859527588\n",
            "Epoch 153/250, Loss: 0.9219568371772766, Test Loss: 0.9311814904212952\n",
            "Epoch 154/250, Loss: 1.3590320348739624, Test Loss: 0.9540109634399414\n",
            "Epoch 155/250, Loss: 1.2287029027938843, Test Loss: 0.9542419910430908\n",
            "Epoch 156/250, Loss: 1.0149357318878174, Test Loss: 0.9519754648208618\n",
            "Epoch 157/250, Loss: 0.978993833065033, Test Loss: 0.9437862634658813\n",
            "Epoch 158/250, Loss: 0.9965048432350159, Test Loss: 0.9416664242744446\n",
            "Epoch 159/250, Loss: 0.7944737672805786, Test Loss: 0.9349514245986938\n",
            "Epoch 160/250, Loss: 1.1195368766784668, Test Loss: 0.9448568820953369\n",
            "Epoch 161/250, Loss: 0.7474316954612732, Test Loss: 0.9310189485549927\n",
            "Epoch 162/250, Loss: 0.8586539626121521, Test Loss: 0.9259952306747437\n",
            "Epoch 163/250, Loss: 0.9705531001091003, Test Loss: 0.9170150756835938\n",
            "Epoch 164/250, Loss: 0.8984562754631042, Test Loss: 0.9055717587471008\n",
            "Epoch 165/250, Loss: 0.8354414105415344, Test Loss: 0.9101545214653015\n",
            "Epoch 166/250, Loss: 0.7612449526786804, Test Loss: 0.9193111658096313\n",
            "Epoch 167/250, Loss: 0.751144289970398, Test Loss: 0.9180638790130615\n",
            "Epoch 168/250, Loss: 1.140190601348877, Test Loss: 0.9259470701217651\n",
            "Epoch 169/250, Loss: 0.97102952003479, Test Loss: 0.9081045985221863\n",
            "Epoch 170/250, Loss: 0.8865910172462463, Test Loss: 0.9203755855560303\n",
            "Epoch 171/250, Loss: 0.6250966191291809, Test Loss: 0.9424453973770142\n",
            "Epoch 172/250, Loss: 0.790673553943634, Test Loss: 0.9600332975387573\n",
            "Epoch 173/250, Loss: 1.2590720653533936, Test Loss: 0.9483704566955566\n",
            "Epoch 174/250, Loss: 0.8358843922615051, Test Loss: 0.9245357513427734\n",
            "Epoch 175/250, Loss: 0.8501411080360413, Test Loss: 0.9200817346572876\n",
            "Epoch 176/250, Loss: 0.8351569771766663, Test Loss: 0.9292557835578918\n",
            "Epoch 177/250, Loss: 0.5776882171630859, Test Loss: 0.9443532228469849\n",
            "Epoch 178/250, Loss: 1.1783852577209473, Test Loss: 0.9427374601364136\n",
            "Epoch 179/250, Loss: 0.7322041392326355, Test Loss: 0.9223287105560303\n",
            "Epoch 180/250, Loss: 0.8391459584236145, Test Loss: 0.9127284288406372\n",
            "Epoch 181/250, Loss: 0.8868204951286316, Test Loss: 0.9145109057426453\n",
            "Epoch 182/250, Loss: 1.12451171875, Test Loss: 0.9259722828865051\n",
            "Epoch 183/250, Loss: 0.8895800709724426, Test Loss: 0.9481591582298279\n",
            "Epoch 184/250, Loss: 1.0777372121810913, Test Loss: 0.9684334993362427\n",
            "Epoch 185/250, Loss: 1.1743289232254028, Test Loss: 0.967654824256897\n",
            "Epoch 186/250, Loss: 0.9317330718040466, Test Loss: 0.9688543677330017\n",
            "Epoch 187/250, Loss: 0.8765777945518494, Test Loss: 0.9514974355697632\n",
            "Epoch 188/250, Loss: 0.6020346283912659, Test Loss: 0.9453495740890503\n",
            "Epoch 189/250, Loss: 0.9803616404533386, Test Loss: 0.940889298915863\n",
            "Epoch 190/250, Loss: 0.8276574611663818, Test Loss: 0.9214321970939636\n",
            "Epoch 191/250, Loss: 1.0772522687911987, Test Loss: 0.900445282459259\n",
            "Epoch 192/250, Loss: 0.9686185717582703, Test Loss: 0.8904428482055664\n",
            "Epoch 193/250, Loss: 0.9690104126930237, Test Loss: 0.8915232419967651\n",
            "Epoch 194/250, Loss: 1.1556483507156372, Test Loss: 0.904303252696991\n",
            "Epoch 195/250, Loss: 0.979296863079071, Test Loss: 0.9453060030937195\n",
            "Epoch 196/250, Loss: 0.9224387407302856, Test Loss: 0.9537109136581421\n",
            "Epoch 197/250, Loss: 0.7865630388259888, Test Loss: 0.9346916079521179\n",
            "Epoch 198/250, Loss: 0.9182005524635315, Test Loss: 0.9135293364524841\n",
            "Epoch 199/250, Loss: 1.067681908607483, Test Loss: 0.9075605869293213\n",
            "Epoch 200/250, Loss: 0.7862830758094788, Test Loss: 0.9214901328086853\n",
            "Epoch 201/250, Loss: 0.9189943075180054, Test Loss: 0.9360716342926025\n",
            "Epoch 202/250, Loss: 1.06797456741333, Test Loss: 0.9407004714012146\n",
            "Epoch 203/250, Loss: 1.1979873180389404, Test Loss: 0.9234257936477661\n",
            "Epoch 204/250, Loss: 1.2788432836532593, Test Loss: 0.9039404988288879\n",
            "Epoch 205/250, Loss: 1.084708571434021, Test Loss: 0.8875489234924316\n",
            "Epoch 206/250, Loss: 0.76289963722229, Test Loss: 0.9013736844062805\n",
            "Epoch 207/250, Loss: 0.9034486413002014, Test Loss: 0.9170462489128113\n",
            "Epoch 208/250, Loss: 0.6784153580665588, Test Loss: 0.9414489269256592\n",
            "Epoch 209/250, Loss: 0.7882176041603088, Test Loss: 0.9525201320648193\n",
            "Epoch 210/250, Loss: 0.9349725842475891, Test Loss: 0.9338468909263611\n",
            "Epoch 211/250, Loss: 0.5343571305274963, Test Loss: 0.9179427027702332\n",
            "Epoch 212/250, Loss: 0.7080783843994141, Test Loss: 0.913468062877655\n",
            "Epoch 213/250, Loss: 0.7822679877281189, Test Loss: 0.9239094257354736\n",
            "Epoch 214/250, Loss: 0.9948317408561707, Test Loss: 0.9376379251480103\n",
            "Epoch 215/250, Loss: 0.9849215149879456, Test Loss: 0.9260188937187195\n",
            "Epoch 216/250, Loss: 0.8836514949798584, Test Loss: 0.9318897128105164\n",
            "Epoch 217/250, Loss: 1.4546704292297363, Test Loss: 0.9305055141448975\n",
            "Epoch 218/250, Loss: 0.9015287160873413, Test Loss: 0.9312409162521362\n",
            "Epoch 219/250, Loss: 0.9332349300384521, Test Loss: 0.9195507168769836\n",
            "Epoch 220/250, Loss: 0.9170476198196411, Test Loss: 0.9189165234565735\n",
            "Epoch 221/250, Loss: 0.751588761806488, Test Loss: 0.9134461879730225\n",
            "Epoch 222/250, Loss: 0.7527185082435608, Test Loss: 0.9163668751716614\n",
            "Epoch 223/250, Loss: 0.9250432252883911, Test Loss: 0.933468759059906\n",
            "Epoch 224/250, Loss: 1.5034072399139404, Test Loss: 0.9490422010421753\n",
            "Epoch 225/250, Loss: 0.9178084135055542, Test Loss: 0.941775381565094\n",
            "Epoch 226/250, Loss: 0.642002284526825, Test Loss: 0.9508769512176514\n",
            "Epoch 227/250, Loss: 0.948253870010376, Test Loss: 0.9584735035896301\n",
            "Epoch 228/250, Loss: 0.9795624017715454, Test Loss: 0.9604872465133667\n",
            "Epoch 229/250, Loss: 1.0149441957473755, Test Loss: 0.9481170177459717\n",
            "Epoch 230/250, Loss: 0.787736177444458, Test Loss: 0.9236189723014832\n",
            "Epoch 231/250, Loss: 0.8986820578575134, Test Loss: 0.9234198927879333\n",
            "Epoch 232/250, Loss: 1.0862853527069092, Test Loss: 0.9346358180046082\n",
            "Epoch 233/250, Loss: 0.9566039443016052, Test Loss: 0.9405422210693359\n",
            "Epoch 234/250, Loss: 1.022527813911438, Test Loss: 0.9609075784683228\n",
            "Epoch 235/250, Loss: 0.964185893535614, Test Loss: 0.9401631355285645\n",
            "Epoch 236/250, Loss: 0.982101559638977, Test Loss: 0.9346838593482971\n",
            "Epoch 237/250, Loss: 0.6381884813308716, Test Loss: 0.9368432760238647\n",
            "Epoch 238/250, Loss: 0.9889362454414368, Test Loss: 0.9330393075942993\n",
            "Epoch 239/250, Loss: 1.0696967840194702, Test Loss: 0.9244465827941895\n",
            "Epoch 240/250, Loss: 0.9184820652008057, Test Loss: 0.9073238968849182\n",
            "Epoch 241/250, Loss: 1.0086073875427246, Test Loss: 0.9146029353141785\n",
            "Epoch 242/250, Loss: 1.0548975467681885, Test Loss: 0.9251724481582642\n",
            "Epoch 243/250, Loss: 1.2054723501205444, Test Loss: 0.9252521395683289\n",
            "Epoch 244/250, Loss: 0.9218864440917969, Test Loss: 0.9501788020133972\n",
            "Epoch 245/250, Loss: 0.9716989994049072, Test Loss: 0.974679708480835\n",
            "Epoch 246/250, Loss: 1.0065292119979858, Test Loss: 0.9593918919563293\n",
            "Epoch 247/250, Loss: 0.8321210145950317, Test Loss: 0.9371224045753479\n",
            "Epoch 248/250, Loss: 0.9991411566734314, Test Loss: 0.9139696955680847\n",
            "Epoch 249/250, Loss: 0.8290457129478455, Test Loss: 0.9217672348022461\n",
            "Epoch 250/250, Loss: 0.5998147130012512, Test Loss: 0.9437053203582764\n",
            "Final MSE: 0.943705141544342\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=256, epochs=1\n",
            "Epoch 1/1, Loss: 266.2071838378906, Test Loss: 260.4900207519531\n",
            "Final MSE: 260.489990234375\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=256, epochs=10\n",
            "Epoch 1/10, Loss: 289.87213134765625, Test Loss: 281.4270324707031\n",
            "Epoch 2/10, Loss: 284.7786865234375, Test Loss: 281.0475769042969\n",
            "Epoch 3/10, Loss: 284.2689208984375, Test Loss: 280.6662902832031\n",
            "Epoch 4/10, Loss: 288.93731689453125, Test Loss: 280.28143310546875\n",
            "Epoch 5/10, Loss: 285.5157165527344, Test Loss: 279.8960266113281\n",
            "Epoch 6/10, Loss: 281.2660217285156, Test Loss: 279.5076904296875\n",
            "Epoch 7/10, Loss: 295.3805847167969, Test Loss: 279.11517333984375\n",
            "Epoch 8/10, Loss: 284.1350402832031, Test Loss: 278.724853515625\n",
            "Epoch 9/10, Loss: 285.6958312988281, Test Loss: 278.33489990234375\n",
            "Epoch 10/10, Loss: 279.7119140625, Test Loss: 277.9436950683594\n",
            "Final MSE: 277.9436950683594\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=256, epochs=25\n",
            "Epoch 1/25, Loss: 270.4897766113281, Test Loss: 258.951416015625\n",
            "Epoch 2/25, Loss: 266.2926330566406, Test Loss: 258.6988830566406\n",
            "Epoch 3/25, Loss: 256.8922424316406, Test Loss: 258.44140625\n",
            "Epoch 4/25, Loss: 271.6549072265625, Test Loss: 258.17755126953125\n",
            "Epoch 5/25, Loss: 258.16070556640625, Test Loss: 257.91534423828125\n",
            "Epoch 6/25, Loss: 261.5503845214844, Test Loss: 257.6522216796875\n",
            "Epoch 7/25, Loss: 257.8811950683594, Test Loss: 257.39129638671875\n",
            "Epoch 8/25, Loss: 259.87384033203125, Test Loss: 257.13299560546875\n",
            "Epoch 9/25, Loss: 264.3989562988281, Test Loss: 256.8704528808594\n",
            "Epoch 10/25, Loss: 255.9539031982422, Test Loss: 256.60125732421875\n",
            "Epoch 11/25, Loss: 271.1036376953125, Test Loss: 256.3280334472656\n",
            "Epoch 12/25, Loss: 263.41705322265625, Test Loss: 256.0538024902344\n",
            "Epoch 13/25, Loss: 261.0934143066406, Test Loss: 255.78173828125\n",
            "Epoch 14/25, Loss: 251.85647583007812, Test Loss: 255.50836181640625\n",
            "Epoch 15/25, Loss: 258.40899658203125, Test Loss: 255.2288360595703\n",
            "Epoch 16/25, Loss: 252.5724639892578, Test Loss: 254.94422912597656\n",
            "Epoch 17/25, Loss: 260.47882080078125, Test Loss: 254.6622772216797\n",
            "Epoch 18/25, Loss: 256.4314880371094, Test Loss: 254.384521484375\n",
            "Epoch 19/25, Loss: 260.6687927246094, Test Loss: 254.1052703857422\n",
            "Epoch 20/25, Loss: 262.9982604980469, Test Loss: 253.8205108642578\n",
            "Epoch 21/25, Loss: 256.61273193359375, Test Loss: 253.5330352783203\n",
            "Epoch 22/25, Loss: 251.42613220214844, Test Loss: 253.24221801757812\n",
            "Epoch 23/25, Loss: 248.95953369140625, Test Loss: 252.94845581054688\n",
            "Epoch 24/25, Loss: 253.1488800048828, Test Loss: 252.65518188476562\n",
            "Epoch 25/25, Loss: 254.18589782714844, Test Loss: 252.3549041748047\n",
            "Final MSE: 252.3549041748047\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=256, epochs=50\n",
            "Epoch 1/50, Loss: 291.1315612792969, Test Loss: 285.8460998535156\n",
            "Epoch 2/50, Loss: 290.7798767089844, Test Loss: 285.5007629394531\n",
            "Epoch 3/50, Loss: 302.0020751953125, Test Loss: 285.1549377441406\n",
            "Epoch 4/50, Loss: 287.5320129394531, Test Loss: 284.80609130859375\n",
            "Epoch 5/50, Loss: 291.3334655761719, Test Loss: 284.452880859375\n",
            "Epoch 6/50, Loss: 291.0265808105469, Test Loss: 284.1023254394531\n",
            "Epoch 7/50, Loss: 285.59820556640625, Test Loss: 283.7529296875\n",
            "Epoch 8/50, Loss: 288.7269592285156, Test Loss: 283.4069519042969\n",
            "Epoch 9/50, Loss: 284.15838623046875, Test Loss: 283.0582275390625\n",
            "Epoch 10/50, Loss: 292.6825866699219, Test Loss: 282.7008361816406\n",
            "Epoch 11/50, Loss: 282.28070068359375, Test Loss: 282.33795166015625\n",
            "Epoch 12/50, Loss: 292.9979553222656, Test Loss: 281.97637939453125\n",
            "Epoch 13/50, Loss: 286.30853271484375, Test Loss: 281.61419677734375\n",
            "Epoch 14/50, Loss: 282.0647888183594, Test Loss: 281.25079345703125\n",
            "Epoch 15/50, Loss: 286.8100891113281, Test Loss: 280.8846435546875\n",
            "Epoch 16/50, Loss: 289.634765625, Test Loss: 280.5143737792969\n",
            "Epoch 17/50, Loss: 295.7594909667969, Test Loss: 280.1426086425781\n",
            "Epoch 18/50, Loss: 295.98980712890625, Test Loss: 279.76922607421875\n",
            "Epoch 19/50, Loss: 286.5320739746094, Test Loss: 279.3930969238281\n",
            "Epoch 20/50, Loss: 291.5562438964844, Test Loss: 279.0166931152344\n",
            "Epoch 21/50, Loss: 273.5598449707031, Test Loss: 278.6332702636719\n",
            "Epoch 22/50, Loss: 285.4952392578125, Test Loss: 278.2413635253906\n",
            "Epoch 23/50, Loss: 284.8858947753906, Test Loss: 277.8449401855469\n",
            "Epoch 24/50, Loss: 280.18621826171875, Test Loss: 277.4448547363281\n",
            "Epoch 25/50, Loss: 279.0899658203125, Test Loss: 277.03656005859375\n",
            "Epoch 26/50, Loss: 282.4582824707031, Test Loss: 276.615478515625\n",
            "Epoch 27/50, Loss: 275.55267333984375, Test Loss: 276.1862487792969\n",
            "Epoch 28/50, Loss: 288.73980712890625, Test Loss: 275.7430725097656\n",
            "Epoch 29/50, Loss: 275.7082214355469, Test Loss: 275.29315185546875\n",
            "Epoch 30/50, Loss: 280.83135986328125, Test Loss: 274.8492431640625\n",
            "Epoch 31/50, Loss: 281.32928466796875, Test Loss: 274.3907165527344\n",
            "Epoch 32/50, Loss: 285.01690673828125, Test Loss: 273.9184265136719\n",
            "Epoch 33/50, Loss: 284.08038330078125, Test Loss: 273.4348449707031\n",
            "Epoch 34/50, Loss: 278.9703674316406, Test Loss: 272.9414978027344\n",
            "Epoch 35/50, Loss: 275.22320556640625, Test Loss: 272.4431457519531\n",
            "Epoch 36/50, Loss: 272.7250671386719, Test Loss: 271.94024658203125\n",
            "Epoch 37/50, Loss: 274.56451416015625, Test Loss: 271.4254455566406\n",
            "Epoch 38/50, Loss: 271.60430908203125, Test Loss: 270.880615234375\n",
            "Epoch 39/50, Loss: 264.8927307128906, Test Loss: 270.31414794921875\n",
            "Epoch 40/50, Loss: 270.62713623046875, Test Loss: 269.7298889160156\n",
            "Epoch 41/50, Loss: 268.4978332519531, Test Loss: 269.1484069824219\n",
            "Epoch 42/50, Loss: 269.40216064453125, Test Loss: 268.5448913574219\n",
            "Epoch 43/50, Loss: 269.61138916015625, Test Loss: 267.93548583984375\n",
            "Epoch 44/50, Loss: 270.1576843261719, Test Loss: 267.3266296386719\n",
            "Epoch 45/50, Loss: 260.8124084472656, Test Loss: 266.69451904296875\n",
            "Epoch 46/50, Loss: 260.9557800292969, Test Loss: 266.06097412109375\n",
            "Epoch 47/50, Loss: 264.52178955078125, Test Loss: 265.4174499511719\n",
            "Epoch 48/50, Loss: 254.24244689941406, Test Loss: 264.7591857910156\n",
            "Epoch 49/50, Loss: 258.48114013671875, Test Loss: 264.11029052734375\n",
            "Epoch 50/50, Loss: 247.7947540283203, Test Loss: 263.4261169433594\n",
            "Final MSE: 263.4261169433594\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=256, epochs=100\n",
            "Epoch 1/100, Loss: 295.3580017089844, Test Loss: 279.83795166015625\n",
            "Epoch 2/100, Loss: 280.4432373046875, Test Loss: 279.4535827636719\n",
            "Epoch 3/100, Loss: 284.2669677734375, Test Loss: 279.058837890625\n",
            "Epoch 4/100, Loss: 284.7956237792969, Test Loss: 278.6544189453125\n",
            "Epoch 5/100, Loss: 273.4400939941406, Test Loss: 278.2335510253906\n",
            "Epoch 6/100, Loss: 285.5034484863281, Test Loss: 277.8075256347656\n",
            "Epoch 7/100, Loss: 277.8940124511719, Test Loss: 277.3770446777344\n",
            "Epoch 8/100, Loss: 281.0135192871094, Test Loss: 276.9449157714844\n",
            "Epoch 9/100, Loss: 287.2466125488281, Test Loss: 276.5101318359375\n",
            "Epoch 10/100, Loss: 278.8700256347656, Test Loss: 276.0651550292969\n",
            "Epoch 11/100, Loss: 282.5412902832031, Test Loss: 275.6114501953125\n",
            "Epoch 12/100, Loss: 285.3560485839844, Test Loss: 275.1539611816406\n",
            "Epoch 13/100, Loss: 279.6851501464844, Test Loss: 274.69293212890625\n",
            "Epoch 14/100, Loss: 277.9594421386719, Test Loss: 274.2222595214844\n",
            "Epoch 15/100, Loss: 273.9909973144531, Test Loss: 273.7462158203125\n",
            "Epoch 16/100, Loss: 274.3141174316406, Test Loss: 273.2724914550781\n",
            "Epoch 17/100, Loss: 283.6345520019531, Test Loss: 272.7958984375\n",
            "Epoch 18/100, Loss: 275.0416564941406, Test Loss: 272.30987548828125\n",
            "Epoch 19/100, Loss: 273.1757507324219, Test Loss: 271.8191833496094\n",
            "Epoch 20/100, Loss: 272.42352294921875, Test Loss: 271.33319091796875\n",
            "Epoch 21/100, Loss: 269.80657958984375, Test Loss: 270.85150146484375\n",
            "Epoch 22/100, Loss: 271.9132080078125, Test Loss: 270.3667297363281\n",
            "Epoch 23/100, Loss: 283.39373779296875, Test Loss: 269.8512268066406\n",
            "Epoch 24/100, Loss: 274.4664611816406, Test Loss: 269.2944641113281\n",
            "Epoch 25/100, Loss: 271.0152587890625, Test Loss: 268.7276611328125\n",
            "Epoch 26/100, Loss: 271.47967529296875, Test Loss: 268.16802978515625\n",
            "Epoch 27/100, Loss: 275.69622802734375, Test Loss: 267.6056213378906\n",
            "Epoch 28/100, Loss: 273.7967529296875, Test Loss: 267.0292053222656\n",
            "Epoch 29/100, Loss: 272.73486328125, Test Loss: 266.4375305175781\n",
            "Epoch 30/100, Loss: 274.5895690917969, Test Loss: 265.8279724121094\n",
            "Epoch 31/100, Loss: 267.5244140625, Test Loss: 265.1941223144531\n",
            "Epoch 32/100, Loss: 268.1401062011719, Test Loss: 264.5495910644531\n",
            "Epoch 33/100, Loss: 261.6933288574219, Test Loss: 263.93438720703125\n",
            "Epoch 34/100, Loss: 257.9410095214844, Test Loss: 263.3387145996094\n",
            "Epoch 35/100, Loss: 257.62066650390625, Test Loss: 262.73321533203125\n",
            "Epoch 36/100, Loss: 261.2781677246094, Test Loss: 262.10711669921875\n",
            "Epoch 37/100, Loss: 254.16632080078125, Test Loss: 261.468017578125\n",
            "Epoch 38/100, Loss: 255.91793823242188, Test Loss: 260.8175048828125\n",
            "Epoch 39/100, Loss: 249.16397094726562, Test Loss: 260.1333923339844\n",
            "Epoch 40/100, Loss: 247.3805694580078, Test Loss: 259.42669677734375\n",
            "Epoch 41/100, Loss: 251.5240936279297, Test Loss: 258.7218017578125\n",
            "Epoch 42/100, Loss: 255.95803833007812, Test Loss: 258.0169372558594\n",
            "Epoch 43/100, Loss: 250.3603057861328, Test Loss: 257.313720703125\n",
            "Epoch 44/100, Loss: 244.34457397460938, Test Loss: 256.617919921875\n",
            "Epoch 45/100, Loss: 247.25941467285156, Test Loss: 255.90113830566406\n",
            "Epoch 46/100, Loss: 250.2335968017578, Test Loss: 255.16116333007812\n",
            "Epoch 47/100, Loss: 243.60406494140625, Test Loss: 254.39161682128906\n",
            "Epoch 48/100, Loss: 239.58995056152344, Test Loss: 253.6190185546875\n",
            "Epoch 49/100, Loss: 238.44259643554688, Test Loss: 252.83274841308594\n",
            "Epoch 50/100, Loss: 242.31716918945312, Test Loss: 251.97711181640625\n",
            "Epoch 51/100, Loss: 231.05319213867188, Test Loss: 251.03140258789062\n",
            "Epoch 52/100, Loss: 246.162353515625, Test Loss: 249.99114990234375\n",
            "Epoch 53/100, Loss: 228.37588500976562, Test Loss: 248.8125\n",
            "Epoch 54/100, Loss: 221.58448791503906, Test Loss: 247.572509765625\n",
            "Epoch 55/100, Loss: 224.2943878173828, Test Loss: 246.24530029296875\n",
            "Epoch 56/100, Loss: 214.86911010742188, Test Loss: 244.90171813964844\n",
            "Epoch 57/100, Loss: 229.23069763183594, Test Loss: 243.55078125\n",
            "Epoch 58/100, Loss: 216.54022216796875, Test Loss: 242.03782653808594\n",
            "Epoch 59/100, Loss: 218.57179260253906, Test Loss: 240.33106994628906\n",
            "Epoch 60/100, Loss: 204.93215942382812, Test Loss: 238.46484375\n",
            "Epoch 61/100, Loss: 201.30331420898438, Test Loss: 236.49624633789062\n",
            "Epoch 62/100, Loss: 185.52256774902344, Test Loss: 234.4606475830078\n",
            "Epoch 63/100, Loss: 197.30355834960938, Test Loss: 232.39532470703125\n",
            "Epoch 64/100, Loss: 180.45120239257812, Test Loss: 230.20790100097656\n",
            "Epoch 65/100, Loss: 198.59332275390625, Test Loss: 227.9085235595703\n",
            "Epoch 66/100, Loss: 177.98666381835938, Test Loss: 225.3324737548828\n",
            "Epoch 67/100, Loss: 185.0260467529297, Test Loss: 222.63931274414062\n",
            "Epoch 68/100, Loss: 168.9741973876953, Test Loss: 219.7482147216797\n",
            "Epoch 69/100, Loss: 164.8056640625, Test Loss: 216.7220916748047\n",
            "Epoch 70/100, Loss: 161.20469665527344, Test Loss: 213.58135986328125\n",
            "Epoch 71/100, Loss: 163.41162109375, Test Loss: 210.18798828125\n",
            "Epoch 72/100, Loss: 156.99319458007812, Test Loss: 206.49122619628906\n",
            "Epoch 73/100, Loss: 139.47418212890625, Test Loss: 202.61044311523438\n",
            "Epoch 74/100, Loss: 140.18161010742188, Test Loss: 198.73382568359375\n",
            "Epoch 75/100, Loss: 127.44055938720703, Test Loss: 194.798828125\n",
            "Epoch 76/100, Loss: 148.5546875, Test Loss: 190.61109924316406\n",
            "Epoch 77/100, Loss: 124.58956146240234, Test Loss: 186.08755493164062\n",
            "Epoch 78/100, Loss: 137.53697204589844, Test Loss: 181.3156280517578\n",
            "Epoch 79/100, Loss: 104.70333862304688, Test Loss: 176.2711639404297\n",
            "Epoch 80/100, Loss: 113.30529022216797, Test Loss: 171.15646362304688\n",
            "Epoch 81/100, Loss: 114.84669494628906, Test Loss: 165.87982177734375\n",
            "Epoch 82/100, Loss: 121.815673828125, Test Loss: 160.32614135742188\n",
            "Epoch 83/100, Loss: 104.4937744140625, Test Loss: 154.42282104492188\n",
            "Epoch 84/100, Loss: 90.90624237060547, Test Loss: 148.26536560058594\n",
            "Epoch 85/100, Loss: 98.5999755859375, Test Loss: 141.76217651367188\n",
            "Epoch 86/100, Loss: 84.49495697021484, Test Loss: 135.04710388183594\n",
            "Epoch 87/100, Loss: 93.2954330444336, Test Loss: 128.14598083496094\n",
            "Epoch 88/100, Loss: 65.78844451904297, Test Loss: 121.19286346435547\n",
            "Epoch 89/100, Loss: 70.46450805664062, Test Loss: 114.58332061767578\n",
            "Epoch 90/100, Loss: 58.8355598449707, Test Loss: 108.22885131835938\n",
            "Epoch 91/100, Loss: 67.07820892333984, Test Loss: 102.05549621582031\n",
            "Epoch 92/100, Loss: 51.76872634887695, Test Loss: 95.78521728515625\n",
            "Epoch 93/100, Loss: 54.63051986694336, Test Loss: 89.39237213134766\n",
            "Epoch 94/100, Loss: 51.232975006103516, Test Loss: 82.90792083740234\n",
            "Epoch 95/100, Loss: 54.949363708496094, Test Loss: 76.32000732421875\n",
            "Epoch 96/100, Loss: 45.44274139404297, Test Loss: 69.64903259277344\n",
            "Epoch 97/100, Loss: 36.3744010925293, Test Loss: 63.19015121459961\n",
            "Epoch 98/100, Loss: 44.47107696533203, Test Loss: 56.91455841064453\n",
            "Epoch 99/100, Loss: 30.216251373291016, Test Loss: 50.72388458251953\n",
            "Epoch 100/100, Loss: 26.88475799560547, Test Loss: 44.948543548583984\n",
            "Final MSE: 44.948543548583984\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=256, epochs=250\n",
            "Epoch 1/250, Loss: 261.9698791503906, Test Loss: 259.0130920410156\n",
            "Epoch 2/250, Loss: 262.0617370605469, Test Loss: 258.649169921875\n",
            "Epoch 3/250, Loss: 263.7012023925781, Test Loss: 258.29620361328125\n",
            "Epoch 4/250, Loss: 257.4444885253906, Test Loss: 257.94219970703125\n",
            "Epoch 5/250, Loss: 275.2369079589844, Test Loss: 257.5885009765625\n",
            "Epoch 6/250, Loss: 271.98138427734375, Test Loss: 257.23651123046875\n",
            "Epoch 7/250, Loss: 267.74725341796875, Test Loss: 256.8907470703125\n",
            "Epoch 8/250, Loss: 261.6578674316406, Test Loss: 256.5486755371094\n",
            "Epoch 9/250, Loss: 265.1828918457031, Test Loss: 256.20672607421875\n",
            "Epoch 10/250, Loss: 256.70074462890625, Test Loss: 255.8637237548828\n",
            "Epoch 11/250, Loss: 263.49908447265625, Test Loss: 255.5167694091797\n",
            "Epoch 12/250, Loss: 256.71697998046875, Test Loss: 255.16513061523438\n",
            "Epoch 13/250, Loss: 262.24835205078125, Test Loss: 254.81480407714844\n",
            "Epoch 14/250, Loss: 257.806640625, Test Loss: 254.46568298339844\n",
            "Epoch 15/250, Loss: 263.5125732421875, Test Loss: 254.1156005859375\n",
            "Epoch 16/250, Loss: 258.4910583496094, Test Loss: 253.7672576904297\n",
            "Epoch 17/250, Loss: 261.5525207519531, Test Loss: 253.41282653808594\n",
            "Epoch 18/250, Loss: 264.053955078125, Test Loss: 253.0504913330078\n",
            "Epoch 19/250, Loss: 272.6426086425781, Test Loss: 252.6802520751953\n",
            "Epoch 20/250, Loss: 257.3869934082031, Test Loss: 252.29647827148438\n",
            "Epoch 21/250, Loss: 259.5708312988281, Test Loss: 251.90219116210938\n",
            "Epoch 22/250, Loss: 256.5108642578125, Test Loss: 251.50804138183594\n",
            "Epoch 23/250, Loss: 254.32814025878906, Test Loss: 251.10850524902344\n",
            "Epoch 24/250, Loss: 251.84097290039062, Test Loss: 250.69981384277344\n",
            "Epoch 25/250, Loss: 257.97589111328125, Test Loss: 250.28184509277344\n",
            "Epoch 26/250, Loss: 254.15245056152344, Test Loss: 249.8632049560547\n",
            "Epoch 27/250, Loss: 256.52227783203125, Test Loss: 249.44000244140625\n",
            "Epoch 28/250, Loss: 254.7266082763672, Test Loss: 249.00433349609375\n",
            "Epoch 29/250, Loss: 259.6901550292969, Test Loss: 248.54489135742188\n",
            "Epoch 30/250, Loss: 256.9370422363281, Test Loss: 248.06161499023438\n",
            "Epoch 31/250, Loss: 256.94927978515625, Test Loss: 247.54212951660156\n",
            "Epoch 32/250, Loss: 248.94093322753906, Test Loss: 246.99334716796875\n",
            "Epoch 33/250, Loss: 246.41384887695312, Test Loss: 246.41477966308594\n",
            "Epoch 34/250, Loss: 243.0144805908203, Test Loss: 245.822265625\n",
            "Epoch 35/250, Loss: 245.27967834472656, Test Loss: 245.22117614746094\n",
            "Epoch 36/250, Loss: 252.69723510742188, Test Loss: 244.6136016845703\n",
            "Epoch 37/250, Loss: 245.54574584960938, Test Loss: 244.00350952148438\n",
            "Epoch 38/250, Loss: 240.26217651367188, Test Loss: 243.37744140625\n",
            "Epoch 39/250, Loss: 241.13682556152344, Test Loss: 242.73744201660156\n",
            "Epoch 40/250, Loss: 245.35862731933594, Test Loss: 242.05894470214844\n",
            "Epoch 41/250, Loss: 246.04713439941406, Test Loss: 241.34194946289062\n",
            "Epoch 42/250, Loss: 243.0178680419922, Test Loss: 240.59893798828125\n",
            "Epoch 43/250, Loss: 239.2755889892578, Test Loss: 239.8569793701172\n",
            "Epoch 44/250, Loss: 227.3274688720703, Test Loss: 239.11135864257812\n",
            "Epoch 45/250, Loss: 231.81039428710938, Test Loss: 238.35946655273438\n",
            "Epoch 46/250, Loss: 242.94444274902344, Test Loss: 237.5796356201172\n",
            "Epoch 47/250, Loss: 235.06637573242188, Test Loss: 236.7599639892578\n",
            "Epoch 48/250, Loss: 230.9715118408203, Test Loss: 235.91500854492188\n",
            "Epoch 49/250, Loss: 229.38922119140625, Test Loss: 235.04202270507812\n",
            "Epoch 50/250, Loss: 227.80067443847656, Test Loss: 234.13723754882812\n",
            "Epoch 51/250, Loss: 221.9180145263672, Test Loss: 233.15309143066406\n",
            "Epoch 52/250, Loss: 216.2442169189453, Test Loss: 232.16183471679688\n",
            "Epoch 53/250, Loss: 229.07540893554688, Test Loss: 231.1509552001953\n",
            "Epoch 54/250, Loss: 227.07960510253906, Test Loss: 230.02145385742188\n",
            "Epoch 55/250, Loss: 211.7043914794922, Test Loss: 228.8172149658203\n",
            "Epoch 56/250, Loss: 209.14088439941406, Test Loss: 227.56727600097656\n",
            "Epoch 57/250, Loss: 218.87440490722656, Test Loss: 226.23497009277344\n",
            "Epoch 58/250, Loss: 204.0125274658203, Test Loss: 224.81927490234375\n",
            "Epoch 59/250, Loss: 200.70343017578125, Test Loss: 223.3938446044922\n",
            "Epoch 60/250, Loss: 217.43878173828125, Test Loss: 221.88719177246094\n",
            "Epoch 61/250, Loss: 203.7323455810547, Test Loss: 220.12811279296875\n",
            "Epoch 62/250, Loss: 188.14480590820312, Test Loss: 218.26829528808594\n",
            "Epoch 63/250, Loss: 195.01348876953125, Test Loss: 216.38780212402344\n",
            "Epoch 64/250, Loss: 180.89613342285156, Test Loss: 214.44970703125\n",
            "Epoch 65/250, Loss: 186.75430297851562, Test Loss: 212.46182250976562\n",
            "Epoch 66/250, Loss: 177.2205352783203, Test Loss: 210.38699340820312\n",
            "Epoch 67/250, Loss: 163.47189331054688, Test Loss: 208.25454711914062\n",
            "Epoch 68/250, Loss: 172.6461944580078, Test Loss: 206.01275634765625\n",
            "Epoch 69/250, Loss: 178.9003143310547, Test Loss: 203.48834228515625\n",
            "Epoch 70/250, Loss: 162.01458740234375, Test Loss: 200.74583435058594\n",
            "Epoch 71/250, Loss: 167.03211975097656, Test Loss: 197.87164306640625\n",
            "Epoch 72/250, Loss: 165.77381896972656, Test Loss: 194.75732421875\n",
            "Epoch 73/250, Loss: 148.04676818847656, Test Loss: 191.372314453125\n",
            "Epoch 74/250, Loss: 146.86509704589844, Test Loss: 187.8494110107422\n",
            "Epoch 75/250, Loss: 156.71395874023438, Test Loss: 184.08502197265625\n",
            "Epoch 76/250, Loss: 135.0349578857422, Test Loss: 179.9598846435547\n",
            "Epoch 77/250, Loss: 137.0086212158203, Test Loss: 175.57614135742188\n",
            "Epoch 78/250, Loss: 113.21437072753906, Test Loss: 171.05641174316406\n",
            "Epoch 79/250, Loss: 129.13475036621094, Test Loss: 166.4944305419922\n",
            "Epoch 80/250, Loss: 120.53992462158203, Test Loss: 161.76918029785156\n",
            "Epoch 81/250, Loss: 117.69075012207031, Test Loss: 156.9715576171875\n",
            "Epoch 82/250, Loss: 111.23197937011719, Test Loss: 152.03817749023438\n",
            "Epoch 83/250, Loss: 119.1747817993164, Test Loss: 146.83534240722656\n",
            "Epoch 84/250, Loss: 97.47232818603516, Test Loss: 141.40101623535156\n",
            "Epoch 85/250, Loss: 85.5299072265625, Test Loss: 135.920166015625\n",
            "Epoch 86/250, Loss: 92.9422836303711, Test Loss: 130.40875244140625\n",
            "Epoch 87/250, Loss: 84.57903289794922, Test Loss: 124.78457641601562\n",
            "Epoch 88/250, Loss: 91.43034362792969, Test Loss: 119.03221130371094\n",
            "Epoch 89/250, Loss: 76.7184066772461, Test Loss: 113.02670288085938\n",
            "Epoch 90/250, Loss: 80.78856658935547, Test Loss: 106.85730743408203\n",
            "Epoch 91/250, Loss: 62.2537727355957, Test Loss: 100.51236724853516\n",
            "Epoch 92/250, Loss: 65.43756866455078, Test Loss: 94.24458312988281\n",
            "Epoch 93/250, Loss: 51.55174255371094, Test Loss: 88.09859466552734\n",
            "Epoch 94/250, Loss: 62.174983978271484, Test Loss: 81.9924545288086\n",
            "Epoch 95/250, Loss: 41.31637191772461, Test Loss: 75.9537582397461\n",
            "Epoch 96/250, Loss: 43.52322006225586, Test Loss: 70.16419982910156\n",
            "Epoch 97/250, Loss: 38.816951751708984, Test Loss: 64.52538299560547\n",
            "Epoch 98/250, Loss: 42.54597091674805, Test Loss: 58.981754302978516\n",
            "Epoch 99/250, Loss: 37.50690841674805, Test Loss: 53.394309997558594\n",
            "Epoch 100/250, Loss: 28.634458541870117, Test Loss: 47.841041564941406\n",
            "Epoch 101/250, Loss: 28.728864669799805, Test Loss: 42.54385757446289\n",
            "Epoch 102/250, Loss: 18.892295837402344, Test Loss: 37.55625915527344\n",
            "Epoch 103/250, Loss: 22.31464958190918, Test Loss: 32.98505783081055\n",
            "Epoch 104/250, Loss: 16.408231735229492, Test Loss: 28.736339569091797\n",
            "Epoch 105/250, Loss: 17.738656997680664, Test Loss: 24.847497940063477\n",
            "Epoch 106/250, Loss: 14.743875503540039, Test Loss: 21.254993438720703\n",
            "Epoch 107/250, Loss: 9.820483207702637, Test Loss: 17.994165420532227\n",
            "Epoch 108/250, Loss: 10.74232006072998, Test Loss: 15.14488697052002\n",
            "Epoch 109/250, Loss: 8.680431365966797, Test Loss: 12.683401107788086\n",
            "Epoch 110/250, Loss: 6.140147686004639, Test Loss: 10.5724515914917\n",
            "Epoch 111/250, Loss: 5.1112565994262695, Test Loss: 8.759613037109375\n",
            "Epoch 112/250, Loss: 4.754777431488037, Test Loss: 7.21950101852417\n",
            "Epoch 113/250, Loss: 2.9660120010375977, Test Loss: 5.92766809463501\n",
            "Epoch 114/250, Loss: 2.7781519889831543, Test Loss: 4.852901458740234\n",
            "Epoch 115/250, Loss: 4.226292610168457, Test Loss: 3.983301877975464\n",
            "Epoch 116/250, Loss: 2.9643912315368652, Test Loss: 3.2975001335144043\n",
            "Epoch 117/250, Loss: 2.1274402141571045, Test Loss: 2.7799527645111084\n",
            "Epoch 118/250, Loss: 2.802281141281128, Test Loss: 2.3930277824401855\n",
            "Epoch 119/250, Loss: 1.5457863807678223, Test Loss: 2.094640016555786\n",
            "Epoch 120/250, Loss: 1.6194716691970825, Test Loss: 1.8762363195419312\n",
            "Epoch 121/250, Loss: 1.4722955226898193, Test Loss: 1.7357083559036255\n",
            "Epoch 122/250, Loss: 1.8814228773117065, Test Loss: 1.6651099920272827\n",
            "Epoch 123/250, Loss: 2.0935139656066895, Test Loss: 1.6223630905151367\n",
            "Epoch 124/250, Loss: 1.4147679805755615, Test Loss: 1.5756186246871948\n",
            "Epoch 125/250, Loss: 1.3724725246429443, Test Loss: 1.5072367191314697\n",
            "Epoch 126/250, Loss: 1.3699419498443604, Test Loss: 1.441588044166565\n",
            "Epoch 127/250, Loss: 1.3132716417312622, Test Loss: 1.3862621784210205\n",
            "Epoch 128/250, Loss: 1.0324175357818604, Test Loss: 1.344377875328064\n",
            "Epoch 129/250, Loss: 1.1127649545669556, Test Loss: 1.3133445978164673\n",
            "Epoch 130/250, Loss: 1.3948605060577393, Test Loss: 1.2939995527267456\n",
            "Epoch 131/250, Loss: 1.106558084487915, Test Loss: 1.2834045886993408\n",
            "Epoch 132/250, Loss: 1.1262460947036743, Test Loss: 1.2701352834701538\n",
            "Epoch 133/250, Loss: 0.7969150543212891, Test Loss: 1.2582911252975464\n",
            "Epoch 134/250, Loss: 1.1826521158218384, Test Loss: 1.237932801246643\n",
            "Epoch 135/250, Loss: 1.0668119192123413, Test Loss: 1.2062721252441406\n",
            "Epoch 136/250, Loss: 1.0913842916488647, Test Loss: 1.1761125326156616\n",
            "Epoch 137/250, Loss: 0.8136336803436279, Test Loss: 1.1495397090911865\n",
            "Epoch 138/250, Loss: 1.252008080482483, Test Loss: 1.1163181066513062\n",
            "Epoch 139/250, Loss: 0.907239556312561, Test Loss: 1.0669503211975098\n",
            "Epoch 140/250, Loss: 1.042109727859497, Test Loss: 1.025336503982544\n",
            "Epoch 141/250, Loss: 1.236451506614685, Test Loss: 1.0047798156738281\n",
            "Epoch 142/250, Loss: 0.8764990568161011, Test Loss: 0.997046947479248\n",
            "Epoch 143/250, Loss: 0.7555092573165894, Test Loss: 0.9989505410194397\n",
            "Epoch 144/250, Loss: 1.1888868808746338, Test Loss: 1.0079435110092163\n",
            "Epoch 145/250, Loss: 0.7803415656089783, Test Loss: 1.0112297534942627\n",
            "Epoch 146/250, Loss: 1.0236632823944092, Test Loss: 1.0122097730636597\n",
            "Epoch 147/250, Loss: 0.9478729367256165, Test Loss: 1.0178571939468384\n",
            "Epoch 148/250, Loss: 1.0335601568222046, Test Loss: 1.0260320901870728\n",
            "Epoch 149/250, Loss: 0.9438082575798035, Test Loss: 1.0313764810562134\n",
            "Epoch 150/250, Loss: 1.1057934761047363, Test Loss: 1.0196905136108398\n",
            "Epoch 151/250, Loss: 0.6295412182807922, Test Loss: 0.9927476048469543\n",
            "Epoch 152/250, Loss: 0.8587450385093689, Test Loss: 0.9647940993309021\n",
            "Epoch 153/250, Loss: 0.7892505526542664, Test Loss: 0.9370229244232178\n",
            "Epoch 154/250, Loss: 0.7988247275352478, Test Loss: 0.916274905204773\n",
            "Epoch 155/250, Loss: 1.0662705898284912, Test Loss: 0.9063369035720825\n",
            "Epoch 156/250, Loss: 0.9298844933509827, Test Loss: 0.9096013307571411\n",
            "Epoch 157/250, Loss: 0.9302701354026794, Test Loss: 0.9234933853149414\n",
            "Epoch 158/250, Loss: 1.1440149545669556, Test Loss: 0.9394311308860779\n",
            "Epoch 159/250, Loss: 1.0891611576080322, Test Loss: 0.9537649750709534\n",
            "Epoch 160/250, Loss: 1.270352840423584, Test Loss: 0.9603176116943359\n",
            "Epoch 161/250, Loss: 0.6635832190513611, Test Loss: 0.9627187252044678\n",
            "Epoch 162/250, Loss: 0.9954445362091064, Test Loss: 0.9621388912200928\n",
            "Epoch 163/250, Loss: 0.8782128095626831, Test Loss: 0.9658649563789368\n",
            "Epoch 164/250, Loss: 0.9721635580062866, Test Loss: 0.9731891751289368\n",
            "Epoch 165/250, Loss: 0.9414094686508179, Test Loss: 0.9801291227340698\n",
            "Epoch 166/250, Loss: 0.8707741498947144, Test Loss: 0.9814131259918213\n",
            "Epoch 167/250, Loss: 0.6705750823020935, Test Loss: 0.9766252636909485\n",
            "Epoch 168/250, Loss: 0.971461832523346, Test Loss: 0.9695962071418762\n",
            "Epoch 169/250, Loss: 0.7614295482635498, Test Loss: 0.9643182158470154\n",
            "Epoch 170/250, Loss: 0.6877374053001404, Test Loss: 0.9585500955581665\n",
            "Epoch 171/250, Loss: 0.479638010263443, Test Loss: 0.9536141753196716\n",
            "Epoch 172/250, Loss: 0.9295772314071655, Test Loss: 0.9457955956459045\n",
            "Epoch 173/250, Loss: 1.2790011167526245, Test Loss: 0.938553512096405\n",
            "Epoch 174/250, Loss: 0.7407459616661072, Test Loss: 0.9310476779937744\n",
            "Epoch 175/250, Loss: 0.9428731203079224, Test Loss: 0.9289577603340149\n",
            "Epoch 176/250, Loss: 1.129956841468811, Test Loss: 0.9237220883369446\n",
            "Epoch 177/250, Loss: 1.092166543006897, Test Loss: 0.9109832048416138\n",
            "Epoch 178/250, Loss: 1.1243395805358887, Test Loss: 0.907356858253479\n",
            "Epoch 179/250, Loss: 0.843883216381073, Test Loss: 0.9138532280921936\n",
            "Epoch 180/250, Loss: 1.0051201581954956, Test Loss: 0.9158698916435242\n",
            "Epoch 181/250, Loss: 1.155993938446045, Test Loss: 0.9140111207962036\n",
            "Epoch 182/250, Loss: 0.835736870765686, Test Loss: 0.9123080372810364\n",
            "Epoch 183/250, Loss: 0.9402875900268555, Test Loss: 0.9083521366119385\n",
            "Epoch 184/250, Loss: 0.8370473384857178, Test Loss: 0.9056128859519958\n",
            "Epoch 185/250, Loss: 1.0618053674697876, Test Loss: 0.9123831391334534\n",
            "Epoch 186/250, Loss: 0.8641204237937927, Test Loss: 0.9202955365180969\n",
            "Epoch 187/250, Loss: 0.8454349040985107, Test Loss: 0.9224204421043396\n",
            "Epoch 188/250, Loss: 0.989684522151947, Test Loss: 0.914572536945343\n",
            "Epoch 189/250, Loss: 0.9361772537231445, Test Loss: 0.9036926031112671\n",
            "Epoch 190/250, Loss: 1.107695460319519, Test Loss: 0.9093348979949951\n",
            "Epoch 191/250, Loss: 0.9754129648208618, Test Loss: 0.9268929362297058\n",
            "Epoch 192/250, Loss: 1.0198490619659424, Test Loss: 0.9432874321937561\n",
            "Epoch 193/250, Loss: 1.106261968612671, Test Loss: 0.9443312883377075\n",
            "Epoch 194/250, Loss: 0.9048097133636475, Test Loss: 0.93570476770401\n",
            "Epoch 195/250, Loss: 0.8400651216506958, Test Loss: 0.9306567907333374\n",
            "Epoch 196/250, Loss: 0.8216193914413452, Test Loss: 0.9242979884147644\n",
            "Epoch 197/250, Loss: 0.8049449324607849, Test Loss: 0.9170846343040466\n",
            "Epoch 198/250, Loss: 0.8648231029510498, Test Loss: 0.9091172814369202\n",
            "Epoch 199/250, Loss: 1.3180285692214966, Test Loss: 0.9028628468513489\n",
            "Epoch 200/250, Loss: 0.9854879975318909, Test Loss: 0.8930662274360657\n",
            "Epoch 201/250, Loss: 0.9990624785423279, Test Loss: 0.8924139738082886\n",
            "Epoch 202/250, Loss: 0.9878164529800415, Test Loss: 0.8997780680656433\n",
            "Epoch 203/250, Loss: 0.8807607293128967, Test Loss: 0.9108679294586182\n",
            "Epoch 204/250, Loss: 1.2490241527557373, Test Loss: 0.9174252152442932\n",
            "Epoch 205/250, Loss: 0.8452708125114441, Test Loss: 0.920161783695221\n",
            "Epoch 206/250, Loss: 0.8938539028167725, Test Loss: 0.9233325123786926\n",
            "Epoch 207/250, Loss: 0.7947344183921814, Test Loss: 0.9245495200157166\n",
            "Epoch 208/250, Loss: 0.852739155292511, Test Loss: 0.9287810325622559\n",
            "Epoch 209/250, Loss: 1.1792339086532593, Test Loss: 0.9321483373641968\n",
            "Epoch 210/250, Loss: 1.0002878904342651, Test Loss: 0.929650604724884\n",
            "Epoch 211/250, Loss: 0.863820493221283, Test Loss: 0.9280504584312439\n",
            "Epoch 212/250, Loss: 1.0166131258010864, Test Loss: 0.922126293182373\n",
            "Epoch 213/250, Loss: 0.6280164122581482, Test Loss: 0.9169899821281433\n",
            "Epoch 214/250, Loss: 0.8254574537277222, Test Loss: 0.914808452129364\n",
            "Epoch 215/250, Loss: 1.02910578250885, Test Loss: 0.9249765872955322\n",
            "Epoch 216/250, Loss: 1.0123965740203857, Test Loss: 0.9471103549003601\n",
            "Epoch 217/250, Loss: 1.0560927391052246, Test Loss: 0.9648990631103516\n",
            "Epoch 218/250, Loss: 1.1509348154067993, Test Loss: 0.9710035920143127\n",
            "Epoch 219/250, Loss: 0.8370952010154724, Test Loss: 0.9674596786499023\n",
            "Epoch 220/250, Loss: 1.0321964025497437, Test Loss: 0.9596075415611267\n",
            "Epoch 221/250, Loss: 0.9400891065597534, Test Loss: 0.9500168561935425\n",
            "Epoch 222/250, Loss: 1.0097174644470215, Test Loss: 0.9319593906402588\n",
            "Epoch 223/250, Loss: 1.351070523262024, Test Loss: 0.9097545146942139\n",
            "Epoch 224/250, Loss: 0.8897914886474609, Test Loss: 0.8943933248519897\n",
            "Epoch 225/250, Loss: 0.9752713441848755, Test Loss: 0.8897085785865784\n",
            "Epoch 226/250, Loss: 1.0631489753723145, Test Loss: 0.9056148529052734\n",
            "Epoch 227/250, Loss: 0.9727235436439514, Test Loss: 0.9393911361694336\n",
            "Epoch 228/250, Loss: 0.7041423320770264, Test Loss: 0.967968225479126\n",
            "Epoch 229/250, Loss: 0.9753797650337219, Test Loss: 0.9749431610107422\n",
            "Epoch 230/250, Loss: 1.082470417022705, Test Loss: 0.954580545425415\n",
            "Epoch 231/250, Loss: 1.0557568073272705, Test Loss: 0.9300206899642944\n",
            "Epoch 232/250, Loss: 0.8063375353813171, Test Loss: 0.927736222743988\n",
            "Epoch 233/250, Loss: 0.7837835550308228, Test Loss: 0.9299836158752441\n",
            "Epoch 234/250, Loss: 0.8908346891403198, Test Loss: 0.9244306683540344\n",
            "Epoch 235/250, Loss: 0.9322351217269897, Test Loss: 0.9229979515075684\n",
            "Epoch 236/250, Loss: 0.975933313369751, Test Loss: 0.9343555569648743\n",
            "Epoch 237/250, Loss: 0.7005345225334167, Test Loss: 0.9590997695922852\n",
            "Epoch 238/250, Loss: 0.7871593832969666, Test Loss: 0.9712831377983093\n",
            "Epoch 239/250, Loss: 0.6334056854248047, Test Loss: 0.9735434055328369\n",
            "Epoch 240/250, Loss: 0.9935193657875061, Test Loss: 0.957594096660614\n",
            "Epoch 241/250, Loss: 0.7062727212905884, Test Loss: 0.933589518070221\n",
            "Epoch 242/250, Loss: 1.0079166889190674, Test Loss: 0.9214064478874207\n",
            "Epoch 243/250, Loss: 0.712213397026062, Test Loss: 0.9140642285346985\n",
            "Epoch 244/250, Loss: 0.9891613721847534, Test Loss: 0.9056615829467773\n",
            "Epoch 245/250, Loss: 0.8371679782867432, Test Loss: 0.8962658047676086\n",
            "Epoch 246/250, Loss: 1.0020493268966675, Test Loss: 0.8895707130432129\n",
            "Epoch 247/250, Loss: 0.936786413192749, Test Loss: 0.8900658488273621\n",
            "Epoch 248/250, Loss: 0.9732850790023804, Test Loss: 0.8976036906242371\n",
            "Epoch 249/250, Loss: 0.8155279755592346, Test Loss: 0.9096100330352783\n",
            "Epoch 250/250, Loss: 0.9037010073661804, Test Loss: 0.9213243126869202\n",
            "Final MSE: 0.9213243126869202\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=512, epochs=1\n",
            "Epoch 1/1, Loss: 267.1109924316406, Test Loss: 259.661376953125\n",
            "Final MSE: 259.661376953125\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=512, epochs=10\n",
            "Epoch 1/10, Loss: 280.7021789550781, Test Loss: 272.9765625\n",
            "Epoch 2/10, Loss: 280.45721435546875, Test Loss: 272.8659362792969\n",
            "Epoch 3/10, Loss: 280.212890625, Test Loss: 272.75506591796875\n",
            "Epoch 4/10, Loss: 279.9690856933594, Test Loss: 272.64361572265625\n",
            "Epoch 5/10, Loss: 279.7255859375, Test Loss: 272.5312805175781\n",
            "Epoch 6/10, Loss: 279.4820251464844, Test Loss: 272.41815185546875\n",
            "Epoch 7/10, Loss: 279.2381286621094, Test Loss: 272.3048400878906\n",
            "Epoch 8/10, Loss: 278.9937438964844, Test Loss: 272.1917419433594\n",
            "Epoch 9/10, Loss: 278.7486572265625, Test Loss: 272.07916259765625\n",
            "Epoch 10/10, Loss: 278.5027770996094, Test Loss: 271.96734619140625\n",
            "Final MSE: 271.9673767089844\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=512, epochs=25\n",
            "Epoch 1/25, Loss: 280.6766662597656, Test Loss: 272.9326171875\n",
            "Epoch 2/25, Loss: 280.43560791015625, Test Loss: 272.79840087890625\n",
            "Epoch 3/25, Loss: 280.1937561035156, Test Loss: 272.6643981933594\n",
            "Epoch 4/25, Loss: 279.95111083984375, Test Loss: 272.53131103515625\n",
            "Epoch 5/25, Loss: 279.70751953125, Test Loss: 272.3992004394531\n",
            "Epoch 6/25, Loss: 279.46307373046875, Test Loss: 272.2682800292969\n",
            "Epoch 7/25, Loss: 279.2176208496094, Test Loss: 272.1386413574219\n",
            "Epoch 8/25, Loss: 278.97113037109375, Test Loss: 272.0103454589844\n",
            "Epoch 9/25, Loss: 278.7236022949219, Test Loss: 271.88348388671875\n",
            "Epoch 10/25, Loss: 278.474853515625, Test Loss: 271.75823974609375\n",
            "Epoch 11/25, Loss: 278.2248229980469, Test Loss: 271.6346130371094\n",
            "Epoch 12/25, Loss: 277.9735107421875, Test Loss: 271.5127868652344\n",
            "Epoch 13/25, Loss: 277.7206726074219, Test Loss: 271.392822265625\n",
            "Epoch 14/25, Loss: 277.46624755859375, Test Loss: 271.2749328613281\n",
            "Epoch 15/25, Loss: 277.2100830078125, Test Loss: 271.1591796875\n",
            "Epoch 16/25, Loss: 276.9520263671875, Test Loss: 271.0456848144531\n",
            "Epoch 17/25, Loss: 276.6918640136719, Test Loss: 270.93463134765625\n",
            "Epoch 18/25, Loss: 276.42938232421875, Test Loss: 270.8261413574219\n",
            "Epoch 19/25, Loss: 276.16448974609375, Test Loss: 270.72027587890625\n",
            "Epoch 20/25, Loss: 275.896728515625, Test Loss: 270.6171875\n",
            "Epoch 21/25, Loss: 275.6260986328125, Test Loss: 270.51702880859375\n",
            "Epoch 22/25, Loss: 275.3521423339844, Test Loss: 270.41986083984375\n",
            "Epoch 23/25, Loss: 275.0748291015625, Test Loss: 270.3258361816406\n",
            "Epoch 24/25, Loss: 274.79376220703125, Test Loss: 270.23504638671875\n",
            "Epoch 25/25, Loss: 274.5087890625, Test Loss: 270.1476135253906\n",
            "Final MSE: 270.14764404296875\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=512, epochs=50\n",
            "Epoch 1/50, Loss: 288.0263366699219, Test Loss: 280.1538391113281\n",
            "Epoch 2/50, Loss: 287.81170654296875, Test Loss: 280.0644836425781\n",
            "Epoch 3/50, Loss: 287.59576416015625, Test Loss: 279.9754638671875\n",
            "Epoch 4/50, Loss: 287.3780212402344, Test Loss: 279.8871765136719\n",
            "Epoch 5/50, Loss: 287.1583557128906, Test Loss: 279.7996520996094\n",
            "Epoch 6/50, Loss: 286.93658447265625, Test Loss: 279.7131652832031\n",
            "Epoch 7/50, Loss: 286.7126159667969, Test Loss: 279.6277770996094\n",
            "Epoch 8/50, Loss: 286.48638916015625, Test Loss: 279.54364013671875\n",
            "Epoch 9/50, Loss: 286.2577209472656, Test Loss: 279.46087646484375\n",
            "Epoch 10/50, Loss: 286.0264892578125, Test Loss: 279.37969970703125\n",
            "Epoch 11/50, Loss: 285.792724609375, Test Loss: 279.30023193359375\n",
            "Epoch 12/50, Loss: 285.5560607910156, Test Loss: 279.22271728515625\n",
            "Epoch 13/50, Loss: 285.31646728515625, Test Loss: 279.14739990234375\n",
            "Epoch 14/50, Loss: 285.0737609863281, Test Loss: 279.0744934082031\n",
            "Epoch 15/50, Loss: 284.8277282714844, Test Loss: 279.00421142578125\n",
            "Epoch 16/50, Loss: 284.5782775878906, Test Loss: 278.936767578125\n",
            "Epoch 17/50, Loss: 284.3251953125, Test Loss: 278.8724670410156\n",
            "Epoch 18/50, Loss: 284.068359375, Test Loss: 278.8115539550781\n",
            "Epoch 19/50, Loss: 283.8075866699219, Test Loss: 278.7543029785156\n",
            "Epoch 20/50, Loss: 283.5426330566406, Test Loss: 278.70098876953125\n",
            "Epoch 21/50, Loss: 283.27337646484375, Test Loss: 278.6518249511719\n",
            "Epoch 22/50, Loss: 282.9996337890625, Test Loss: 278.6071472167969\n",
            "Epoch 23/50, Loss: 282.72119140625, Test Loss: 278.5672302246094\n",
            "Epoch 24/50, Loss: 282.4378356933594, Test Loss: 278.5323181152344\n",
            "Epoch 25/50, Loss: 282.1494140625, Test Loss: 278.5026550292969\n",
            "Epoch 26/50, Loss: 281.85565185546875, Test Loss: 278.4786682128906\n",
            "Epoch 27/50, Loss: 281.55645751953125, Test Loss: 278.46051025390625\n",
            "Epoch 28/50, Loss: 281.25152587890625, Test Loss: 278.44854736328125\n",
            "Epoch 29/50, Loss: 280.9406433105469, Test Loss: 278.443115234375\n",
            "Epoch 30/50, Loss: 280.6236572265625, Test Loss: 278.4444580078125\n",
            "Epoch 31/50, Loss: 280.3003234863281, Test Loss: 278.4530029296875\n",
            "Epoch 32/50, Loss: 279.97039794921875, Test Loss: 278.46905517578125\n",
            "Epoch 33/50, Loss: 279.63372802734375, Test Loss: 278.4928894042969\n",
            "Epoch 34/50, Loss: 279.2900085449219, Test Loss: 278.5249328613281\n",
            "Epoch 35/50, Loss: 278.9390869140625, Test Loss: 278.5655212402344\n",
            "Epoch 36/50, Loss: 278.5806579589844, Test Loss: 278.6149597167969\n",
            "Epoch 37/50, Loss: 278.21466064453125, Test Loss: 278.67364501953125\n",
            "Epoch 38/50, Loss: 277.8406982421875, Test Loss: 278.741943359375\n",
            "Epoch 39/50, Loss: 277.4586486816406, Test Loss: 278.8201904296875\n",
            "Epoch 40/50, Loss: 277.0682373046875, Test Loss: 278.9087219238281\n",
            "Epoch 41/50, Loss: 276.6693115234375, Test Loss: 279.0079345703125\n",
            "Epoch 42/50, Loss: 276.2615661621094, Test Loss: 279.1180419921875\n",
            "Epoch 43/50, Loss: 275.8449401855469, Test Loss: 279.2394714355469\n",
            "Epoch 44/50, Loss: 275.4190979003906, Test Loss: 279.3724670410156\n",
            "Epoch 45/50, Loss: 274.9838562011719, Test Loss: 279.5172424316406\n",
            "Epoch 46/50, Loss: 274.5390930175781, Test Loss: 279.6741638183594\n",
            "Epoch 47/50, Loss: 274.0845031738281, Test Loss: 279.8432922363281\n",
            "Epoch 48/50, Loss: 273.61993408203125, Test Loss: 280.0248718261719\n",
            "Epoch 49/50, Loss: 273.145263671875, Test Loss: 280.2190246582031\n",
            "Epoch 50/50, Loss: 272.6603088378906, Test Loss: 280.42584228515625\n",
            "Final MSE: 280.42584228515625\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=512, epochs=100\n",
            "Epoch 1/100, Loss: 290.4667663574219, Test Loss: 282.2914733886719\n",
            "Epoch 2/100, Loss: 290.25567626953125, Test Loss: 282.1583251953125\n",
            "Epoch 3/100, Loss: 290.044677734375, Test Loss: 282.0256652832031\n",
            "Epoch 4/100, Loss: 289.83380126953125, Test Loss: 281.8935546875\n",
            "Epoch 5/100, Loss: 289.62286376953125, Test Loss: 281.76214599609375\n",
            "Epoch 6/100, Loss: 289.4118347167969, Test Loss: 281.6315002441406\n",
            "Epoch 7/100, Loss: 289.2005310058594, Test Loss: 281.50177001953125\n",
            "Epoch 8/100, Loss: 288.98895263671875, Test Loss: 281.37310791015625\n",
            "Epoch 9/100, Loss: 288.7768859863281, Test Loss: 281.24554443359375\n",
            "Epoch 10/100, Loss: 288.56439208984375, Test Loss: 281.1192321777344\n",
            "Epoch 11/100, Loss: 288.351318359375, Test Loss: 280.99432373046875\n",
            "Epoch 12/100, Loss: 288.1375732421875, Test Loss: 280.8708190917969\n",
            "Epoch 13/100, Loss: 287.9231262207031, Test Loss: 280.7489013671875\n",
            "Epoch 14/100, Loss: 287.7078857421875, Test Loss: 280.62860107421875\n",
            "Epoch 15/100, Loss: 287.49176025390625, Test Loss: 280.5101013183594\n",
            "Epoch 16/100, Loss: 287.27459716796875, Test Loss: 280.39349365234375\n",
            "Epoch 17/100, Loss: 287.0563049316406, Test Loss: 280.2788391113281\n",
            "Epoch 18/100, Loss: 286.8367919921875, Test Loss: 280.1663818359375\n",
            "Epoch 19/100, Loss: 286.6158142089844, Test Loss: 280.05609130859375\n",
            "Epoch 20/100, Loss: 286.3932800292969, Test Loss: 279.9482421875\n",
            "Epoch 21/100, Loss: 286.16900634765625, Test Loss: 279.8428039550781\n",
            "Epoch 22/100, Loss: 285.9428405761719, Test Loss: 279.7400207519531\n",
            "Epoch 23/100, Loss: 285.7145080566406, Test Loss: 279.63995361328125\n",
            "Epoch 24/100, Loss: 285.4839782714844, Test Loss: 279.54278564453125\n",
            "Epoch 25/100, Loss: 285.2509765625, Test Loss: 279.4485778808594\n",
            "Epoch 26/100, Loss: 285.0152893066406, Test Loss: 279.3575439453125\n",
            "Epoch 27/100, Loss: 284.7767639160156, Test Loss: 279.26971435546875\n",
            "Epoch 28/100, Loss: 284.53521728515625, Test Loss: 279.18536376953125\n",
            "Epoch 29/100, Loss: 284.2903137207031, Test Loss: 279.10455322265625\n",
            "Epoch 30/100, Loss: 284.0418701171875, Test Loss: 279.0274353027344\n",
            "Epoch 31/100, Loss: 283.7897644042969, Test Loss: 278.9541931152344\n",
            "Epoch 32/100, Loss: 283.5336608886719, Test Loss: 278.8848571777344\n",
            "Epoch 33/100, Loss: 283.2734069824219, Test Loss: 278.8197326660156\n",
            "Epoch 34/100, Loss: 283.0086975097656, Test Loss: 278.7588195800781\n",
            "Epoch 35/100, Loss: 282.7392883300781, Test Loss: 278.7023620605469\n",
            "Epoch 36/100, Loss: 282.4649963378906, Test Loss: 278.65045166015625\n",
            "Epoch 37/100, Loss: 282.18560791015625, Test Loss: 278.6032409667969\n",
            "Epoch 38/100, Loss: 281.9009094238281, Test Loss: 278.56097412109375\n",
            "Epoch 39/100, Loss: 281.6105651855469, Test Loss: 278.523681640625\n",
            "Epoch 40/100, Loss: 281.3144836425781, Test Loss: 278.49163818359375\n",
            "Epoch 41/100, Loss: 281.0122985839844, Test Loss: 278.4649658203125\n",
            "Epoch 42/100, Loss: 280.70391845703125, Test Loss: 278.4438171386719\n",
            "Epoch 43/100, Loss: 280.3889465332031, Test Loss: 278.4284362792969\n",
            "Epoch 44/100, Loss: 280.0672607421875, Test Loss: 278.4189758300781\n",
            "Epoch 45/100, Loss: 279.7386169433594, Test Loss: 278.4157409667969\n",
            "Epoch 46/100, Loss: 279.4028015136719, Test Loss: 278.4189147949219\n",
            "Epoch 47/100, Loss: 279.0594482421875, Test Loss: 278.4288635253906\n",
            "Epoch 48/100, Loss: 278.7084045410156, Test Loss: 278.4457702636719\n",
            "Epoch 49/100, Loss: 278.3493957519531, Test Loss: 278.4700012207031\n",
            "Epoch 50/100, Loss: 277.9821472167969, Test Loss: 278.50177001953125\n",
            "Epoch 51/100, Loss: 277.6064453125, Test Loss: 278.5415344238281\n",
            "Epoch 52/100, Loss: 277.2220153808594, Test Loss: 278.589599609375\n",
            "Epoch 53/100, Loss: 276.82861328125, Test Loss: 278.646240234375\n",
            "Epoch 54/100, Loss: 276.4259033203125, Test Loss: 278.7118835449219\n",
            "Epoch 55/100, Loss: 276.013671875, Test Loss: 278.786865234375\n",
            "Epoch 56/100, Loss: 275.5917053222656, Test Loss: 278.87164306640625\n",
            "Epoch 57/100, Loss: 275.1596984863281, Test Loss: 278.96661376953125\n",
            "Epoch 58/100, Loss: 274.7173767089844, Test Loss: 279.0723571777344\n",
            "Epoch 59/100, Loss: 274.2645263671875, Test Loss: 279.18939208984375\n",
            "Epoch 60/100, Loss: 273.8009338378906, Test Loss: 279.3184509277344\n",
            "Epoch 61/100, Loss: 273.3262939453125, Test Loss: 279.46026611328125\n",
            "Epoch 62/100, Loss: 272.8404235839844, Test Loss: 279.6155090332031\n",
            "Epoch 63/100, Loss: 272.3431091308594, Test Loss: 279.7850646972656\n",
            "Epoch 64/100, Loss: 271.8341064453125, Test Loss: 279.969482421875\n",
            "Epoch 65/100, Loss: 271.31329345703125, Test Loss: 280.1692810058594\n",
            "Epoch 66/100, Loss: 270.7803955078125, Test Loss: 280.38482666015625\n",
            "Epoch 67/100, Loss: 270.2353210449219, Test Loss: 280.6163024902344\n",
            "Epoch 68/100, Loss: 269.6778564453125, Test Loss: 280.8638610839844\n",
            "Epoch 69/100, Loss: 269.10797119140625, Test Loss: 281.1273498535156\n",
            "Epoch 70/100, Loss: 268.5254211425781, Test Loss: 281.4065856933594\n",
            "Epoch 71/100, Loss: 267.93023681640625, Test Loss: 281.7012023925781\n",
            "Epoch 72/100, Loss: 267.3222961425781, Test Loss: 282.0106201171875\n",
            "Epoch 73/100, Loss: 266.7015075683594, Test Loss: 282.3341369628906\n",
            "Epoch 74/100, Loss: 266.06787109375, Test Loss: 282.6710510253906\n",
            "Epoch 75/100, Loss: 265.42132568359375, Test Loss: 283.0205078125\n",
            "Epoch 76/100, Loss: 264.7618713378906, Test Loss: 283.3819274902344\n",
            "Epoch 77/100, Loss: 264.0894775390625, Test Loss: 283.7546081542969\n",
            "Epoch 78/100, Loss: 263.4041748046875, Test Loss: 284.13812255859375\n",
            "Epoch 79/100, Loss: 262.70587158203125, Test Loss: 284.5323181152344\n",
            "Epoch 80/100, Loss: 261.9946594238281, Test Loss: 284.9367980957031\n",
            "Epoch 81/100, Loss: 261.2704772949219, Test Loss: 285.3515625\n",
            "Epoch 82/100, Loss: 260.53326416015625, Test Loss: 285.7762756347656\n",
            "Epoch 83/100, Loss: 259.7830810546875, Test Loss: 286.21075439453125\n",
            "Epoch 84/100, Loss: 259.01983642578125, Test Loss: 286.65472412109375\n",
            "Epoch 85/100, Loss: 258.2434997558594, Test Loss: 287.10760498046875\n",
            "Epoch 86/100, Loss: 257.4540710449219, Test Loss: 287.5687561035156\n",
            "Epoch 87/100, Loss: 256.65142822265625, Test Loss: 288.03692626953125\n",
            "Epoch 88/100, Loss: 255.83551025390625, Test Loss: 288.5102233886719\n",
            "Epoch 89/100, Loss: 255.00625610351562, Test Loss: 288.9858703613281\n",
            "Epoch 90/100, Loss: 254.16357421875, Test Loss: 289.4606018066406\n",
            "Epoch 91/100, Loss: 253.30735778808594, Test Loss: 289.9314270019531\n",
            "Epoch 92/100, Loss: 252.4375, Test Loss: 290.3959655761719\n",
            "Epoch 93/100, Loss: 251.55374145507812, Test Loss: 290.8528747558594\n",
            "Epoch 94/100, Loss: 250.65602111816406, Test Loss: 291.30108642578125\n",
            "Epoch 95/100, Loss: 249.74404907226562, Test Loss: 291.7397155761719\n",
            "Epoch 96/100, Loss: 248.8175506591797, Test Loss: 292.16717529296875\n",
            "Epoch 97/100, Loss: 247.8762664794922, Test Loss: 292.5807800292969\n",
            "Epoch 98/100, Loss: 246.9198455810547, Test Loss: 292.97637939453125\n",
            "Epoch 99/100, Loss: 245.94786071777344, Test Loss: 293.34918212890625\n",
            "Epoch 100/100, Loss: 244.95989990234375, Test Loss: 293.6937255859375\n",
            "Final MSE: 293.6936950683594\n",
            "Training with [16, 8, 4], linear, lr=0.001, batch_size=512, epochs=250\n",
            "Epoch 1/250, Loss: 286.2234802246094, Test Loss: 278.28802490234375\n",
            "Epoch 2/250, Loss: 286.0314025878906, Test Loss: 278.1660461425781\n",
            "Epoch 3/250, Loss: 285.8393249511719, Test Loss: 278.0439147949219\n",
            "Epoch 4/250, Loss: 285.6470642089844, Test Loss: 277.9217224121094\n",
            "Epoch 5/250, Loss: 285.4547424316406, Test Loss: 277.7996520996094\n",
            "Epoch 6/250, Loss: 285.26226806640625, Test Loss: 277.6778564453125\n",
            "Epoch 7/250, Loss: 285.0696105957031, Test Loss: 277.55657958984375\n",
            "Epoch 8/250, Loss: 284.8766174316406, Test Loss: 277.4361877441406\n",
            "Epoch 9/250, Loss: 284.6833190917969, Test Loss: 277.31683349609375\n",
            "Epoch 10/250, Loss: 284.48956298828125, Test Loss: 277.1986389160156\n",
            "Epoch 11/250, Loss: 284.2951354980469, Test Loss: 277.08184814453125\n",
            "Epoch 12/250, Loss: 284.1000061035156, Test Loss: 276.9664001464844\n",
            "Epoch 13/250, Loss: 283.9040222167969, Test Loss: 276.8524169921875\n",
            "Epoch 14/250, Loss: 283.70697021484375, Test Loss: 276.73992919921875\n",
            "Epoch 15/250, Loss: 283.5088195800781, Test Loss: 276.6291809082031\n",
            "Epoch 16/250, Loss: 283.309326171875, Test Loss: 276.5202331542969\n",
            "Epoch 17/250, Loss: 283.10845947265625, Test Loss: 276.413330078125\n",
            "Epoch 18/250, Loss: 282.906005859375, Test Loss: 276.30853271484375\n",
            "Epoch 19/250, Loss: 282.701904296875, Test Loss: 276.2059631347656\n",
            "Epoch 20/250, Loss: 282.4959411621094, Test Loss: 276.10577392578125\n",
            "Epoch 21/250, Loss: 282.28802490234375, Test Loss: 276.0080871582031\n",
            "Epoch 22/250, Loss: 282.0780334472656, Test Loss: 275.9130859375\n",
            "Epoch 23/250, Loss: 281.8656921386719, Test Loss: 275.82098388671875\n",
            "Epoch 24/250, Loss: 281.6509704589844, Test Loss: 275.7320251464844\n",
            "Epoch 25/250, Loss: 281.43359375, Test Loss: 275.6463928222656\n",
            "Epoch 26/250, Loss: 281.2134704589844, Test Loss: 275.5644226074219\n",
            "Epoch 27/250, Loss: 280.99041748046875, Test Loss: 275.48614501953125\n",
            "Epoch 28/250, Loss: 280.76416015625, Test Loss: 275.4117431640625\n",
            "Epoch 29/250, Loss: 280.53460693359375, Test Loss: 275.34136962890625\n",
            "Epoch 30/250, Loss: 280.3014831542969, Test Loss: 275.27520751953125\n",
            "Epoch 31/250, Loss: 280.0646667480469, Test Loss: 275.2135925292969\n",
            "Epoch 32/250, Loss: 279.8238830566406, Test Loss: 275.15673828125\n",
            "Epoch 33/250, Loss: 279.578857421875, Test Loss: 275.1048583984375\n",
            "Epoch 34/250, Loss: 279.3294372558594, Test Loss: 275.05792236328125\n",
            "Epoch 35/250, Loss: 279.0753479003906, Test Loss: 275.0161437988281\n",
            "Epoch 36/250, Loss: 278.81640625, Test Loss: 274.9797668457031\n",
            "Epoch 37/250, Loss: 278.5522766113281, Test Loss: 274.94915771484375\n",
            "Epoch 38/250, Loss: 278.2826843261719, Test Loss: 274.92437744140625\n",
            "Epoch 39/250, Loss: 278.00738525390625, Test Loss: 274.9054870605469\n",
            "Epoch 40/250, Loss: 277.72607421875, Test Loss: 274.89263916015625\n",
            "Epoch 41/250, Loss: 277.4384765625, Test Loss: 274.8862609863281\n",
            "Epoch 42/250, Loss: 277.144287109375, Test Loss: 274.8864440917969\n",
            "Epoch 43/250, Loss: 276.8431701660156, Test Loss: 274.89324951171875\n",
            "Epoch 44/250, Loss: 276.53485107421875, Test Loss: 274.90679931640625\n",
            "Epoch 45/250, Loss: 276.21893310546875, Test Loss: 274.9275207519531\n",
            "Epoch 46/250, Loss: 275.8951721191406, Test Loss: 274.9553527832031\n",
            "Epoch 47/250, Loss: 275.5631103515625, Test Loss: 274.99029541015625\n",
            "Epoch 48/250, Loss: 275.22259521484375, Test Loss: 275.0329284667969\n",
            "Epoch 49/250, Loss: 274.8731689453125, Test Loss: 275.0831298828125\n",
            "Epoch 50/250, Loss: 274.5145263671875, Test Loss: 275.14093017578125\n",
            "Epoch 51/250, Loss: 274.1463623046875, Test Loss: 275.20684814453125\n",
            "Epoch 52/250, Loss: 273.7684020996094, Test Loss: 275.28045654296875\n",
            "Epoch 53/250, Loss: 273.3802795410156, Test Loss: 275.3625793457031\n",
            "Epoch 54/250, Loss: 272.9818420410156, Test Loss: 275.45263671875\n",
            "Epoch 55/250, Loss: 272.57269287109375, Test Loss: 275.55181884765625\n",
            "Epoch 56/250, Loss: 272.1525573730469, Test Loss: 275.6585998535156\n",
            "Epoch 57/250, Loss: 271.7213134765625, Test Loss: 275.7757568359375\n",
            "Epoch 58/250, Loss: 271.27862548828125, Test Loss: 275.9002380371094\n",
            "Epoch 59/250, Loss: 270.8243408203125, Test Loss: 276.03509521484375\n",
            "Epoch 60/250, Loss: 270.3582458496094, Test Loss: 276.1794738769531\n",
            "Epoch 61/250, Loss: 269.8800964355469, Test Loss: 276.3325500488281\n",
            "Epoch 62/250, Loss: 269.3897705078125, Test Loss: 276.4974365234375\n",
            "Epoch 63/250, Loss: 268.88714599609375, Test Loss: 276.6716003417969\n",
            "Epoch 64/250, Loss: 268.3720397949219, Test Loss: 276.8572082519531\n",
            "Epoch 65/250, Loss: 267.8442687988281, Test Loss: 277.0552978515625\n",
            "Epoch 66/250, Loss: 267.3038024902344, Test Loss: 277.263916015625\n",
            "Epoch 67/250, Loss: 266.7505187988281, Test Loss: 277.4870300292969\n",
            "Epoch 68/250, Loss: 266.1843566894531, Test Loss: 277.72186279296875\n",
            "Epoch 69/250, Loss: 265.60528564453125, Test Loss: 277.97039794921875\n",
            "Epoch 70/250, Loss: 265.0131530761719, Test Loss: 278.2331848144531\n",
            "Epoch 71/250, Loss: 264.40802001953125, Test Loss: 278.50762939453125\n",
            "Epoch 72/250, Loss: 263.7897644042969, Test Loss: 278.7972717285156\n",
            "Epoch 73/250, Loss: 263.158447265625, Test Loss: 279.0975646972656\n",
            "Epoch 74/250, Loss: 262.5140686035156, Test Loss: 279.41094970703125\n",
            "Epoch 75/250, Loss: 261.8565673828125, Test Loss: 279.7358703613281\n",
            "Epoch 76/250, Loss: 261.1860046386719, Test Loss: 280.0695495605469\n",
            "Epoch 77/250, Loss: 260.5023498535156, Test Loss: 280.41473388671875\n",
            "Epoch 78/250, Loss: 259.8055725097656, Test Loss: 280.76470947265625\n",
            "Epoch 79/250, Loss: 259.0956726074219, Test Loss: 281.1230163574219\n",
            "Epoch 80/250, Loss: 258.3725891113281, Test Loss: 281.4857482910156\n",
            "Epoch 81/250, Loss: 257.6363220214844, Test Loss: 281.8501892089844\n",
            "Epoch 82/250, Loss: 256.8867492675781, Test Loss: 282.21917724609375\n",
            "Epoch 83/250, Loss: 256.1238708496094, Test Loss: 282.58648681640625\n",
            "Epoch 84/250, Loss: 255.3474884033203, Test Loss: 282.9548034667969\n",
            "Epoch 85/250, Loss: 254.55751037597656, Test Loss: 283.32269287109375\n",
            "Epoch 86/250, Loss: 253.75375366210938, Test Loss: 283.6866149902344\n",
            "Epoch 87/250, Loss: 252.9361114501953, Test Loss: 284.050537109375\n",
            "Epoch 88/250, Loss: 252.10438537597656, Test Loss: 284.4086608886719\n",
            "Epoch 89/250, Loss: 251.25823974609375, Test Loss: 284.7625732421875\n",
            "Epoch 90/250, Loss: 250.39752197265625, Test Loss: 285.11138916015625\n",
            "Epoch 91/250, Loss: 249.52195739746094, Test Loss: 285.45013427734375\n",
            "Epoch 92/250, Loss: 248.6311798095703, Test Loss: 285.7821960449219\n",
            "Epoch 93/250, Loss: 247.7249298095703, Test Loss: 286.0992126464844\n",
            "Epoch 94/250, Loss: 246.8028564453125, Test Loss: 286.4023132324219\n",
            "Epoch 95/250, Loss: 245.86471557617188, Test Loss: 286.68701171875\n",
            "Epoch 96/250, Loss: 244.9100341796875, Test Loss: 286.9473571777344\n",
            "Epoch 97/250, Loss: 243.93865966796875, Test Loss: 287.18438720703125\n",
            "Epoch 98/250, Loss: 242.95016479492188, Test Loss: 287.38885498046875\n",
            "Epoch 99/250, Loss: 241.94427490234375, Test Loss: 287.5614318847656\n",
            "Epoch 100/250, Loss: 240.92063903808594, Test Loss: 287.6976013183594\n",
            "Epoch 101/250, Loss: 239.87890625, Test Loss: 287.7928771972656\n",
            "Epoch 102/250, Loss: 238.81887817382812, Test Loss: 287.8493957519531\n",
            "Epoch 103/250, Loss: 237.7401885986328, Test Loss: 287.8598937988281\n",
            "Epoch 104/250, Loss: 236.642578125, Test Loss: 287.82635498046875\n",
            "Epoch 105/250, Loss: 235.52574157714844, Test Loss: 287.74420166015625\n",
            "Epoch 106/250, Loss: 234.38943481445312, Test Loss: 287.60955810546875\n",
            "Epoch 107/250, Loss: 233.23338317871094, Test Loss: 287.4231262207031\n",
            "Epoch 108/250, Loss: 232.05728149414062, Test Loss: 287.1770324707031\n",
            "Epoch 109/250, Loss: 230.86077880859375, Test Loss: 286.8766174316406\n",
            "Epoch 110/250, Loss: 229.64353942871094, Test Loss: 286.51751708984375\n",
            "Epoch 111/250, Loss: 228.40521240234375, Test Loss: 286.10272216796875\n",
            "Epoch 112/250, Loss: 227.14541625976562, Test Loss: 285.6372985839844\n",
            "Epoch 113/250, Loss: 225.8637237548828, Test Loss: 285.1175231933594\n",
            "Epoch 114/250, Loss: 224.55963134765625, Test Loss: 284.55084228515625\n",
            "Epoch 115/250, Loss: 223.23269653320312, Test Loss: 283.9288330078125\n",
            "Epoch 116/250, Loss: 221.88226318359375, Test Loss: 283.25042724609375\n",
            "Epoch 117/250, Loss: 220.50784301757812, Test Loss: 282.5131530761719\n",
            "Epoch 118/250, Loss: 219.10865783691406, Test Loss: 281.70819091796875\n",
            "Epoch 119/250, Loss: 217.68414306640625, Test Loss: 280.8415222167969\n",
            "Epoch 120/250, Loss: 216.23345947265625, Test Loss: 279.90423583984375\n",
            "Epoch 121/250, Loss: 214.7559356689453, Test Loss: 278.8990783691406\n",
            "Epoch 122/250, Loss: 213.25079345703125, Test Loss: 277.82275390625\n",
            "Epoch 123/250, Loss: 211.71731567382812, Test Loss: 276.66522216796875\n",
            "Epoch 124/250, Loss: 210.15484619140625, Test Loss: 275.4302978515625\n",
            "Epoch 125/250, Loss: 208.562744140625, Test Loss: 274.10504150390625\n",
            "Epoch 126/250, Loss: 206.94039916992188, Test Loss: 272.7006530761719\n",
            "Epoch 127/250, Loss: 205.2871856689453, Test Loss: 271.21563720703125\n",
            "Epoch 128/250, Loss: 203.60256958007812, Test Loss: 269.6501159667969\n",
            "Epoch 129/250, Loss: 201.8860626220703, Test Loss: 268.0113830566406\n",
            "Epoch 130/250, Loss: 200.13714599609375, Test Loss: 266.2822570800781\n",
            "Epoch 131/250, Loss: 198.3554229736328, Test Loss: 264.47821044921875\n",
            "Epoch 132/250, Loss: 196.5405731201172, Test Loss: 262.5889587402344\n",
            "Epoch 133/250, Loss: 194.69235229492188, Test Loss: 260.6221008300781\n",
            "Epoch 134/250, Loss: 192.81048583984375, Test Loss: 258.58416748046875\n",
            "Epoch 135/250, Loss: 190.894775390625, Test Loss: 256.4546203613281\n",
            "Epoch 136/250, Loss: 188.945068359375, Test Loss: 254.25489807128906\n",
            "Epoch 137/250, Loss: 186.961181640625, Test Loss: 251.96742248535156\n",
            "Epoch 138/250, Loss: 184.94317626953125, Test Loss: 249.61181640625\n",
            "Epoch 139/250, Loss: 182.89111328125, Test Loss: 247.19105529785156\n",
            "Epoch 140/250, Loss: 180.80520629882812, Test Loss: 244.6822052001953\n",
            "Epoch 141/250, Loss: 178.6856689453125, Test Loss: 242.11587524414062\n",
            "Epoch 142/250, Loss: 176.5329132080078, Test Loss: 239.4575958251953\n",
            "Epoch 143/250, Loss: 174.3473663330078, Test Loss: 236.74545288085938\n",
            "Epoch 144/250, Loss: 172.1295928955078, Test Loss: 233.95045471191406\n",
            "Epoch 145/250, Loss: 169.880126953125, Test Loss: 231.06593322753906\n",
            "Epoch 146/250, Loss: 167.59962463378906, Test Loss: 228.1289825439453\n",
            "Epoch 147/250, Loss: 165.28858947753906, Test Loss: 225.0972137451172\n",
            "Epoch 148/250, Loss: 162.94764709472656, Test Loss: 222.04086303710938\n",
            "Epoch 149/250, Loss: 160.57737731933594, Test Loss: 218.88162231445312\n",
            "Epoch 150/250, Loss: 158.1784210205078, Test Loss: 215.69203186035156\n",
            "Epoch 151/250, Loss: 155.75143432617188, Test Loss: 212.44430541992188\n",
            "Epoch 152/250, Loss: 153.297119140625, Test Loss: 209.10494995117188\n",
            "Epoch 153/250, Loss: 150.8162384033203, Test Loss: 205.7460479736328\n",
            "Epoch 154/250, Loss: 148.3096160888672, Test Loss: 202.27479553222656\n",
            "Epoch 155/250, Loss: 145.7781982421875, Test Loss: 198.82530212402344\n",
            "Epoch 156/250, Loss: 143.22296142578125, Test Loss: 195.26768493652344\n",
            "Epoch 157/250, Loss: 140.64500427246094, Test Loss: 191.6980743408203\n",
            "Epoch 158/250, Loss: 138.04542541503906, Test Loss: 188.11045837402344\n",
            "Epoch 159/250, Loss: 135.42538452148438, Test Loss: 184.4207305908203\n",
            "Epoch 160/250, Loss: 132.78594970703125, Test Loss: 180.75628662109375\n",
            "Epoch 161/250, Loss: 130.1282501220703, Test Loss: 176.97840881347656\n",
            "Epoch 162/250, Loss: 127.45343780517578, Test Loss: 173.24124145507812\n",
            "Epoch 163/250, Loss: 124.76277160644531, Test Loss: 169.4568328857422\n",
            "Epoch 164/250, Loss: 122.05757904052734, Test Loss: 165.63966369628906\n",
            "Epoch 165/250, Loss: 119.33920288085938, Test Loss: 161.87535095214844\n",
            "Epoch 166/250, Loss: 116.60903930664062, Test Loss: 158.01268005371094\n",
            "Epoch 167/250, Loss: 113.86852264404297, Test Loss: 154.23440551757812\n",
            "Epoch 168/250, Loss: 111.11902618408203, Test Loss: 150.35813903808594\n",
            "Epoch 169/250, Loss: 108.362060546875, Test Loss: 146.54037475585938\n",
            "Epoch 170/250, Loss: 105.59911346435547, Test Loss: 142.71371459960938\n",
            "Epoch 171/250, Loss: 102.83173370361328, Test Loss: 138.82611083984375\n",
            "Epoch 172/250, Loss: 100.06148529052734, Test Loss: 135.01272583007812\n",
            "Epoch 173/250, Loss: 97.29006958007812, Test Loss: 131.0854949951172\n",
            "Epoch 174/250, Loss: 94.51922607421875, Test Loss: 127.2741928100586\n",
            "Epoch 175/250, Loss: 91.75089263916016, Test Loss: 123.34982299804688\n",
            "Epoch 176/250, Loss: 88.98709106445312, Test Loss: 119.50804901123047\n",
            "Epoch 177/250, Loss: 86.2300796508789, Test Loss: 115.67707061767578\n",
            "Epoch 178/250, Loss: 83.48212432861328, Test Loss: 111.82105255126953\n",
            "Epoch 179/250, Loss: 80.74552154541016, Test Loss: 108.08489990234375\n",
            "Epoch 180/250, Loss: 78.02269744873047, Test Loss: 104.27494812011719\n",
            "Epoch 181/250, Loss: 75.3160629272461, Test Loss: 100.65426635742188\n",
            "Epoch 182/250, Loss: 72.6280746459961, Test Loss: 96.91937255859375\n",
            "Epoch 183/250, Loss: 69.96131134033203, Test Loss: 93.35385131835938\n",
            "Epoch 184/250, Loss: 67.31853485107422, Test Loss: 89.77589416503906\n",
            "Epoch 185/250, Loss: 64.70252990722656, Test Loss: 86.20376586914062\n",
            "Epoch 186/250, Loss: 62.115882873535156, Test Loss: 82.77494812011719\n",
            "Epoch 187/250, Loss: 59.56110763549805, Test Loss: 79.23412322998047\n",
            "Epoch 188/250, Loss: 57.04065704345703, Test Loss: 75.88745880126953\n",
            "Epoch 189/250, Loss: 54.55704879760742, Test Loss: 72.4564437866211\n",
            "Epoch 190/250, Loss: 52.11281204223633, Test Loss: 69.15534973144531\n",
            "Epoch 191/250, Loss: 49.71062088012695, Test Loss: 65.89190673828125\n",
            "Epoch 192/250, Loss: 47.35287857055664, Test Loss: 62.62869644165039\n",
            "Epoch 193/250, Loss: 45.0418815612793, Test Loss: 59.55530548095703\n",
            "Epoch 194/250, Loss: 42.779842376708984, Test Loss: 56.371971130371094\n",
            "Epoch 195/250, Loss: 40.56876754760742, Test Loss: 53.42898941040039\n",
            "Epoch 196/250, Loss: 38.410682678222656, Test Loss: 50.40667724609375\n",
            "Epoch 197/250, Loss: 36.307613372802734, Test Loss: 47.53501892089844\n",
            "Epoch 198/250, Loss: 34.26163864135742, Test Loss: 44.730987548828125\n",
            "Epoch 199/250, Loss: 32.274635314941406, Test Loss: 41.93606948852539\n",
            "Epoch 200/250, Loss: 30.348255157470703, Test Loss: 39.34154510498047\n",
            "Epoch 201/250, Loss: 28.483966827392578, Test Loss: 36.68751907348633\n",
            "Epoch 202/250, Loss: 26.683130264282227, Test Loss: 34.27997589111328\n",
            "Epoch 203/250, Loss: 24.946990966796875, Test Loss: 31.840238571166992\n",
            "Epoch 204/250, Loss: 23.276702880859375, Test Loss: 29.570262908935547\n",
            "Epoch 205/250, Loss: 21.673439025878906, Test Loss: 27.402599334716797\n",
            "Epoch 206/250, Loss: 20.137943267822266, Test Loss: 25.277788162231445\n",
            "Epoch 207/250, Loss: 18.67060089111328, Test Loss: 23.360788345336914\n",
            "Epoch 208/250, Loss: 17.27176284790039, Test Loss: 21.42872428894043\n",
            "Epoch 209/250, Loss: 15.941336631774902, Test Loss: 19.704668045043945\n",
            "Epoch 210/250, Loss: 14.678977012634277, Test Loss: 18.003812789916992\n",
            "Epoch 211/250, Loss: 13.484390258789062, Test Loss: 16.45901870727539\n",
            "Epoch 212/250, Loss: 12.357217788696289, Test Loss: 14.97179126739502\n",
            "Epoch 213/250, Loss: 11.296788215637207, Test Loss: 13.598931312561035\n",
            "Epoch 214/250, Loss: 10.302189826965332, Test Loss: 12.3379545211792\n",
            "Epoch 215/250, Loss: 9.372150421142578, Test Loss: 11.100032806396484\n",
            "Epoch 216/250, Loss: 8.505258560180664, Test Loss: 10.056077003479004\n",
            "Epoch 217/250, Loss: 7.699934959411621, Test Loss: 8.979652404785156\n",
            "Epoch 218/250, Loss: 6.954254627227783, Test Loss: 8.065033912658691\n",
            "Epoch 219/250, Loss: 6.266443729400635, Test Loss: 7.194756984710693\n",
            "Epoch 220/250, Loss: 5.634483814239502, Test Loss: 6.3975372314453125\n",
            "Epoch 221/250, Loss: 5.056375503540039, Test Loss: 5.687741279602051\n",
            "Epoch 222/250, Loss: 4.529740810394287, Test Loss: 5.02325963973999\n",
            "Epoch 223/250, Loss: 4.05201530456543, Test Loss: 4.457329273223877\n",
            "Epoch 224/250, Loss: 3.6208128929138184, Test Loss: 3.9071338176727295\n",
            "Epoch 225/250, Loss: 3.233513832092285, Test Loss: 3.464200973510742\n",
            "Epoch 226/250, Loss: 2.887303352355957, Test Loss: 3.028251886367798\n",
            "Epoch 227/250, Loss: 2.579684019088745, Test Loss: 2.676137685775757\n",
            "Epoch 228/250, Loss: 2.3081412315368652, Test Loss: 2.348803997039795\n",
            "Epoch 229/250, Loss: 2.069925546646118, Test Loss: 2.0764055252075195\n",
            "Epoch 230/250, Loss: 1.8625510931015015, Test Loss: 1.8419232368469238\n",
            "Epoch 231/250, Loss: 1.6834850311279297, Test Loss: 1.6349215507507324\n",
            "Epoch 232/250, Loss: 1.530025601387024, Test Loss: 1.4832874536514282\n",
            "Epoch 233/250, Loss: 1.3997458219528198, Test Loss: 1.329337239265442\n",
            "Epoch 234/250, Loss: 1.290360927581787, Test Loss: 1.234740972518921\n",
            "Epoch 235/250, Loss: 1.1994885206222534, Test Loss: 1.1321513652801514\n",
            "Epoch 236/250, Loss: 1.1248888969421387, Test Loss: 1.0730278491973877\n",
            "Epoch 237/250, Loss: 1.0646895170211792, Test Loss: 1.0106549263000488\n",
            "Epoch 238/250, Loss: 1.0169926881790161, Test Loss: 0.9773252606391907\n",
            "Epoch 239/250, Loss: 0.9800482392311096, Test Loss: 0.9477398991584778\n",
            "Epoch 240/250, Loss: 0.9523670077323914, Test Loss: 0.927457869052887\n",
            "Epoch 241/250, Loss: 0.9324873089790344, Test Loss: 0.9241247773170471\n",
            "Epoch 242/250, Loss: 0.9189935326576233, Test Loss: 0.9142873287200928\n",
            "Epoch 243/250, Loss: 0.9106711149215698, Test Loss: 0.9231700897216797\n",
            "Epoch 244/250, Loss: 0.9064705967903137, Test Loss: 0.924662172794342\n",
            "Epoch 245/250, Loss: 0.905397355556488, Test Loss: 0.9393674731254578\n",
            "Epoch 246/250, Loss: 0.9066470861434937, Test Loss: 0.9447413682937622\n",
            "Epoch 247/250, Loss: 0.9096090793609619, Test Loss: 0.962704062461853\n",
            "Epoch 248/250, Loss: 0.9137018918991089, Test Loss: 0.9720773100852966\n",
            "Epoch 249/250, Loss: 0.9184557795524597, Test Loss: 0.9841824173927307\n",
            "Epoch 250/250, Loss: 0.923543393611908, Test Loss: 1.0001147985458374\n",
            "Final MSE: 1.000114917755127\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=16, epochs=1\n",
            "Epoch 1/1, Loss: 319.7654113769531, Test Loss: 281.67352294921875\n",
            "Final MSE: 281.673583984375\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=16, epochs=10\n",
            "Epoch 1/10, Loss: 283.55548095703125, Test Loss: 288.58880615234375\n",
            "Epoch 2/10, Loss: 293.9582824707031, Test Loss: 288.3137512207031\n",
            "Epoch 3/10, Loss: 262.40863037109375, Test Loss: 288.0439758300781\n",
            "Epoch 4/10, Loss: 279.2807922363281, Test Loss: 287.7728576660156\n",
            "Epoch 5/10, Loss: 295.494140625, Test Loss: 287.5048828125\n",
            "Epoch 6/10, Loss: 303.8137512207031, Test Loss: 287.2345886230469\n",
            "Epoch 7/10, Loss: 298.83636474609375, Test Loss: 286.9674987792969\n",
            "Epoch 8/10, Loss: 290.0131530761719, Test Loss: 286.698486328125\n",
            "Epoch 9/10, Loss: 299.5167541503906, Test Loss: 286.4262390136719\n",
            "Epoch 10/10, Loss: 273.6560974121094, Test Loss: 286.1562805175781\n",
            "Final MSE: 286.1562805175781\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=16, epochs=25\n",
            "Epoch 1/25, Loss: 281.9967041015625, Test Loss: 289.8061218261719\n",
            "Epoch 2/25, Loss: 297.0726318359375, Test Loss: 289.4454345703125\n",
            "Epoch 3/25, Loss: 302.7563171386719, Test Loss: 289.08135986328125\n",
            "Epoch 4/25, Loss: 300.2962341308594, Test Loss: 288.7317810058594\n",
            "Epoch 5/25, Loss: 311.8142395019531, Test Loss: 288.3801574707031\n",
            "Epoch 6/25, Loss: 299.7822570800781, Test Loss: 288.0334777832031\n",
            "Epoch 7/25, Loss: 292.4419250488281, Test Loss: 287.6889953613281\n",
            "Epoch 8/25, Loss: 301.0998229980469, Test Loss: 287.3418884277344\n",
            "Epoch 9/25, Loss: 291.38861083984375, Test Loss: 286.99786376953125\n",
            "Epoch 10/25, Loss: 300.9460754394531, Test Loss: 286.6551208496094\n",
            "Epoch 11/25, Loss: 300.0317077636719, Test Loss: 286.31048583984375\n",
            "Epoch 12/25, Loss: 279.7441101074219, Test Loss: 285.9698181152344\n",
            "Epoch 13/25, Loss: 298.6634216308594, Test Loss: 285.63287353515625\n",
            "Epoch 14/25, Loss: 298.9601745605469, Test Loss: 285.2886047363281\n",
            "Epoch 15/25, Loss: 292.7492980957031, Test Loss: 284.9495544433594\n",
            "Epoch 16/25, Loss: 267.1110534667969, Test Loss: 284.60760498046875\n",
            "Epoch 17/25, Loss: 296.21868896484375, Test Loss: 284.2597351074219\n",
            "Epoch 18/25, Loss: 291.0067443847656, Test Loss: 283.9129638671875\n",
            "Epoch 19/25, Loss: 299.3365478515625, Test Loss: 283.55682373046875\n",
            "Epoch 20/25, Loss: 301.8364562988281, Test Loss: 283.2025451660156\n",
            "Epoch 21/25, Loss: 275.0767517089844, Test Loss: 282.8406982421875\n",
            "Epoch 22/25, Loss: 288.4317321777344, Test Loss: 282.4762268066406\n",
            "Epoch 23/25, Loss: 314.92919921875, Test Loss: 282.10003662109375\n",
            "Epoch 24/25, Loss: 307.0293884277344, Test Loss: 281.72503662109375\n",
            "Epoch 25/25, Loss: 286.3156433105469, Test Loss: 281.344482421875\n",
            "Final MSE: 281.3444519042969\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=16, epochs=50\n",
            "Epoch 1/50, Loss: 283.8242492675781, Test Loss: 276.1421203613281\n",
            "Epoch 2/50, Loss: 278.8744201660156, Test Loss: 275.7862548828125\n",
            "Epoch 3/50, Loss: 275.9336853027344, Test Loss: 275.4459533691406\n",
            "Epoch 4/50, Loss: 273.9217834472656, Test Loss: 275.08795166015625\n",
            "Epoch 5/50, Loss: 268.8791809082031, Test Loss: 274.7430114746094\n",
            "Epoch 6/50, Loss: 273.2143249511719, Test Loss: 274.4048767089844\n",
            "Epoch 7/50, Loss: 278.4753112792969, Test Loss: 274.0603332519531\n",
            "Epoch 8/50, Loss: 288.9622497558594, Test Loss: 273.70440673828125\n",
            "Epoch 9/50, Loss: 277.7823486328125, Test Loss: 273.3544921875\n",
            "Epoch 10/50, Loss: 280.6550598144531, Test Loss: 272.99530029296875\n",
            "Epoch 11/50, Loss: 275.5729064941406, Test Loss: 272.64056396484375\n",
            "Epoch 12/50, Loss: 283.3931884765625, Test Loss: 272.2800598144531\n",
            "Epoch 13/50, Loss: 255.74632263183594, Test Loss: 271.919677734375\n",
            "Epoch 14/50, Loss: 265.7315368652344, Test Loss: 271.5626525878906\n",
            "Epoch 15/50, Loss: 288.18408203125, Test Loss: 271.188720703125\n",
            "Epoch 16/50, Loss: 280.05120849609375, Test Loss: 270.8090515136719\n",
            "Epoch 17/50, Loss: 274.306640625, Test Loss: 270.43536376953125\n",
            "Epoch 18/50, Loss: 295.8354797363281, Test Loss: 270.0514831542969\n",
            "Epoch 19/50, Loss: 273.4076843261719, Test Loss: 269.6617431640625\n",
            "Epoch 20/50, Loss: 289.664794921875, Test Loss: 269.2545166015625\n",
            "Epoch 21/50, Loss: 292.3077697753906, Test Loss: 268.8570251464844\n",
            "Epoch 22/50, Loss: 294.19915771484375, Test Loss: 268.44140625\n",
            "Epoch 23/50, Loss: 285.5152282714844, Test Loss: 268.01763916015625\n",
            "Epoch 24/50, Loss: 264.44586181640625, Test Loss: 267.5914306640625\n",
            "Epoch 25/50, Loss: 263.46722412109375, Test Loss: 267.162353515625\n",
            "Epoch 26/50, Loss: 295.137939453125, Test Loss: 266.71539306640625\n",
            "Epoch 27/50, Loss: 280.2947082519531, Test Loss: 266.2444152832031\n",
            "Epoch 28/50, Loss: 256.8426818847656, Test Loss: 265.76190185546875\n",
            "Epoch 29/50, Loss: 284.3308410644531, Test Loss: 265.2809143066406\n",
            "Epoch 30/50, Loss: 267.7189025878906, Test Loss: 264.7828674316406\n",
            "Epoch 31/50, Loss: 282.72222900390625, Test Loss: 264.2691955566406\n",
            "Epoch 32/50, Loss: 291.33819580078125, Test Loss: 263.7216796875\n",
            "Epoch 33/50, Loss: 279.8560791015625, Test Loss: 263.17340087890625\n",
            "Epoch 34/50, Loss: 279.8006591796875, Test Loss: 262.5922546386719\n",
            "Epoch 35/50, Loss: 276.8687438964844, Test Loss: 261.9823913574219\n",
            "Epoch 36/50, Loss: 261.0834655761719, Test Loss: 261.3621826171875\n",
            "Epoch 37/50, Loss: 260.6364440917969, Test Loss: 260.7276611328125\n",
            "Epoch 38/50, Loss: 265.52239990234375, Test Loss: 260.0530090332031\n",
            "Epoch 39/50, Loss: 269.0316162109375, Test Loss: 259.3608703613281\n",
            "Epoch 40/50, Loss: 261.8431091308594, Test Loss: 258.61907958984375\n",
            "Epoch 41/50, Loss: 247.1941375732422, Test Loss: 257.8563537597656\n",
            "Epoch 42/50, Loss: 263.3266906738281, Test Loss: 257.0605773925781\n",
            "Epoch 43/50, Loss: 264.8525085449219, Test Loss: 256.2189636230469\n",
            "Epoch 44/50, Loss: 262.78277587890625, Test Loss: 255.34658813476562\n",
            "Epoch 45/50, Loss: 277.1890563964844, Test Loss: 254.42796325683594\n",
            "Epoch 46/50, Loss: 277.7477111816406, Test Loss: 253.45726013183594\n",
            "Epoch 47/50, Loss: 257.1145324707031, Test Loss: 252.4586639404297\n",
            "Epoch 48/50, Loss: 267.1026611328125, Test Loss: 251.42555236816406\n",
            "Epoch 49/50, Loss: 252.1660614013672, Test Loss: 250.3099822998047\n",
            "Epoch 50/50, Loss: 262.8104553222656, Test Loss: 249.1291046142578\n",
            "Final MSE: 249.1291046142578\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=16, epochs=100\n",
            "Epoch 1/100, Loss: 268.86688232421875, Test Loss: 266.4114685058594\n",
            "Epoch 2/100, Loss: 268.96038818359375, Test Loss: 265.98248291015625\n",
            "Epoch 3/100, Loss: 274.2660827636719, Test Loss: 265.5457763671875\n",
            "Epoch 4/100, Loss: 265.45953369140625, Test Loss: 265.0924987792969\n",
            "Epoch 5/100, Loss: 251.572998046875, Test Loss: 264.63848876953125\n",
            "Epoch 6/100, Loss: 277.1207580566406, Test Loss: 264.1649169921875\n",
            "Epoch 7/100, Loss: 259.2137145996094, Test Loss: 263.68060302734375\n",
            "Epoch 8/100, Loss: 265.9344177246094, Test Loss: 263.1856689453125\n",
            "Epoch 9/100, Loss: 266.3568420410156, Test Loss: 262.6741638183594\n",
            "Epoch 10/100, Loss: 282.8727111816406, Test Loss: 262.144775390625\n",
            "Epoch 11/100, Loss: 275.6694030761719, Test Loss: 261.5969543457031\n",
            "Epoch 12/100, Loss: 292.8652038574219, Test Loss: 261.03228759765625\n",
            "Epoch 13/100, Loss: 267.7314147949219, Test Loss: 260.45440673828125\n",
            "Epoch 14/100, Loss: 247.2178955078125, Test Loss: 259.8330383300781\n",
            "Epoch 15/100, Loss: 277.2329406738281, Test Loss: 259.2005920410156\n",
            "Epoch 16/100, Loss: 251.16131591796875, Test Loss: 258.5434265136719\n",
            "Epoch 17/100, Loss: 251.8627471923828, Test Loss: 257.8551025390625\n",
            "Epoch 18/100, Loss: 278.0560607910156, Test Loss: 257.1418151855469\n",
            "Epoch 19/100, Loss: 258.6601867675781, Test Loss: 256.37506103515625\n",
            "Epoch 20/100, Loss: 261.1233215332031, Test Loss: 255.60610961914062\n",
            "Epoch 21/100, Loss: 260.28564453125, Test Loss: 254.78518676757812\n",
            "Epoch 22/100, Loss: 230.4693145751953, Test Loss: 253.94020080566406\n",
            "Epoch 23/100, Loss: 265.7497253417969, Test Loss: 253.05819702148438\n",
            "Epoch 24/100, Loss: 262.9783020019531, Test Loss: 252.11642456054688\n",
            "Epoch 25/100, Loss: 239.2741241455078, Test Loss: 251.1461944580078\n",
            "Epoch 26/100, Loss: 272.8970642089844, Test Loss: 250.15771484375\n",
            "Epoch 27/100, Loss: 254.7020721435547, Test Loss: 249.0919189453125\n",
            "Epoch 28/100, Loss: 279.6280212402344, Test Loss: 248.01585388183594\n",
            "Epoch 29/100, Loss: 260.46533203125, Test Loss: 246.88401794433594\n",
            "Epoch 30/100, Loss: 240.63780212402344, Test Loss: 245.7018585205078\n",
            "Epoch 31/100, Loss: 252.64930725097656, Test Loss: 244.44898986816406\n",
            "Epoch 32/100, Loss: 253.0568084716797, Test Loss: 243.1800079345703\n",
            "Epoch 33/100, Loss: 254.86671447753906, Test Loss: 241.84829711914062\n",
            "Epoch 34/100, Loss: 244.77423095703125, Test Loss: 240.4337921142578\n",
            "Epoch 35/100, Loss: 233.88230895996094, Test Loss: 239.02627563476562\n",
            "Epoch 36/100, Loss: 235.987060546875, Test Loss: 237.54592895507812\n",
            "Epoch 37/100, Loss: 230.09173583984375, Test Loss: 235.98773193359375\n",
            "Epoch 38/100, Loss: 222.98716735839844, Test Loss: 234.3875274658203\n",
            "Epoch 39/100, Loss: 229.2236785888672, Test Loss: 232.72802734375\n",
            "Epoch 40/100, Loss: 222.71209716796875, Test Loss: 230.9874725341797\n",
            "Epoch 41/100, Loss: 225.87876892089844, Test Loss: 229.23680114746094\n",
            "Epoch 42/100, Loss: 210.8302459716797, Test Loss: 227.37823486328125\n",
            "Epoch 43/100, Loss: 242.6351318359375, Test Loss: 225.4797821044922\n",
            "Epoch 44/100, Loss: 233.66302490234375, Test Loss: 223.51419067382812\n",
            "Epoch 45/100, Loss: 231.13319396972656, Test Loss: 221.5254364013672\n",
            "Epoch 46/100, Loss: 207.91546630859375, Test Loss: 219.4314727783203\n",
            "Epoch 47/100, Loss: 221.3689422607422, Test Loss: 217.2722930908203\n",
            "Epoch 48/100, Loss: 209.3351593017578, Test Loss: 215.02139282226562\n",
            "Epoch 49/100, Loss: 205.5482635498047, Test Loss: 212.76527404785156\n",
            "Epoch 50/100, Loss: 205.4417266845703, Test Loss: 210.36956787109375\n",
            "Epoch 51/100, Loss: 206.9574737548828, Test Loss: 208.00148010253906\n",
            "Epoch 52/100, Loss: 207.7025909423828, Test Loss: 205.51806640625\n",
            "Epoch 53/100, Loss: 195.76731872558594, Test Loss: 202.94725036621094\n",
            "Epoch 54/100, Loss: 205.213134765625, Test Loss: 200.37620544433594\n",
            "Epoch 55/100, Loss: 184.27980041503906, Test Loss: 197.7238006591797\n",
            "Epoch 56/100, Loss: 167.6867218017578, Test Loss: 195.01792907714844\n",
            "Epoch 57/100, Loss: 180.163818359375, Test Loss: 192.20204162597656\n",
            "Epoch 58/100, Loss: 178.42628479003906, Test Loss: 189.3264617919922\n",
            "Epoch 59/100, Loss: 172.607177734375, Test Loss: 186.43075561523438\n",
            "Epoch 60/100, Loss: 168.05377197265625, Test Loss: 183.4705352783203\n",
            "Epoch 61/100, Loss: 206.3883514404297, Test Loss: 180.40283203125\n",
            "Epoch 62/100, Loss: 165.3061065673828, Test Loss: 177.2760772705078\n",
            "Epoch 63/100, Loss: 186.56773376464844, Test Loss: 174.15335083007812\n",
            "Epoch 64/100, Loss: 145.47830200195312, Test Loss: 170.88636779785156\n",
            "Epoch 65/100, Loss: 169.52120971679688, Test Loss: 167.58180236816406\n",
            "Epoch 66/100, Loss: 149.80540466308594, Test Loss: 164.28616333007812\n",
            "Epoch 67/100, Loss: 145.3314666748047, Test Loss: 160.84738159179688\n",
            "Epoch 68/100, Loss: 142.71018981933594, Test Loss: 157.47811889648438\n",
            "Epoch 69/100, Loss: 140.6944580078125, Test Loss: 154.00729370117188\n",
            "Epoch 70/100, Loss: 132.66896057128906, Test Loss: 150.42091369628906\n",
            "Epoch 71/100, Loss: 116.92224884033203, Test Loss: 146.8190155029297\n",
            "Epoch 72/100, Loss: 99.99954223632812, Test Loss: 143.1652374267578\n",
            "Epoch 73/100, Loss: 112.61681365966797, Test Loss: 139.5011444091797\n",
            "Epoch 74/100, Loss: 111.14720916748047, Test Loss: 135.8420867919922\n",
            "Epoch 75/100, Loss: 123.5163803100586, Test Loss: 132.1174774169922\n",
            "Epoch 76/100, Loss: 124.03275299072266, Test Loss: 128.3605194091797\n",
            "Epoch 77/100, Loss: 105.95883178710938, Test Loss: 124.54976654052734\n",
            "Epoch 78/100, Loss: 107.44408416748047, Test Loss: 120.69174194335938\n",
            "Epoch 79/100, Loss: 107.28412628173828, Test Loss: 116.9423599243164\n",
            "Epoch 80/100, Loss: 101.98246002197266, Test Loss: 113.03812408447266\n",
            "Epoch 81/100, Loss: 91.38985443115234, Test Loss: 109.08860778808594\n",
            "Epoch 82/100, Loss: 113.0550308227539, Test Loss: 105.41492462158203\n",
            "Epoch 83/100, Loss: 86.970947265625, Test Loss: 101.51769256591797\n",
            "Epoch 84/100, Loss: 65.7718276977539, Test Loss: 97.67383575439453\n",
            "Epoch 85/100, Loss: 62.646881103515625, Test Loss: 93.79241943359375\n",
            "Epoch 86/100, Loss: 63.63931655883789, Test Loss: 90.01014709472656\n",
            "Epoch 87/100, Loss: 99.27520751953125, Test Loss: 86.30135345458984\n",
            "Epoch 88/100, Loss: 83.19011688232422, Test Loss: 82.55402374267578\n",
            "Epoch 89/100, Loss: 50.259883880615234, Test Loss: 78.78485107421875\n",
            "Epoch 90/100, Loss: 53.2165641784668, Test Loss: 75.13455200195312\n",
            "Epoch 91/100, Loss: 48.50779724121094, Test Loss: 71.57696533203125\n",
            "Epoch 92/100, Loss: 48.48274612426758, Test Loss: 67.98385620117188\n",
            "Epoch 93/100, Loss: 51.215518951416016, Test Loss: 64.52061462402344\n",
            "Epoch 94/100, Loss: 51.43964767456055, Test Loss: 61.01163101196289\n",
            "Epoch 95/100, Loss: 38.48934555053711, Test Loss: 57.73307800292969\n",
            "Epoch 96/100, Loss: 35.03596878051758, Test Loss: 54.52749252319336\n",
            "Epoch 97/100, Loss: 41.9997444152832, Test Loss: 51.39737319946289\n",
            "Epoch 98/100, Loss: 38.85639190673828, Test Loss: 48.37116622924805\n",
            "Epoch 99/100, Loss: 31.394807815551758, Test Loss: 45.419158935546875\n",
            "Epoch 100/100, Loss: 37.427284240722656, Test Loss: 42.538326263427734\n",
            "Final MSE: 42.538326263427734\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=16, epochs=250\n",
            "Epoch 1/250, Loss: 272.66229248046875, Test Loss: 266.5336608886719\n",
            "Epoch 2/250, Loss: 268.2010803222656, Test Loss: 266.21099853515625\n",
            "Epoch 3/250, Loss: 280.528564453125, Test Loss: 265.8876037597656\n",
            "Epoch 4/250, Loss: 278.3280944824219, Test Loss: 265.5533142089844\n",
            "Epoch 5/250, Loss: 270.7549743652344, Test Loss: 265.2213134765625\n",
            "Epoch 6/250, Loss: 256.16510009765625, Test Loss: 264.8775329589844\n",
            "Epoch 7/250, Loss: 270.9951477050781, Test Loss: 264.54168701171875\n",
            "Epoch 8/250, Loss: 286.8750305175781, Test Loss: 264.1867980957031\n",
            "Epoch 9/250, Loss: 299.4502868652344, Test Loss: 263.8332824707031\n",
            "Epoch 10/250, Loss: 285.0376281738281, Test Loss: 263.4638671875\n",
            "Epoch 11/250, Loss: 295.0538635253906, Test Loss: 263.09527587890625\n",
            "Epoch 12/250, Loss: 283.47039794921875, Test Loss: 262.7168273925781\n",
            "Epoch 13/250, Loss: 290.9898986816406, Test Loss: 262.3216857910156\n",
            "Epoch 14/250, Loss: 272.6612548828125, Test Loss: 261.913818359375\n",
            "Epoch 15/250, Loss: 267.6036682128906, Test Loss: 261.49810791015625\n",
            "Epoch 16/250, Loss: 275.46868896484375, Test Loss: 261.07037353515625\n",
            "Epoch 17/250, Loss: 246.72837829589844, Test Loss: 260.63604736328125\n",
            "Epoch 18/250, Loss: 258.791748046875, Test Loss: 260.18560791015625\n",
            "Epoch 19/250, Loss: 274.81036376953125, Test Loss: 259.7211608886719\n",
            "Epoch 20/250, Loss: 256.2750244140625, Test Loss: 259.2427062988281\n",
            "Epoch 21/250, Loss: 244.6914520263672, Test Loss: 258.74847412109375\n",
            "Epoch 22/250, Loss: 267.510498046875, Test Loss: 258.2497863769531\n",
            "Epoch 23/250, Loss: 248.18589782714844, Test Loss: 257.7251892089844\n",
            "Epoch 24/250, Loss: 249.1862030029297, Test Loss: 257.1875\n",
            "Epoch 25/250, Loss: 254.54547119140625, Test Loss: 256.64080810546875\n",
            "Epoch 26/250, Loss: 261.3132629394531, Test Loss: 256.0673828125\n",
            "Epoch 27/250, Loss: 238.77618408203125, Test Loss: 255.47903442382812\n",
            "Epoch 28/250, Loss: 272.8580017089844, Test Loss: 254.87799072265625\n",
            "Epoch 29/250, Loss: 269.0904235839844, Test Loss: 254.2503204345703\n",
            "Epoch 30/250, Loss: 259.4989929199219, Test Loss: 253.61080932617188\n",
            "Epoch 31/250, Loss: 277.7080383300781, Test Loss: 252.947265625\n",
            "Epoch 32/250, Loss: 264.8461608886719, Test Loss: 252.27420043945312\n",
            "Epoch 33/250, Loss: 275.40350341796875, Test Loss: 251.56358337402344\n",
            "Epoch 34/250, Loss: 255.92530822753906, Test Loss: 250.839599609375\n",
            "Epoch 35/250, Loss: 260.7499084472656, Test Loss: 250.08633422851562\n",
            "Epoch 36/250, Loss: 253.0819091796875, Test Loss: 249.31581115722656\n",
            "Epoch 37/250, Loss: 266.2820739746094, Test Loss: 248.50973510742188\n",
            "Epoch 38/250, Loss: 255.5944061279297, Test Loss: 247.69216918945312\n",
            "Epoch 39/250, Loss: 247.14125061035156, Test Loss: 246.84019470214844\n",
            "Epoch 40/250, Loss: 249.2884063720703, Test Loss: 245.96070861816406\n",
            "Epoch 41/250, Loss: 246.65411376953125, Test Loss: 245.045166015625\n",
            "Epoch 42/250, Loss: 242.46241760253906, Test Loss: 244.1043243408203\n",
            "Epoch 43/250, Loss: 273.2507019042969, Test Loss: 243.1344757080078\n",
            "Epoch 44/250, Loss: 250.6998748779297, Test Loss: 242.11949157714844\n",
            "Epoch 45/250, Loss: 249.86248779296875, Test Loss: 241.09693908691406\n",
            "Epoch 46/250, Loss: 254.7617645263672, Test Loss: 240.0277099609375\n",
            "Epoch 47/250, Loss: 236.33966064453125, Test Loss: 238.9158172607422\n",
            "Epoch 48/250, Loss: 234.576171875, Test Loss: 237.76527404785156\n",
            "Epoch 49/250, Loss: 236.2871856689453, Test Loss: 236.56947326660156\n",
            "Epoch 50/250, Loss: 247.3129425048828, Test Loss: 235.3499755859375\n",
            "Epoch 51/250, Loss: 226.55894470214844, Test Loss: 234.0994415283203\n",
            "Epoch 52/250, Loss: 229.5342254638672, Test Loss: 232.78411865234375\n",
            "Epoch 53/250, Loss: 225.720947265625, Test Loss: 231.4025421142578\n",
            "Epoch 54/250, Loss: 234.8072509765625, Test Loss: 230.00172424316406\n",
            "Epoch 55/250, Loss: 216.5470733642578, Test Loss: 228.55233764648438\n",
            "Epoch 56/250, Loss: 209.03428649902344, Test Loss: 227.03805541992188\n",
            "Epoch 57/250, Loss: 234.84056091308594, Test Loss: 225.46788024902344\n",
            "Epoch 58/250, Loss: 220.7149200439453, Test Loss: 223.8648681640625\n",
            "Epoch 59/250, Loss: 228.3070068359375, Test Loss: 222.22804260253906\n",
            "Epoch 60/250, Loss: 243.15362548828125, Test Loss: 220.47509765625\n",
            "Epoch 61/250, Loss: 220.3623809814453, Test Loss: 218.6313018798828\n",
            "Epoch 62/250, Loss: 228.8078155517578, Test Loss: 216.7989501953125\n",
            "Epoch 63/250, Loss: 213.0256805419922, Test Loss: 214.89053344726562\n",
            "Epoch 64/250, Loss: 208.81739807128906, Test Loss: 212.9183349609375\n",
            "Epoch 65/250, Loss: 224.56890869140625, Test Loss: 210.84140014648438\n",
            "Epoch 66/250, Loss: 195.2296905517578, Test Loss: 208.75486755371094\n",
            "Epoch 67/250, Loss: 190.6067352294922, Test Loss: 206.56532287597656\n",
            "Epoch 68/250, Loss: 198.09019470214844, Test Loss: 204.30897521972656\n",
            "Epoch 69/250, Loss: 185.7158660888672, Test Loss: 201.9780731201172\n",
            "Epoch 70/250, Loss: 210.11729431152344, Test Loss: 199.59140014648438\n",
            "Epoch 71/250, Loss: 190.51121520996094, Test Loss: 197.1542205810547\n",
            "Epoch 72/250, Loss: 184.12684631347656, Test Loss: 194.55325317382812\n",
            "Epoch 73/250, Loss: 177.8455047607422, Test Loss: 191.986083984375\n",
            "Epoch 74/250, Loss: 194.5686492919922, Test Loss: 189.3245391845703\n",
            "Epoch 75/250, Loss: 185.31837463378906, Test Loss: 186.6117706298828\n",
            "Epoch 76/250, Loss: 171.34071350097656, Test Loss: 183.78155517578125\n",
            "Epoch 77/250, Loss: 167.84698486328125, Test Loss: 180.879150390625\n",
            "Epoch 78/250, Loss: 171.5395050048828, Test Loss: 177.91969299316406\n",
            "Epoch 79/250, Loss: 144.07203674316406, Test Loss: 174.9461212158203\n",
            "Epoch 80/250, Loss: 174.64332580566406, Test Loss: 171.9117889404297\n",
            "Epoch 81/250, Loss: 155.68096923828125, Test Loss: 168.67417907714844\n",
            "Epoch 82/250, Loss: 171.27996826171875, Test Loss: 165.4747772216797\n",
            "Epoch 83/250, Loss: 150.7533721923828, Test Loss: 162.20079040527344\n",
            "Epoch 84/250, Loss: 150.1410369873047, Test Loss: 158.8217010498047\n",
            "Epoch 85/250, Loss: 142.59439086914062, Test Loss: 155.47315979003906\n",
            "Epoch 86/250, Loss: 132.45616149902344, Test Loss: 151.86480712890625\n",
            "Epoch 87/250, Loss: 156.39442443847656, Test Loss: 148.4730987548828\n",
            "Epoch 88/250, Loss: 130.2873992919922, Test Loss: 144.88206481933594\n",
            "Epoch 89/250, Loss: 132.17066955566406, Test Loss: 141.287353515625\n",
            "Epoch 90/250, Loss: 141.71035766601562, Test Loss: 137.61767578125\n",
            "Epoch 91/250, Loss: 95.26342010498047, Test Loss: 133.82470703125\n",
            "Epoch 92/250, Loss: 132.7803955078125, Test Loss: 130.14820861816406\n",
            "Epoch 93/250, Loss: 114.7929916381836, Test Loss: 126.48043060302734\n",
            "Epoch 94/250, Loss: 100.56280517578125, Test Loss: 122.56327819824219\n",
            "Epoch 95/250, Loss: 92.5761489868164, Test Loss: 118.6282730102539\n",
            "Epoch 96/250, Loss: 101.5987548828125, Test Loss: 114.85289764404297\n",
            "Epoch 97/250, Loss: 75.99724578857422, Test Loss: 111.0372085571289\n",
            "Epoch 98/250, Loss: 98.23273468017578, Test Loss: 107.23648834228516\n",
            "Epoch 99/250, Loss: 76.38561248779297, Test Loss: 103.31893157958984\n",
            "Epoch 100/250, Loss: 80.20463562011719, Test Loss: 99.4863510131836\n",
            "Epoch 101/250, Loss: 69.75789642333984, Test Loss: 95.66123962402344\n",
            "Epoch 102/250, Loss: 77.1324234008789, Test Loss: 91.83685302734375\n",
            "Epoch 103/250, Loss: 97.89944458007812, Test Loss: 87.948486328125\n",
            "Epoch 104/250, Loss: 68.13380432128906, Test Loss: 84.154541015625\n",
            "Epoch 105/250, Loss: 51.400325775146484, Test Loss: 80.41071319580078\n",
            "Epoch 106/250, Loss: 53.2891960144043, Test Loss: 76.6463851928711\n",
            "Epoch 107/250, Loss: 59.70205307006836, Test Loss: 73.10570526123047\n",
            "Epoch 108/250, Loss: 59.05215072631836, Test Loss: 69.56974792480469\n",
            "Epoch 109/250, Loss: 40.916141510009766, Test Loss: 65.98834228515625\n",
            "Epoch 110/250, Loss: 51.84578323364258, Test Loss: 62.49590301513672\n",
            "Epoch 111/250, Loss: 39.475040435791016, Test Loss: 59.10127258300781\n",
            "Epoch 112/250, Loss: 48.208770751953125, Test Loss: 55.88944625854492\n",
            "Epoch 113/250, Loss: 39.651859283447266, Test Loss: 52.65861892700195\n",
            "Epoch 114/250, Loss: 29.08603858947754, Test Loss: 49.44658660888672\n",
            "Epoch 115/250, Loss: 33.8748893737793, Test Loss: 46.5281982421875\n",
            "Epoch 116/250, Loss: 40.75102996826172, Test Loss: 43.5842170715332\n",
            "Epoch 117/250, Loss: 17.482667922973633, Test Loss: 40.68461608886719\n",
            "Epoch 118/250, Loss: 39.897884368896484, Test Loss: 37.99934768676758\n",
            "Epoch 119/250, Loss: 21.19465446472168, Test Loss: 35.371280670166016\n",
            "Epoch 120/250, Loss: 18.59792137145996, Test Loss: 32.92145919799805\n",
            "Epoch 121/250, Loss: 19.20871353149414, Test Loss: 30.56278419494629\n",
            "Epoch 122/250, Loss: 30.72544288635254, Test Loss: 28.394859313964844\n",
            "Epoch 123/250, Loss: 19.868202209472656, Test Loss: 26.156002044677734\n",
            "Epoch 124/250, Loss: 12.887675285339355, Test Loss: 24.146617889404297\n",
            "Epoch 125/250, Loss: 9.09933853149414, Test Loss: 22.23402214050293\n",
            "Epoch 126/250, Loss: 13.907057762145996, Test Loss: 20.467044830322266\n",
            "Epoch 127/250, Loss: 9.907343864440918, Test Loss: 18.811298370361328\n",
            "Epoch 128/250, Loss: 11.071117401123047, Test Loss: 17.27178192138672\n",
            "Epoch 129/250, Loss: 7.491374969482422, Test Loss: 15.784924507141113\n",
            "Epoch 130/250, Loss: 12.278620719909668, Test Loss: 14.45312786102295\n",
            "Epoch 131/250, Loss: 5.756180286407471, Test Loss: 13.229288101196289\n",
            "Epoch 132/250, Loss: 10.565964698791504, Test Loss: 12.087454795837402\n",
            "Epoch 133/250, Loss: 10.38113021850586, Test Loss: 10.994867324829102\n",
            "Epoch 134/250, Loss: 6.351794719696045, Test Loss: 9.95879077911377\n",
            "Epoch 135/250, Loss: 3.9142701625823975, Test Loss: 9.077298164367676\n",
            "Epoch 136/250, Loss: 8.214463233947754, Test Loss: 8.249273300170898\n",
            "Epoch 137/250, Loss: 2.7833967208862305, Test Loss: 7.523894309997559\n",
            "Epoch 138/250, Loss: 5.74297571182251, Test Loss: 6.855641841888428\n",
            "Epoch 139/250, Loss: 2.584125518798828, Test Loss: 6.221982479095459\n",
            "Epoch 140/250, Loss: 3.5698306560516357, Test Loss: 5.672975540161133\n",
            "Epoch 141/250, Loss: 2.70782208442688, Test Loss: 5.166206359863281\n",
            "Epoch 142/250, Loss: 3.1661736965179443, Test Loss: 4.728702545166016\n",
            "Epoch 143/250, Loss: 3.3378524780273438, Test Loss: 4.310615062713623\n",
            "Epoch 144/250, Loss: 2.6995041370391846, Test Loss: 3.9503042697906494\n",
            "Epoch 145/250, Loss: 1.5362848043441772, Test Loss: 3.6265432834625244\n",
            "Epoch 146/250, Loss: 1.2372589111328125, Test Loss: 3.326284646987915\n",
            "Epoch 147/250, Loss: 2.3163249492645264, Test Loss: 3.0692977905273438\n",
            "Epoch 148/250, Loss: 1.580589771270752, Test Loss: 2.838376760482788\n",
            "Epoch 149/250, Loss: 2.01170015335083, Test Loss: 2.640228748321533\n",
            "Epoch 150/250, Loss: 0.9065131545066833, Test Loss: 2.452080488204956\n",
            "Epoch 151/250, Loss: 0.7403319478034973, Test Loss: 2.2931370735168457\n",
            "Epoch 152/250, Loss: 1.030242919921875, Test Loss: 2.148247241973877\n",
            "Epoch 153/250, Loss: 1.276319146156311, Test Loss: 2.0149266719818115\n",
            "Epoch 154/250, Loss: 2.1264894008636475, Test Loss: 1.9070380926132202\n",
            "Epoch 155/250, Loss: 1.9256027936935425, Test Loss: 1.8011665344238281\n",
            "Epoch 156/250, Loss: 1.1831756830215454, Test Loss: 1.7179067134857178\n",
            "Epoch 157/250, Loss: 0.5757938027381897, Test Loss: 1.6344571113586426\n",
            "Epoch 158/250, Loss: 1.0623668432235718, Test Loss: 1.5618466138839722\n",
            "Epoch 159/250, Loss: 1.4182696342468262, Test Loss: 1.4990211725234985\n",
            "Epoch 160/250, Loss: 0.6259132027626038, Test Loss: 1.445311188697815\n",
            "Epoch 161/250, Loss: 0.5558701157569885, Test Loss: 1.3921658992767334\n",
            "Epoch 162/250, Loss: 0.960109531879425, Test Loss: 1.350823998451233\n",
            "Epoch 163/250, Loss: 1.5055214166641235, Test Loss: 1.3055431842803955\n",
            "Epoch 164/250, Loss: 0.5363982915878296, Test Loss: 1.2792483568191528\n",
            "Epoch 165/250, Loss: 0.4938453137874603, Test Loss: 1.2409158945083618\n",
            "Epoch 166/250, Loss: 2.19181489944458, Test Loss: 1.2110165357589722\n",
            "Epoch 167/250, Loss: 0.6059374809265137, Test Loss: 1.1859360933303833\n",
            "Epoch 168/250, Loss: 1.7354134321212769, Test Loss: 1.1699341535568237\n",
            "Epoch 169/250, Loss: 2.037287473678589, Test Loss: 1.1451629400253296\n",
            "Epoch 170/250, Loss: 0.41771042346954346, Test Loss: 1.1196057796478271\n",
            "Epoch 171/250, Loss: 0.4892517626285553, Test Loss: 1.0960849523544312\n",
            "Epoch 172/250, Loss: 0.8269464373588562, Test Loss: 1.0884181261062622\n",
            "Epoch 173/250, Loss: 2.7593867778778076, Test Loss: 1.0746288299560547\n",
            "Epoch 174/250, Loss: 0.9885130524635315, Test Loss: 1.0621944665908813\n",
            "Epoch 175/250, Loss: 0.6011378169059753, Test Loss: 1.0498346090316772\n",
            "Epoch 176/250, Loss: 0.7612690329551697, Test Loss: 1.04049551486969\n",
            "Epoch 177/250, Loss: 0.8528441786766052, Test Loss: 1.0344574451446533\n",
            "Epoch 178/250, Loss: 1.6994305849075317, Test Loss: 1.0210951566696167\n",
            "Epoch 179/250, Loss: 0.7188425064086914, Test Loss: 1.015136957168579\n",
            "Epoch 180/250, Loss: 0.9000599980354309, Test Loss: 1.0033903121948242\n",
            "Epoch 181/250, Loss: 0.7373144626617432, Test Loss: 1.000395655632019\n",
            "Epoch 182/250, Loss: 0.7897534966468811, Test Loss: 0.9936563968658447\n",
            "Epoch 183/250, Loss: 1.2175625562667847, Test Loss: 0.9883062839508057\n",
            "Epoch 184/250, Loss: 1.300307273864746, Test Loss: 0.9812888503074646\n",
            "Epoch 185/250, Loss: 1.6758770942687988, Test Loss: 0.9765897393226624\n",
            "Epoch 186/250, Loss: 0.5113851428031921, Test Loss: 0.9678055047988892\n",
            "Epoch 187/250, Loss: 0.12027434259653091, Test Loss: 0.9678621888160706\n",
            "Epoch 188/250, Loss: 1.9838162660598755, Test Loss: 0.9605849981307983\n",
            "Epoch 189/250, Loss: 0.5018033385276794, Test Loss: 0.9630899429321289\n",
            "Epoch 190/250, Loss: 0.7961416840553284, Test Loss: 0.9580504298210144\n",
            "Epoch 191/250, Loss: 1.2623487710952759, Test Loss: 0.9562504291534424\n",
            "Epoch 192/250, Loss: 1.4518686532974243, Test Loss: 0.9512406587600708\n",
            "Epoch 193/250, Loss: 1.0166635513305664, Test Loss: 0.9410995244979858\n",
            "Epoch 194/250, Loss: 0.8479113578796387, Test Loss: 0.9467068314552307\n",
            "Epoch 195/250, Loss: 0.6867521405220032, Test Loss: 0.9448349475860596\n",
            "Epoch 196/250, Loss: 0.8533919453620911, Test Loss: 0.9335139989852905\n",
            "Epoch 197/250, Loss: 0.6771561503410339, Test Loss: 0.9382666945457458\n",
            "Epoch 198/250, Loss: 0.7160702347755432, Test Loss: 0.9312326908111572\n",
            "Epoch 199/250, Loss: 0.7377952933311462, Test Loss: 0.931031346321106\n",
            "Epoch 200/250, Loss: 0.7145318984985352, Test Loss: 0.9342952966690063\n",
            "Epoch 201/250, Loss: 0.8234543204307556, Test Loss: 0.9306201934814453\n",
            "Epoch 202/250, Loss: 1.3318225145339966, Test Loss: 0.9335607886314392\n",
            "Epoch 203/250, Loss: 0.6366298794746399, Test Loss: 0.9208296537399292\n",
            "Epoch 204/250, Loss: 0.758341372013092, Test Loss: 0.9226024150848389\n",
            "Epoch 205/250, Loss: 1.3271387815475464, Test Loss: 0.9209386110305786\n",
            "Epoch 206/250, Loss: 1.3213938474655151, Test Loss: 0.9255589246749878\n",
            "Epoch 207/250, Loss: 1.5693974494934082, Test Loss: 0.9137821197509766\n",
            "Epoch 208/250, Loss: 1.8763327598571777, Test Loss: 0.9246159791946411\n",
            "Epoch 209/250, Loss: 1.2635886669158936, Test Loss: 0.9159063696861267\n",
            "Epoch 210/250, Loss: 0.3780057430267334, Test Loss: 0.9246703386306763\n",
            "Epoch 211/250, Loss: 1.189908504486084, Test Loss: 0.9204629063606262\n",
            "Epoch 212/250, Loss: 0.850405752658844, Test Loss: 0.9186208248138428\n",
            "Epoch 213/250, Loss: 1.4121812582015991, Test Loss: 0.9242777824401855\n",
            "Epoch 214/250, Loss: 0.8206501603126526, Test Loss: 0.9214904308319092\n",
            "Epoch 215/250, Loss: 1.0999269485473633, Test Loss: 0.9089735746383667\n",
            "Epoch 216/250, Loss: 0.5156545042991638, Test Loss: 0.9104053378105164\n",
            "Epoch 217/250, Loss: 0.8500227332115173, Test Loss: 0.9151997566223145\n",
            "Epoch 218/250, Loss: 1.4698971509933472, Test Loss: 0.9102516770362854\n",
            "Epoch 219/250, Loss: 1.5900014638900757, Test Loss: 0.925558865070343\n",
            "Epoch 220/250, Loss: 0.4608669579029083, Test Loss: 0.9168030023574829\n",
            "Epoch 221/250, Loss: 0.4633966386318207, Test Loss: 0.9123414158821106\n",
            "Epoch 222/250, Loss: 1.238938331604004, Test Loss: 0.9177714586257935\n",
            "Epoch 223/250, Loss: 1.1270018815994263, Test Loss: 0.9221140146255493\n",
            "Epoch 224/250, Loss: 0.84465092420578, Test Loss: 0.9132207632064819\n",
            "Epoch 225/250, Loss: 1.3662152290344238, Test Loss: 0.91262286901474\n",
            "Epoch 226/250, Loss: 0.6510470509529114, Test Loss: 0.914627730846405\n",
            "Epoch 227/250, Loss: 0.8563864827156067, Test Loss: 0.9177446961402893\n",
            "Epoch 228/250, Loss: 1.3211573362350464, Test Loss: 0.9087216258049011\n",
            "Epoch 229/250, Loss: 0.606845498085022, Test Loss: 0.9177575707435608\n",
            "Epoch 230/250, Loss: 1.421452522277832, Test Loss: 0.9129533171653748\n",
            "Epoch 231/250, Loss: 0.6128716468811035, Test Loss: 0.9144794344902039\n",
            "Epoch 232/250, Loss: 0.49313268065452576, Test Loss: 0.91164231300354\n",
            "Epoch 233/250, Loss: 1.1854501962661743, Test Loss: 0.9158660173416138\n",
            "Epoch 234/250, Loss: 0.7902593016624451, Test Loss: 0.9115064740180969\n",
            "Epoch 235/250, Loss: 0.8471002578735352, Test Loss: 0.9126225709915161\n",
            "Epoch 236/250, Loss: 1.2030853033065796, Test Loss: 0.9099254608154297\n",
            "Epoch 237/250, Loss: 0.9352568984031677, Test Loss: 0.9136227965354919\n",
            "Epoch 238/250, Loss: 1.0886354446411133, Test Loss: 0.9227290153503418\n",
            "Epoch 239/250, Loss: 0.644169807434082, Test Loss: 0.9107851386070251\n",
            "Epoch 240/250, Loss: 1.5769346952438354, Test Loss: 0.9135753512382507\n",
            "Epoch 241/250, Loss: 0.8004691004753113, Test Loss: 0.9185232520103455\n",
            "Epoch 242/250, Loss: 0.509865403175354, Test Loss: 0.9103716015815735\n",
            "Epoch 243/250, Loss: 1.0313036441802979, Test Loss: 0.9093616604804993\n",
            "Epoch 244/250, Loss: 0.7096115946769714, Test Loss: 0.9054573178291321\n",
            "Epoch 245/250, Loss: 0.9072423577308655, Test Loss: 0.9242896437644958\n",
            "Epoch 246/250, Loss: 0.9228623509407043, Test Loss: 0.9092762470245361\n",
            "Epoch 247/250, Loss: 0.5840614438056946, Test Loss: 0.9044740200042725\n",
            "Epoch 248/250, Loss: 0.5638303160667419, Test Loss: 0.9151694178581238\n",
            "Epoch 249/250, Loss: 0.5566830635070801, Test Loss: 0.9127002358436584\n",
            "Epoch 250/250, Loss: 1.2610251903533936, Test Loss: 0.9135913252830505\n",
            "Final MSE: 0.9135913252830505\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=32, epochs=1\n",
            "Epoch 1/1, Loss: 304.17327880859375, Test Loss: 288.0696105957031\n",
            "Final MSE: 288.0697021484375\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=32, epochs=10\n",
            "Epoch 1/10, Loss: 316.614013671875, Test Loss: 294.8622131347656\n",
            "Epoch 2/10, Loss: 294.6691589355469, Test Loss: 294.6485290527344\n",
            "Epoch 3/10, Loss: 294.0711364746094, Test Loss: 294.4383544921875\n",
            "Epoch 4/10, Loss: 312.7047424316406, Test Loss: 294.2298278808594\n",
            "Epoch 5/10, Loss: 309.199951171875, Test Loss: 294.0223388671875\n",
            "Epoch 6/10, Loss: 295.0855407714844, Test Loss: 293.8184509277344\n",
            "Epoch 7/10, Loss: 299.5116882324219, Test Loss: 293.6118469238281\n",
            "Epoch 8/10, Loss: 287.04998779296875, Test Loss: 293.40997314453125\n",
            "Epoch 9/10, Loss: 307.60546875, Test Loss: 293.2088928222656\n",
            "Epoch 10/10, Loss: 299.95751953125, Test Loss: 293.00762939453125\n",
            "Final MSE: 293.0076599121094\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=32, epochs=25\n",
            "Epoch 1/25, Loss: 282.97735595703125, Test Loss: 275.3013916015625\n",
            "Epoch 2/25, Loss: 294.1485900878906, Test Loss: 275.1025695800781\n",
            "Epoch 3/25, Loss: 293.6457824707031, Test Loss: 274.8981018066406\n",
            "Epoch 4/25, Loss: 277.6840515136719, Test Loss: 274.6973876953125\n",
            "Epoch 5/25, Loss: 266.1890563964844, Test Loss: 274.4996032714844\n",
            "Epoch 6/25, Loss: 274.1698913574219, Test Loss: 274.2972106933594\n",
            "Epoch 7/25, Loss: 282.6526794433594, Test Loss: 274.10125732421875\n",
            "Epoch 8/25, Loss: 293.5323791503906, Test Loss: 273.901611328125\n",
            "Epoch 9/25, Loss: 291.6831970214844, Test Loss: 273.7063903808594\n",
            "Epoch 10/25, Loss: 284.2091979980469, Test Loss: 273.5083923339844\n",
            "Epoch 11/25, Loss: 281.1310729980469, Test Loss: 273.3094787597656\n",
            "Epoch 12/25, Loss: 290.6805725097656, Test Loss: 273.110595703125\n",
            "Epoch 13/25, Loss: 270.9904479980469, Test Loss: 272.9164733886719\n",
            "Epoch 14/25, Loss: 281.0476379394531, Test Loss: 272.7196044921875\n",
            "Epoch 15/25, Loss: 278.00799560546875, Test Loss: 272.5221862792969\n",
            "Epoch 16/25, Loss: 279.7232360839844, Test Loss: 272.32635498046875\n",
            "Epoch 17/25, Loss: 278.5684814453125, Test Loss: 272.1282043457031\n",
            "Epoch 18/25, Loss: 274.7953796386719, Test Loss: 271.9335021972656\n",
            "Epoch 19/25, Loss: 268.60736083984375, Test Loss: 271.7334289550781\n",
            "Epoch 20/25, Loss: 265.6755676269531, Test Loss: 271.5358581542969\n",
            "Epoch 21/25, Loss: 278.48004150390625, Test Loss: 271.3348083496094\n",
            "Epoch 22/25, Loss: 268.5881652832031, Test Loss: 271.13714599609375\n",
            "Epoch 23/25, Loss: 290.92791748046875, Test Loss: 270.93743896484375\n",
            "Epoch 24/25, Loss: 288.61627197265625, Test Loss: 270.73486328125\n",
            "Epoch 25/25, Loss: 271.33941650390625, Test Loss: 270.5343322753906\n",
            "Final MSE: 270.5343017578125\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=32, epochs=50\n",
            "Epoch 1/50, Loss: 272.9993896484375, Test Loss: 271.8011779785156\n",
            "Epoch 2/50, Loss: 281.3744201660156, Test Loss: 271.6748962402344\n",
            "Epoch 3/50, Loss: 283.4228210449219, Test Loss: 271.55072021484375\n",
            "Epoch 4/50, Loss: 277.6485900878906, Test Loss: 271.42767333984375\n",
            "Epoch 5/50, Loss: 259.8899230957031, Test Loss: 271.30572509765625\n",
            "Epoch 6/50, Loss: 275.601318359375, Test Loss: 271.18670654296875\n",
            "Epoch 7/50, Loss: 272.8039855957031, Test Loss: 271.06793212890625\n",
            "Epoch 8/50, Loss: 280.83575439453125, Test Loss: 270.9492492675781\n",
            "Epoch 9/50, Loss: 277.6921081542969, Test Loss: 270.8347473144531\n",
            "Epoch 10/50, Loss: 259.79351806640625, Test Loss: 270.7197570800781\n",
            "Epoch 11/50, Loss: 281.8778076171875, Test Loss: 270.6071472167969\n",
            "Epoch 12/50, Loss: 277.05255126953125, Test Loss: 270.4952697753906\n",
            "Epoch 13/50, Loss: 270.4468078613281, Test Loss: 270.383544921875\n",
            "Epoch 14/50, Loss: 278.5791931152344, Test Loss: 270.27362060546875\n",
            "Epoch 15/50, Loss: 281.4298400878906, Test Loss: 270.1656799316406\n",
            "Epoch 16/50, Loss: 267.8871154785156, Test Loss: 270.0569763183594\n",
            "Epoch 17/50, Loss: 276.5808410644531, Test Loss: 269.9508361816406\n",
            "Epoch 18/50, Loss: 279.6070861816406, Test Loss: 269.8440856933594\n",
            "Epoch 19/50, Loss: 275.4893493652344, Test Loss: 269.7386169433594\n",
            "Epoch 20/50, Loss: 289.9774475097656, Test Loss: 269.63470458984375\n",
            "Epoch 21/50, Loss: 270.66070556640625, Test Loss: 269.5301818847656\n",
            "Epoch 22/50, Loss: 284.0717468261719, Test Loss: 269.4260559082031\n",
            "Epoch 23/50, Loss: 276.9801025390625, Test Loss: 269.3225402832031\n",
            "Epoch 24/50, Loss: 278.55694580078125, Test Loss: 269.2199401855469\n",
            "Epoch 25/50, Loss: 273.995849609375, Test Loss: 269.11767578125\n",
            "Epoch 26/50, Loss: 275.5936584472656, Test Loss: 269.0147399902344\n",
            "Epoch 27/50, Loss: 276.15374755859375, Test Loss: 268.91363525390625\n",
            "Epoch 28/50, Loss: 266.1921691894531, Test Loss: 268.8099365234375\n",
            "Epoch 29/50, Loss: 286.1938781738281, Test Loss: 268.7087707519531\n",
            "Epoch 30/50, Loss: 269.255859375, Test Loss: 268.6065673828125\n",
            "Epoch 31/50, Loss: 273.5628967285156, Test Loss: 268.5040588378906\n",
            "Epoch 32/50, Loss: 288.7547912597656, Test Loss: 268.4010009765625\n",
            "Epoch 33/50, Loss: 268.7147521972656, Test Loss: 268.29827880859375\n",
            "Epoch 34/50, Loss: 267.94256591796875, Test Loss: 268.1941223144531\n",
            "Epoch 35/50, Loss: 266.89666748046875, Test Loss: 268.0899353027344\n",
            "Epoch 36/50, Loss: 281.0823059082031, Test Loss: 267.9832763671875\n",
            "Epoch 37/50, Loss: 271.04681396484375, Test Loss: 267.8770446777344\n",
            "Epoch 38/50, Loss: 283.3716735839844, Test Loss: 267.7701416015625\n",
            "Epoch 39/50, Loss: 279.3346252441406, Test Loss: 267.6606140136719\n",
            "Epoch 40/50, Loss: 263.5022277832031, Test Loss: 267.5527038574219\n",
            "Epoch 41/50, Loss: 274.4230651855469, Test Loss: 267.4422607421875\n",
            "Epoch 42/50, Loss: 281.5079040527344, Test Loss: 267.3315734863281\n",
            "Epoch 43/50, Loss: 278.7994689941406, Test Loss: 267.2175598144531\n",
            "Epoch 44/50, Loss: 268.7227783203125, Test Loss: 267.10235595703125\n",
            "Epoch 45/50, Loss: 273.8838806152344, Test Loss: 266.98590087890625\n",
            "Epoch 46/50, Loss: 279.6928405761719, Test Loss: 266.8683776855469\n",
            "Epoch 47/50, Loss: 279.2365417480469, Test Loss: 266.74658203125\n",
            "Epoch 48/50, Loss: 273.3943176269531, Test Loss: 266.62249755859375\n",
            "Epoch 49/50, Loss: 274.2148132324219, Test Loss: 266.497802734375\n",
            "Epoch 50/50, Loss: 269.0870666503906, Test Loss: 266.3713073730469\n",
            "Final MSE: 266.37127685546875\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=32, epochs=100\n",
            "Epoch 1/100, Loss: 294.92401123046875, Test Loss: 280.8978576660156\n",
            "Epoch 2/100, Loss: 284.9425354003906, Test Loss: 280.79058837890625\n",
            "Epoch 3/100, Loss: 272.73516845703125, Test Loss: 280.68267822265625\n",
            "Epoch 4/100, Loss: 293.4931640625, Test Loss: 280.5763854980469\n",
            "Epoch 5/100, Loss: 268.3059387207031, Test Loss: 280.46844482421875\n",
            "Epoch 6/100, Loss: 307.7734069824219, Test Loss: 280.3614501953125\n",
            "Epoch 7/100, Loss: 286.0301818847656, Test Loss: 280.2534484863281\n",
            "Epoch 8/100, Loss: 288.06622314453125, Test Loss: 280.14678955078125\n",
            "Epoch 9/100, Loss: 298.2710876464844, Test Loss: 280.0399169921875\n",
            "Epoch 10/100, Loss: 298.3377380371094, Test Loss: 279.9324951171875\n",
            "Epoch 11/100, Loss: 282.57672119140625, Test Loss: 279.8250732421875\n",
            "Epoch 12/100, Loss: 296.5794372558594, Test Loss: 279.7173767089844\n",
            "Epoch 13/100, Loss: 281.499755859375, Test Loss: 279.6092224121094\n",
            "Epoch 14/100, Loss: 292.8379211425781, Test Loss: 279.5009765625\n",
            "Epoch 15/100, Loss: 304.412109375, Test Loss: 279.39208984375\n",
            "Epoch 16/100, Loss: 270.2658996582031, Test Loss: 279.2836608886719\n",
            "Epoch 17/100, Loss: 276.7301330566406, Test Loss: 279.17401123046875\n",
            "Epoch 18/100, Loss: 282.7970886230469, Test Loss: 279.0635986328125\n",
            "Epoch 19/100, Loss: 277.8653869628906, Test Loss: 278.95330810546875\n",
            "Epoch 20/100, Loss: 265.335205078125, Test Loss: 278.84222412109375\n",
            "Epoch 21/100, Loss: 281.1539001464844, Test Loss: 278.7312316894531\n",
            "Epoch 22/100, Loss: 278.1932678222656, Test Loss: 278.6199645996094\n",
            "Epoch 23/100, Loss: 296.3977355957031, Test Loss: 278.50701904296875\n",
            "Epoch 24/100, Loss: 284.4519348144531, Test Loss: 278.3920593261719\n",
            "Epoch 25/100, Loss: 275.05364990234375, Test Loss: 278.2779541015625\n",
            "Epoch 26/100, Loss: 290.4681701660156, Test Loss: 278.16265869140625\n",
            "Epoch 27/100, Loss: 278.5481262207031, Test Loss: 278.0468444824219\n",
            "Epoch 28/100, Loss: 278.33074951171875, Test Loss: 277.9295654296875\n",
            "Epoch 29/100, Loss: 285.4443664550781, Test Loss: 277.8101806640625\n",
            "Epoch 30/100, Loss: 288.59381103515625, Test Loss: 277.69219970703125\n",
            "Epoch 31/100, Loss: 288.84063720703125, Test Loss: 277.57183837890625\n",
            "Epoch 32/100, Loss: 273.3557434082031, Test Loss: 277.4495544433594\n",
            "Epoch 33/100, Loss: 285.6772766113281, Test Loss: 277.32598876953125\n",
            "Epoch 34/100, Loss: 280.6552734375, Test Loss: 277.2017822265625\n",
            "Epoch 35/100, Loss: 278.1620178222656, Test Loss: 277.07659912109375\n",
            "Epoch 36/100, Loss: 279.9703369140625, Test Loss: 276.9461669921875\n",
            "Epoch 37/100, Loss: 294.793212890625, Test Loss: 276.8186340332031\n",
            "Epoch 38/100, Loss: 286.66058349609375, Test Loss: 276.6912841796875\n",
            "Epoch 39/100, Loss: 295.3222961425781, Test Loss: 276.5584411621094\n",
            "Epoch 40/100, Loss: 289.0349426269531, Test Loss: 276.421630859375\n",
            "Epoch 41/100, Loss: 272.87884521484375, Test Loss: 276.28558349609375\n",
            "Epoch 42/100, Loss: 296.0303649902344, Test Loss: 276.1471862792969\n",
            "Epoch 43/100, Loss: 285.3091125488281, Test Loss: 276.00445556640625\n",
            "Epoch 44/100, Loss: 293.7611389160156, Test Loss: 275.8636779785156\n",
            "Epoch 45/100, Loss: 272.3994445800781, Test Loss: 275.7187194824219\n",
            "Epoch 46/100, Loss: 282.6377258300781, Test Loss: 275.571533203125\n",
            "Epoch 47/100, Loss: 283.05914306640625, Test Loss: 275.4219055175781\n",
            "Epoch 48/100, Loss: 274.7044372558594, Test Loss: 275.2687072753906\n",
            "Epoch 49/100, Loss: 281.9876403808594, Test Loss: 275.1112976074219\n",
            "Epoch 50/100, Loss: 282.615966796875, Test Loss: 274.95489501953125\n",
            "Epoch 51/100, Loss: 289.5018005371094, Test Loss: 274.7947998046875\n",
            "Epoch 52/100, Loss: 269.4253234863281, Test Loss: 274.6303405761719\n",
            "Epoch 53/100, Loss: 283.78668212890625, Test Loss: 274.463134765625\n",
            "Epoch 54/100, Loss: 287.12835693359375, Test Loss: 274.2900695800781\n",
            "Epoch 55/100, Loss: 279.98553466796875, Test Loss: 274.1158142089844\n",
            "Epoch 56/100, Loss: 285.8838806152344, Test Loss: 273.9353332519531\n",
            "Epoch 57/100, Loss: 285.6126403808594, Test Loss: 273.7540588378906\n",
            "Epoch 58/100, Loss: 261.1630859375, Test Loss: 273.56829833984375\n",
            "Epoch 59/100, Loss: 289.2189636230469, Test Loss: 273.3775939941406\n",
            "Epoch 60/100, Loss: 286.7464904785156, Test Loss: 273.1871643066406\n",
            "Epoch 61/100, Loss: 269.64794921875, Test Loss: 272.9936218261719\n",
            "Epoch 62/100, Loss: 270.1957702636719, Test Loss: 272.7914733886719\n",
            "Epoch 63/100, Loss: 279.67431640625, Test Loss: 272.59161376953125\n",
            "Epoch 64/100, Loss: 274.11676025390625, Test Loss: 272.3789367675781\n",
            "Epoch 65/100, Loss: 271.6656188964844, Test Loss: 272.1644592285156\n",
            "Epoch 66/100, Loss: 278.6494140625, Test Loss: 271.94970703125\n",
            "Epoch 67/100, Loss: 282.4102478027344, Test Loss: 271.7279357910156\n",
            "Epoch 68/100, Loss: 280.0592956542969, Test Loss: 271.505615234375\n",
            "Epoch 69/100, Loss: 276.1408386230469, Test Loss: 271.26861572265625\n",
            "Epoch 70/100, Loss: 276.0955505371094, Test Loss: 271.0274963378906\n",
            "Epoch 71/100, Loss: 283.81036376953125, Test Loss: 270.78924560546875\n",
            "Epoch 72/100, Loss: 273.80218505859375, Test Loss: 270.53900146484375\n",
            "Epoch 73/100, Loss: 270.01788330078125, Test Loss: 270.2870788574219\n",
            "Epoch 74/100, Loss: 265.54754638671875, Test Loss: 270.03472900390625\n",
            "Epoch 75/100, Loss: 277.5318298339844, Test Loss: 269.77130126953125\n",
            "Epoch 76/100, Loss: 282.205078125, Test Loss: 269.4945983886719\n",
            "Epoch 77/100, Loss: 285.89166259765625, Test Loss: 269.22528076171875\n",
            "Epoch 78/100, Loss: 279.9673156738281, Test Loss: 268.943115234375\n",
            "Epoch 79/100, Loss: 271.8007507324219, Test Loss: 268.6571960449219\n",
            "Epoch 80/100, Loss: 275.3984069824219, Test Loss: 268.3589782714844\n",
            "Epoch 81/100, Loss: 268.5081481933594, Test Loss: 268.0686340332031\n",
            "Epoch 82/100, Loss: 258.5284423828125, Test Loss: 267.75250244140625\n",
            "Epoch 83/100, Loss: 275.1967468261719, Test Loss: 267.4349060058594\n",
            "Epoch 84/100, Loss: 268.94427490234375, Test Loss: 267.12835693359375\n",
            "Epoch 85/100, Loss: 267.41986083984375, Test Loss: 266.7981262207031\n",
            "Epoch 86/100, Loss: 271.05487060546875, Test Loss: 266.46234130859375\n",
            "Epoch 87/100, Loss: 264.8739318847656, Test Loss: 266.1094055175781\n",
            "Epoch 88/100, Loss: 264.9385681152344, Test Loss: 265.7659606933594\n",
            "Epoch 89/100, Loss: 267.15716552734375, Test Loss: 265.4055480957031\n",
            "Epoch 90/100, Loss: 267.7548522949219, Test Loss: 265.0229797363281\n",
            "Epoch 91/100, Loss: 268.035400390625, Test Loss: 264.65997314453125\n",
            "Epoch 92/100, Loss: 257.5890197753906, Test Loss: 264.27728271484375\n",
            "Epoch 93/100, Loss: 253.0037078857422, Test Loss: 263.8694152832031\n",
            "Epoch 94/100, Loss: 253.16221618652344, Test Loss: 263.4704284667969\n",
            "Epoch 95/100, Loss: 246.0800018310547, Test Loss: 263.04742431640625\n",
            "Epoch 96/100, Loss: 259.0849304199219, Test Loss: 262.63958740234375\n",
            "Epoch 97/100, Loss: 266.6131286621094, Test Loss: 262.2049255371094\n",
            "Epoch 98/100, Loss: 261.1322326660156, Test Loss: 261.7542419433594\n",
            "Epoch 99/100, Loss: 255.92062377929688, Test Loss: 261.2869567871094\n",
            "Epoch 100/100, Loss: 260.1776428222656, Test Loss: 260.8107604980469\n",
            "Final MSE: 260.81072998046875\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=32, epochs=250\n",
            "Epoch 1/250, Loss: 299.6718444824219, Test Loss: 276.2334899902344\n",
            "Epoch 2/250, Loss: 276.90802001953125, Test Loss: 276.1162414550781\n",
            "Epoch 3/250, Loss: 276.3435974121094, Test Loss: 276.0007629394531\n",
            "Epoch 4/250, Loss: 283.05853271484375, Test Loss: 275.8864440917969\n",
            "Epoch 5/250, Loss: 277.3593444824219, Test Loss: 275.7706298828125\n",
            "Epoch 6/250, Loss: 283.57196044921875, Test Loss: 275.6576232910156\n",
            "Epoch 7/250, Loss: 281.5133056640625, Test Loss: 275.5438537597656\n",
            "Epoch 8/250, Loss: 280.5806579589844, Test Loss: 275.42791748046875\n",
            "Epoch 9/250, Loss: 294.30377197265625, Test Loss: 275.3128356933594\n",
            "Epoch 10/250, Loss: 271.83441162109375, Test Loss: 275.2016296386719\n",
            "Epoch 11/250, Loss: 291.051025390625, Test Loss: 275.0870666503906\n",
            "Epoch 12/250, Loss: 287.4471740722656, Test Loss: 274.97271728515625\n",
            "Epoch 13/250, Loss: 286.6884460449219, Test Loss: 274.8573303222656\n",
            "Epoch 14/250, Loss: 284.468505859375, Test Loss: 274.7427978515625\n",
            "Epoch 15/250, Loss: 282.1020202636719, Test Loss: 274.6278076171875\n",
            "Epoch 16/250, Loss: 297.62164306640625, Test Loss: 274.5120849609375\n",
            "Epoch 17/250, Loss: 267.94268798828125, Test Loss: 274.3958740234375\n",
            "Epoch 18/250, Loss: 287.8881530761719, Test Loss: 274.27801513671875\n",
            "Epoch 19/250, Loss: 293.4854431152344, Test Loss: 274.16192626953125\n",
            "Epoch 20/250, Loss: 271.8647155761719, Test Loss: 274.041015625\n",
            "Epoch 21/250, Loss: 269.4461975097656, Test Loss: 273.9216003417969\n",
            "Epoch 22/250, Loss: 278.59747314453125, Test Loss: 273.8023376464844\n",
            "Epoch 23/250, Loss: 291.8082580566406, Test Loss: 273.6800842285156\n",
            "Epoch 24/250, Loss: 285.74212646484375, Test Loss: 273.5576171875\n",
            "Epoch 25/250, Loss: 278.95318603515625, Test Loss: 273.43353271484375\n",
            "Epoch 26/250, Loss: 285.99822998046875, Test Loss: 273.3066101074219\n",
            "Epoch 27/250, Loss: 281.23388671875, Test Loss: 273.17755126953125\n",
            "Epoch 28/250, Loss: 276.7528991699219, Test Loss: 273.0470275878906\n",
            "Epoch 29/250, Loss: 289.6668395996094, Test Loss: 272.9149169921875\n",
            "Epoch 30/250, Loss: 286.64404296875, Test Loss: 272.7866516113281\n",
            "Epoch 31/250, Loss: 276.5044860839844, Test Loss: 272.65240478515625\n",
            "Epoch 32/250, Loss: 274.36871337890625, Test Loss: 272.5184631347656\n",
            "Epoch 33/250, Loss: 286.5717468261719, Test Loss: 272.38079833984375\n",
            "Epoch 34/250, Loss: 296.5597229003906, Test Loss: 272.2403564453125\n",
            "Epoch 35/250, Loss: 277.23101806640625, Test Loss: 272.097900390625\n",
            "Epoch 36/250, Loss: 284.12060546875, Test Loss: 271.9555358886719\n",
            "Epoch 37/250, Loss: 275.9019470214844, Test Loss: 271.8074035644531\n",
            "Epoch 38/250, Loss: 275.8906555175781, Test Loss: 271.6562805175781\n",
            "Epoch 39/250, Loss: 279.8844299316406, Test Loss: 271.50384521484375\n",
            "Epoch 40/250, Loss: 278.16522216796875, Test Loss: 271.3518981933594\n",
            "Epoch 41/250, Loss: 288.0881652832031, Test Loss: 271.1926574707031\n",
            "Epoch 42/250, Loss: 261.64215087890625, Test Loss: 271.0292663574219\n",
            "Epoch 43/250, Loss: 277.7757568359375, Test Loss: 270.8660888671875\n",
            "Epoch 44/250, Loss: 275.5441589355469, Test Loss: 270.69879150390625\n",
            "Epoch 45/250, Loss: 273.14739990234375, Test Loss: 270.5299377441406\n",
            "Epoch 46/250, Loss: 262.07501220703125, Test Loss: 270.356689453125\n",
            "Epoch 47/250, Loss: 276.80572509765625, Test Loss: 270.1784973144531\n",
            "Epoch 48/250, Loss: 278.6148986816406, Test Loss: 269.99786376953125\n",
            "Epoch 49/250, Loss: 277.8041076660156, Test Loss: 269.8102111816406\n",
            "Epoch 50/250, Loss: 263.1587219238281, Test Loss: 269.6236877441406\n",
            "Epoch 51/250, Loss: 284.08612060546875, Test Loss: 269.42974853515625\n",
            "Epoch 52/250, Loss: 274.12933349609375, Test Loss: 269.2331237792969\n",
            "Epoch 53/250, Loss: 282.6673278808594, Test Loss: 269.0289611816406\n",
            "Epoch 54/250, Loss: 276.7052917480469, Test Loss: 268.8221130371094\n",
            "Epoch 55/250, Loss: 286.0328674316406, Test Loss: 268.6041259765625\n",
            "Epoch 56/250, Loss: 262.4086608886719, Test Loss: 268.38848876953125\n",
            "Epoch 57/250, Loss: 268.1604309082031, Test Loss: 268.1665954589844\n",
            "Epoch 58/250, Loss: 277.9930725097656, Test Loss: 267.93927001953125\n",
            "Epoch 59/250, Loss: 281.8380432128906, Test Loss: 267.6985168457031\n",
            "Epoch 60/250, Loss: 262.2013244628906, Test Loss: 267.46075439453125\n",
            "Epoch 61/250, Loss: 280.48272705078125, Test Loss: 267.2149963378906\n",
            "Epoch 62/250, Loss: 265.68280029296875, Test Loss: 266.96624755859375\n",
            "Epoch 63/250, Loss: 273.8455810546875, Test Loss: 266.7026672363281\n",
            "Epoch 64/250, Loss: 268.701416015625, Test Loss: 266.4283447265625\n",
            "Epoch 65/250, Loss: 278.80621337890625, Test Loss: 266.1564636230469\n",
            "Epoch 66/250, Loss: 270.92437744140625, Test Loss: 265.88079833984375\n",
            "Epoch 67/250, Loss: 267.3634948730469, Test Loss: 265.591064453125\n",
            "Epoch 68/250, Loss: 277.40606689453125, Test Loss: 265.2944641113281\n",
            "Epoch 69/250, Loss: 272.41632080078125, Test Loss: 264.9920654296875\n",
            "Epoch 70/250, Loss: 272.2186584472656, Test Loss: 264.6805419921875\n",
            "Epoch 71/250, Loss: 260.69635009765625, Test Loss: 264.3570861816406\n",
            "Epoch 72/250, Loss: 260.85186767578125, Test Loss: 264.0317077636719\n",
            "Epoch 73/250, Loss: 269.508056640625, Test Loss: 263.7005920410156\n",
            "Epoch 74/250, Loss: 263.94610595703125, Test Loss: 263.3541259765625\n",
            "Epoch 75/250, Loss: 261.5888671875, Test Loss: 263.0074768066406\n",
            "Epoch 76/250, Loss: 267.9966735839844, Test Loss: 262.63641357421875\n",
            "Epoch 77/250, Loss: 280.1781921386719, Test Loss: 262.2546081542969\n",
            "Epoch 78/250, Loss: 272.9435119628906, Test Loss: 261.8742370605469\n",
            "Epoch 79/250, Loss: 258.0588073730469, Test Loss: 261.4751281738281\n",
            "Epoch 80/250, Loss: 264.4804382324219, Test Loss: 261.0611572265625\n",
            "Epoch 81/250, Loss: 261.0226135253906, Test Loss: 260.6556091308594\n",
            "Epoch 82/250, Loss: 265.8487854003906, Test Loss: 260.2283020019531\n",
            "Epoch 83/250, Loss: 263.9711608886719, Test Loss: 259.7752380371094\n",
            "Epoch 84/250, Loss: 262.7298889160156, Test Loss: 259.3200378417969\n",
            "Epoch 85/250, Loss: 257.1642761230469, Test Loss: 258.8542175292969\n",
            "Epoch 86/250, Loss: 265.7590026855469, Test Loss: 258.3794860839844\n",
            "Epoch 87/250, Loss: 256.5618896484375, Test Loss: 257.8826904296875\n",
            "Epoch 88/250, Loss: 261.1864013671875, Test Loss: 257.37713623046875\n",
            "Epoch 89/250, Loss: 253.2622528076172, Test Loss: 256.8725280761719\n",
            "Epoch 90/250, Loss: 253.43150329589844, Test Loss: 256.3289794921875\n",
            "Epoch 91/250, Loss: 266.0337219238281, Test Loss: 255.77484130859375\n",
            "Epoch 92/250, Loss: 257.6031188964844, Test Loss: 255.2066650390625\n",
            "Epoch 93/250, Loss: 250.0621337890625, Test Loss: 254.63844299316406\n",
            "Epoch 94/250, Loss: 232.6448211669922, Test Loss: 254.0449981689453\n",
            "Epoch 95/250, Loss: 257.9537048339844, Test Loss: 253.43878173828125\n",
            "Epoch 96/250, Loss: 248.89224243164062, Test Loss: 252.81900024414062\n",
            "Epoch 97/250, Loss: 249.76181030273438, Test Loss: 252.17550659179688\n",
            "Epoch 98/250, Loss: 250.9068145751953, Test Loss: 251.52796936035156\n",
            "Epoch 99/250, Loss: 255.80641174316406, Test Loss: 250.86009216308594\n",
            "Epoch 100/250, Loss: 241.21920776367188, Test Loss: 250.18096923828125\n",
            "Epoch 101/250, Loss: 249.46142578125, Test Loss: 249.47134399414062\n",
            "Epoch 102/250, Loss: 245.29638671875, Test Loss: 248.7893829345703\n",
            "Epoch 103/250, Loss: 258.9159851074219, Test Loss: 248.04722595214844\n",
            "Epoch 104/250, Loss: 246.7169647216797, Test Loss: 247.26824951171875\n",
            "Epoch 105/250, Loss: 238.17469787597656, Test Loss: 246.4958038330078\n",
            "Epoch 106/250, Loss: 231.79129028320312, Test Loss: 245.71102905273438\n",
            "Epoch 107/250, Loss: 245.8169403076172, Test Loss: 244.88914489746094\n",
            "Epoch 108/250, Loss: 244.4215087890625, Test Loss: 244.06361389160156\n",
            "Epoch 109/250, Loss: 232.8974609375, Test Loss: 243.2144012451172\n",
            "Epoch 110/250, Loss: 229.71670532226562, Test Loss: 242.3470916748047\n",
            "Epoch 111/250, Loss: 225.12171936035156, Test Loss: 241.4586944580078\n",
            "Epoch 112/250, Loss: 218.19155883789062, Test Loss: 240.57577514648438\n",
            "Epoch 113/250, Loss: 229.81344604492188, Test Loss: 239.64437866210938\n",
            "Epoch 114/250, Loss: 229.3127899169922, Test Loss: 238.69496154785156\n",
            "Epoch 115/250, Loss: 244.3662872314453, Test Loss: 237.77857971191406\n",
            "Epoch 116/250, Loss: 232.96676635742188, Test Loss: 236.7908172607422\n",
            "Epoch 117/250, Loss: 231.0231170654297, Test Loss: 235.81063842773438\n",
            "Epoch 118/250, Loss: 233.2850341796875, Test Loss: 234.7950439453125\n",
            "Epoch 119/250, Loss: 238.63392639160156, Test Loss: 233.73165893554688\n",
            "Epoch 120/250, Loss: 219.90293884277344, Test Loss: 232.67884826660156\n",
            "Epoch 121/250, Loss: 224.41183471679688, Test Loss: 231.56150817871094\n",
            "Epoch 122/250, Loss: 216.676025390625, Test Loss: 230.4613800048828\n",
            "Epoch 123/250, Loss: 208.14549255371094, Test Loss: 229.3396759033203\n",
            "Epoch 124/250, Loss: 206.98472595214844, Test Loss: 228.18759155273438\n",
            "Epoch 125/250, Loss: 223.4191131591797, Test Loss: 227.05413818359375\n",
            "Epoch 126/250, Loss: 205.81936645507812, Test Loss: 225.84765625\n",
            "Epoch 127/250, Loss: 208.88124084472656, Test Loss: 224.6234130859375\n",
            "Epoch 128/250, Loss: 216.6298828125, Test Loss: 223.43492126464844\n",
            "Epoch 129/250, Loss: 193.68736267089844, Test Loss: 222.14013671875\n",
            "Epoch 130/250, Loss: 196.1809844970703, Test Loss: 220.87255859375\n",
            "Epoch 131/250, Loss: 196.8728790283203, Test Loss: 219.57550048828125\n",
            "Epoch 132/250, Loss: 199.90074157714844, Test Loss: 218.2668914794922\n",
            "Epoch 133/250, Loss: 194.0880126953125, Test Loss: 216.91485595703125\n",
            "Epoch 134/250, Loss: 196.86134338378906, Test Loss: 215.5514678955078\n",
            "Epoch 135/250, Loss: 195.37063598632812, Test Loss: 214.13409423828125\n",
            "Epoch 136/250, Loss: 174.38270568847656, Test Loss: 212.68453979492188\n",
            "Epoch 137/250, Loss: 190.23057556152344, Test Loss: 211.2303009033203\n",
            "Epoch 138/250, Loss: 173.12440490722656, Test Loss: 209.72190856933594\n",
            "Epoch 139/250, Loss: 190.3575897216797, Test Loss: 208.2751922607422\n",
            "Epoch 140/250, Loss: 178.3646697998047, Test Loss: 206.7583770751953\n",
            "Epoch 141/250, Loss: 173.1798858642578, Test Loss: 205.21188354492188\n",
            "Epoch 142/250, Loss: 190.64309692382812, Test Loss: 203.6717071533203\n",
            "Epoch 143/250, Loss: 172.96151733398438, Test Loss: 202.0071563720703\n",
            "Epoch 144/250, Loss: 176.9612579345703, Test Loss: 200.4013214111328\n",
            "Epoch 145/250, Loss: 165.43417358398438, Test Loss: 198.77560424804688\n",
            "Epoch 146/250, Loss: 162.5148162841797, Test Loss: 197.04830932617188\n",
            "Epoch 147/250, Loss: 189.04843139648438, Test Loss: 195.36050415039062\n",
            "Epoch 148/250, Loss: 166.4698944091797, Test Loss: 193.6178741455078\n",
            "Epoch 149/250, Loss: 171.4477996826172, Test Loss: 191.89666748046875\n",
            "Epoch 150/250, Loss: 159.96144104003906, Test Loss: 190.06576538085938\n",
            "Epoch 151/250, Loss: 170.5558319091797, Test Loss: 188.28643798828125\n",
            "Epoch 152/250, Loss: 139.1742401123047, Test Loss: 186.4518585205078\n",
            "Epoch 153/250, Loss: 160.1706085205078, Test Loss: 184.60008239746094\n",
            "Epoch 154/250, Loss: 159.36537170410156, Test Loss: 182.73008728027344\n",
            "Epoch 155/250, Loss: 156.96597290039062, Test Loss: 180.80812072753906\n",
            "Epoch 156/250, Loss: 184.92434692382812, Test Loss: 178.93609619140625\n",
            "Epoch 157/250, Loss: 148.13893127441406, Test Loss: 176.85531616210938\n",
            "Epoch 158/250, Loss: 150.9861602783203, Test Loss: 174.90057373046875\n",
            "Epoch 159/250, Loss: 148.85739135742188, Test Loss: 172.9142303466797\n",
            "Epoch 160/250, Loss: 162.54640197753906, Test Loss: 170.86468505859375\n",
            "Epoch 161/250, Loss: 142.34364318847656, Test Loss: 168.81341552734375\n",
            "Epoch 162/250, Loss: 135.32809448242188, Test Loss: 166.68710327148438\n",
            "Epoch 163/250, Loss: 136.41281127929688, Test Loss: 164.62603759765625\n",
            "Epoch 164/250, Loss: 129.9945526123047, Test Loss: 162.484619140625\n",
            "Epoch 165/250, Loss: 104.11073303222656, Test Loss: 160.33840942382812\n",
            "Epoch 166/250, Loss: 125.01628875732422, Test Loss: 158.2019805908203\n",
            "Epoch 167/250, Loss: 116.94871520996094, Test Loss: 156.04873657226562\n",
            "Epoch 168/250, Loss: 111.55113220214844, Test Loss: 153.87257385253906\n",
            "Epoch 169/250, Loss: 127.65303802490234, Test Loss: 151.64260864257812\n",
            "Epoch 170/250, Loss: 121.94165802001953, Test Loss: 149.419921875\n",
            "Epoch 171/250, Loss: 111.6872787475586, Test Loss: 147.1385955810547\n",
            "Epoch 172/250, Loss: 98.74859619140625, Test Loss: 144.86305236816406\n",
            "Epoch 173/250, Loss: 121.30384826660156, Test Loss: 142.6555633544922\n",
            "Epoch 174/250, Loss: 115.80838775634766, Test Loss: 140.32623291015625\n",
            "Epoch 175/250, Loss: 107.2863998413086, Test Loss: 138.06410217285156\n",
            "Epoch 176/250, Loss: 102.2939453125, Test Loss: 135.7261962890625\n",
            "Epoch 177/250, Loss: 86.77442169189453, Test Loss: 133.38394165039062\n",
            "Epoch 178/250, Loss: 112.00951385498047, Test Loss: 131.05386352539062\n",
            "Epoch 179/250, Loss: 90.03186798095703, Test Loss: 128.6236114501953\n",
            "Epoch 180/250, Loss: 101.6517105102539, Test Loss: 126.33531951904297\n",
            "Epoch 181/250, Loss: 105.08448791503906, Test Loss: 124.0179214477539\n",
            "Epoch 182/250, Loss: 91.8090591430664, Test Loss: 121.65877532958984\n",
            "Epoch 183/250, Loss: 77.50401306152344, Test Loss: 119.26798248291016\n",
            "Epoch 184/250, Loss: 71.27165222167969, Test Loss: 116.89827728271484\n",
            "Epoch 185/250, Loss: 77.15974426269531, Test Loss: 114.58277130126953\n",
            "Epoch 186/250, Loss: 81.12146759033203, Test Loss: 112.18463134765625\n",
            "Epoch 187/250, Loss: 71.06586456298828, Test Loss: 109.79147338867188\n",
            "Epoch 188/250, Loss: 78.40565490722656, Test Loss: 107.37358093261719\n",
            "Epoch 189/250, Loss: 71.63164520263672, Test Loss: 105.09113311767578\n",
            "Epoch 190/250, Loss: 67.87041473388672, Test Loss: 102.69966888427734\n",
            "Epoch 191/250, Loss: 74.96955108642578, Test Loss: 100.42259216308594\n",
            "Epoch 192/250, Loss: 59.43852996826172, Test Loss: 97.97673034667969\n",
            "Epoch 193/250, Loss: 70.8977279663086, Test Loss: 95.65974426269531\n",
            "Epoch 194/250, Loss: 79.96913146972656, Test Loss: 93.40141296386719\n",
            "Epoch 195/250, Loss: 75.1959228515625, Test Loss: 91.0767593383789\n",
            "Epoch 196/250, Loss: 62.76099395751953, Test Loss: 88.80365753173828\n",
            "Epoch 197/250, Loss: 48.40216827392578, Test Loss: 86.4454574584961\n",
            "Epoch 198/250, Loss: 64.85316467285156, Test Loss: 84.23387908935547\n",
            "Epoch 199/250, Loss: 54.98108673095703, Test Loss: 81.98912048339844\n",
            "Epoch 200/250, Loss: 57.39617156982422, Test Loss: 79.75010681152344\n",
            "Epoch 201/250, Loss: 63.128326416015625, Test Loss: 77.5020751953125\n",
            "Epoch 202/250, Loss: 54.09379196166992, Test Loss: 75.33139038085938\n",
            "Epoch 203/250, Loss: 48.184364318847656, Test Loss: 73.16183471679688\n",
            "Epoch 204/250, Loss: 47.3350830078125, Test Loss: 71.0234146118164\n",
            "Epoch 205/250, Loss: 48.59163284301758, Test Loss: 68.95108795166016\n",
            "Epoch 206/250, Loss: 41.82118606567383, Test Loss: 66.84284210205078\n",
            "Epoch 207/250, Loss: 51.45194625854492, Test Loss: 64.82080841064453\n",
            "Epoch 208/250, Loss: 42.132591247558594, Test Loss: 62.788326263427734\n",
            "Epoch 209/250, Loss: 38.53028106689453, Test Loss: 60.777854919433594\n",
            "Epoch 210/250, Loss: 34.649654388427734, Test Loss: 58.76976013183594\n",
            "Epoch 211/250, Loss: 39.945152282714844, Test Loss: 56.80636978149414\n",
            "Epoch 212/250, Loss: 39.038963317871094, Test Loss: 54.917240142822266\n",
            "Epoch 213/250, Loss: 36.562103271484375, Test Loss: 53.046714782714844\n",
            "Epoch 214/250, Loss: 27.44533348083496, Test Loss: 51.19504165649414\n",
            "Epoch 215/250, Loss: 37.310279846191406, Test Loss: 49.40096664428711\n",
            "Epoch 216/250, Loss: 27.927310943603516, Test Loss: 47.61441421508789\n",
            "Epoch 217/250, Loss: 28.073259353637695, Test Loss: 45.9227294921875\n",
            "Epoch 218/250, Loss: 27.10318946838379, Test Loss: 44.207122802734375\n",
            "Epoch 219/250, Loss: 28.80984115600586, Test Loss: 42.51628494262695\n",
            "Epoch 220/250, Loss: 27.483293533325195, Test Loss: 40.931480407714844\n",
            "Epoch 221/250, Loss: 21.54734230041504, Test Loss: 39.31685256958008\n",
            "Epoch 222/250, Loss: 30.092145919799805, Test Loss: 37.82925033569336\n",
            "Epoch 223/250, Loss: 18.316726684570312, Test Loss: 36.26677703857422\n",
            "Epoch 224/250, Loss: 26.077836990356445, Test Loss: 34.82188415527344\n",
            "Epoch 225/250, Loss: 22.881071090698242, Test Loss: 33.41533660888672\n",
            "Epoch 226/250, Loss: 16.20073890686035, Test Loss: 31.98587417602539\n",
            "Epoch 227/250, Loss: 17.87520408630371, Test Loss: 30.652143478393555\n",
            "Epoch 228/250, Loss: 24.19931411743164, Test Loss: 29.371671676635742\n",
            "Epoch 229/250, Loss: 13.312148094177246, Test Loss: 28.074935913085938\n",
            "Epoch 230/250, Loss: 16.617136001586914, Test Loss: 26.84566879272461\n",
            "Epoch 231/250, Loss: 16.192913055419922, Test Loss: 25.635705947875977\n",
            "Epoch 232/250, Loss: 15.553935050964355, Test Loss: 24.48335075378418\n",
            "Epoch 233/250, Loss: 12.29232120513916, Test Loss: 23.401247024536133\n",
            "Epoch 234/250, Loss: 14.31451416015625, Test Loss: 22.32609748840332\n",
            "Epoch 235/250, Loss: 10.095239639282227, Test Loss: 21.271595001220703\n",
            "Epoch 236/250, Loss: 9.192344665527344, Test Loss: 20.26956558227539\n",
            "Epoch 237/250, Loss: 9.623086929321289, Test Loss: 19.29860496520996\n",
            "Epoch 238/250, Loss: 10.136659622192383, Test Loss: 18.383014678955078\n",
            "Epoch 239/250, Loss: 9.273159980773926, Test Loss: 17.500131607055664\n",
            "Epoch 240/250, Loss: 12.041314125061035, Test Loss: 16.63344383239746\n",
            "Epoch 241/250, Loss: 10.446245193481445, Test Loss: 15.827178955078125\n",
            "Epoch 242/250, Loss: 10.639946937561035, Test Loss: 15.043841361999512\n",
            "Epoch 243/250, Loss: 7.176189422607422, Test Loss: 14.267620086669922\n",
            "Epoch 244/250, Loss: 5.827995777130127, Test Loss: 13.55463695526123\n",
            "Epoch 245/250, Loss: 6.732019901275635, Test Loss: 12.868000030517578\n",
            "Epoch 246/250, Loss: 7.4106526374816895, Test Loss: 12.212347984313965\n",
            "Epoch 247/250, Loss: 6.843303203582764, Test Loss: 11.592438697814941\n",
            "Epoch 248/250, Loss: 5.3360700607299805, Test Loss: 10.986795425415039\n",
            "Epoch 249/250, Loss: 6.523066520690918, Test Loss: 10.412095069885254\n",
            "Epoch 250/250, Loss: 7.374675750732422, Test Loss: 9.861078262329102\n",
            "Final MSE: 9.861078262329102\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=64, epochs=1\n",
            "Epoch 1/1, Loss: 283.1787414550781, Test Loss: 272.6317443847656\n",
            "Final MSE: 272.6317138671875\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=64, epochs=10\n",
            "Epoch 1/10, Loss: 276.0697326660156, Test Loss: 266.01800537109375\n",
            "Epoch 2/10, Loss: 275.3970947265625, Test Loss: 265.9417419433594\n",
            "Epoch 3/10, Loss: 270.1998596191406, Test Loss: 265.8650817871094\n",
            "Epoch 4/10, Loss: 272.9083251953125, Test Loss: 265.7864074707031\n",
            "Epoch 5/10, Loss: 271.5020751953125, Test Loss: 265.7099609375\n",
            "Epoch 6/10, Loss: 275.43414306640625, Test Loss: 265.63201904296875\n",
            "Epoch 7/10, Loss: 280.83203125, Test Loss: 265.55487060546875\n",
            "Epoch 8/10, Loss: 278.07598876953125, Test Loss: 265.4761962890625\n",
            "Epoch 9/10, Loss: 271.5464782714844, Test Loss: 265.3993225097656\n",
            "Epoch 10/10, Loss: 273.3916320800781, Test Loss: 265.3214111328125\n",
            "Final MSE: 265.3214111328125\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=64, epochs=25\n",
            "Epoch 1/25, Loss: 274.29364013671875, Test Loss: 275.3094482421875\n",
            "Epoch 2/25, Loss: 291.52984619140625, Test Loss: 275.23968505859375\n",
            "Epoch 3/25, Loss: 286.13116455078125, Test Loss: 275.16888427734375\n",
            "Epoch 4/25, Loss: 286.88397216796875, Test Loss: 275.09796142578125\n",
            "Epoch 5/25, Loss: 286.2168273925781, Test Loss: 275.02783203125\n",
            "Epoch 6/25, Loss: 285.4778137207031, Test Loss: 274.956298828125\n",
            "Epoch 7/25, Loss: 282.8095703125, Test Loss: 274.8841857910156\n",
            "Epoch 8/25, Loss: 289.6282653808594, Test Loss: 274.81243896484375\n",
            "Epoch 9/25, Loss: 285.12982177734375, Test Loss: 274.7408142089844\n",
            "Epoch 10/25, Loss: 282.8497314453125, Test Loss: 274.6676330566406\n",
            "Epoch 11/25, Loss: 284.6778259277344, Test Loss: 274.5944519042969\n",
            "Epoch 12/25, Loss: 280.8052673339844, Test Loss: 274.52197265625\n",
            "Epoch 13/25, Loss: 289.53472900390625, Test Loss: 274.448486328125\n",
            "Epoch 14/25, Loss: 275.6267395019531, Test Loss: 274.3759460449219\n",
            "Epoch 15/25, Loss: 289.61572265625, Test Loss: 274.30133056640625\n",
            "Epoch 16/25, Loss: 289.03375244140625, Test Loss: 274.2280578613281\n",
            "Epoch 17/25, Loss: 280.0624084472656, Test Loss: 274.152587890625\n",
            "Epoch 18/25, Loss: 278.9592590332031, Test Loss: 274.0775451660156\n",
            "Epoch 19/25, Loss: 278.8635559082031, Test Loss: 274.00201416015625\n",
            "Epoch 20/25, Loss: 276.86334228515625, Test Loss: 273.9282531738281\n",
            "Epoch 21/25, Loss: 283.5787353515625, Test Loss: 273.85357666015625\n",
            "Epoch 22/25, Loss: 279.58294677734375, Test Loss: 273.7779235839844\n",
            "Epoch 23/25, Loss: 282.2405090332031, Test Loss: 273.7008361816406\n",
            "Epoch 24/25, Loss: 274.83953857421875, Test Loss: 273.62353515625\n",
            "Epoch 25/25, Loss: 277.88092041015625, Test Loss: 273.5464172363281\n",
            "Final MSE: 273.54644775390625\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=64, epochs=50\n",
            "Epoch 1/50, Loss: 282.30902099609375, Test Loss: 281.7889099121094\n",
            "Epoch 2/50, Loss: 288.32489013671875, Test Loss: 281.7193908691406\n",
            "Epoch 3/50, Loss: 292.03948974609375, Test Loss: 281.64935302734375\n",
            "Epoch 4/50, Loss: 293.525390625, Test Loss: 281.57989501953125\n",
            "Epoch 5/50, Loss: 285.73223876953125, Test Loss: 281.510986328125\n",
            "Epoch 6/50, Loss: 283.4062194824219, Test Loss: 281.44195556640625\n",
            "Epoch 7/50, Loss: 292.30780029296875, Test Loss: 281.3736572265625\n",
            "Epoch 8/50, Loss: 295.2579345703125, Test Loss: 281.3056640625\n",
            "Epoch 9/50, Loss: 289.60260009765625, Test Loss: 281.23626708984375\n",
            "Epoch 10/50, Loss: 286.5117492675781, Test Loss: 281.167724609375\n",
            "Epoch 11/50, Loss: 287.5487976074219, Test Loss: 281.0982971191406\n",
            "Epoch 12/50, Loss: 283.9788818359375, Test Loss: 281.0301208496094\n",
            "Epoch 13/50, Loss: 281.1867980957031, Test Loss: 280.961669921875\n",
            "Epoch 14/50, Loss: 282.8915710449219, Test Loss: 280.89404296875\n",
            "Epoch 15/50, Loss: 291.2581787109375, Test Loss: 280.8243103027344\n",
            "Epoch 16/50, Loss: 285.7787780761719, Test Loss: 280.7555847167969\n",
            "Epoch 17/50, Loss: 282.7691955566406, Test Loss: 280.6866149902344\n",
            "Epoch 18/50, Loss: 292.1978454589844, Test Loss: 280.6177673339844\n",
            "Epoch 19/50, Loss: 288.9537353515625, Test Loss: 280.5487976074219\n",
            "Epoch 20/50, Loss: 296.7569580078125, Test Loss: 280.48028564453125\n",
            "Epoch 21/50, Loss: 289.31744384765625, Test Loss: 280.4109191894531\n",
            "Epoch 22/50, Loss: 283.6917724609375, Test Loss: 280.3428649902344\n",
            "Epoch 23/50, Loss: 292.8386535644531, Test Loss: 280.2722473144531\n",
            "Epoch 24/50, Loss: 290.9102478027344, Test Loss: 280.203125\n",
            "Epoch 25/50, Loss: 280.0809326171875, Test Loss: 280.134033203125\n",
            "Epoch 26/50, Loss: 282.859375, Test Loss: 280.064697265625\n",
            "Epoch 27/50, Loss: 287.8496398925781, Test Loss: 279.9953308105469\n",
            "Epoch 28/50, Loss: 282.37921142578125, Test Loss: 279.92584228515625\n",
            "Epoch 29/50, Loss: 292.0159606933594, Test Loss: 279.85601806640625\n",
            "Epoch 30/50, Loss: 294.2415466308594, Test Loss: 279.78533935546875\n",
            "Epoch 31/50, Loss: 293.8967590332031, Test Loss: 279.71563720703125\n",
            "Epoch 32/50, Loss: 293.3244934082031, Test Loss: 279.64544677734375\n",
            "Epoch 33/50, Loss: 291.8622741699219, Test Loss: 279.57440185546875\n",
            "Epoch 34/50, Loss: 283.4569091796875, Test Loss: 279.5039978027344\n",
            "Epoch 35/50, Loss: 289.5065002441406, Test Loss: 279.4327392578125\n",
            "Epoch 36/50, Loss: 286.3729248046875, Test Loss: 279.3611755371094\n",
            "Epoch 37/50, Loss: 291.4473571777344, Test Loss: 279.2918701171875\n",
            "Epoch 38/50, Loss: 297.5354309082031, Test Loss: 279.220703125\n",
            "Epoch 39/50, Loss: 279.3399353027344, Test Loss: 279.1486511230469\n",
            "Epoch 40/50, Loss: 295.6181640625, Test Loss: 279.0766296386719\n",
            "Epoch 41/50, Loss: 294.90631103515625, Test Loss: 279.0043640136719\n",
            "Epoch 42/50, Loss: 293.43951416015625, Test Loss: 278.9325256347656\n",
            "Epoch 43/50, Loss: 287.0083312988281, Test Loss: 278.85980224609375\n",
            "Epoch 44/50, Loss: 287.49932861328125, Test Loss: 278.7873229980469\n",
            "Epoch 45/50, Loss: 296.5129699707031, Test Loss: 278.71417236328125\n",
            "Epoch 46/50, Loss: 289.4830017089844, Test Loss: 278.64013671875\n",
            "Epoch 47/50, Loss: 282.26177978515625, Test Loss: 278.5659484863281\n",
            "Epoch 48/50, Loss: 283.1001281738281, Test Loss: 278.49163818359375\n",
            "Epoch 49/50, Loss: 282.9946594238281, Test Loss: 278.4169921875\n",
            "Epoch 50/50, Loss: 286.9768981933594, Test Loss: 278.3426208496094\n",
            "Final MSE: 278.3426208496094\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=64, epochs=100\n",
            "Epoch 1/100, Loss: 286.5067138671875, Test Loss: 270.5758056640625\n",
            "Epoch 2/100, Loss: 277.0517272949219, Test Loss: 270.48828125\n",
            "Epoch 3/100, Loss: 276.0756530761719, Test Loss: 270.4005126953125\n",
            "Epoch 4/100, Loss: 285.82684326171875, Test Loss: 270.3129577636719\n",
            "Epoch 5/100, Loss: 285.275390625, Test Loss: 270.2254638671875\n",
            "Epoch 6/100, Loss: 282.2819519042969, Test Loss: 270.1382141113281\n",
            "Epoch 7/100, Loss: 284.64385986328125, Test Loss: 270.05035400390625\n",
            "Epoch 8/100, Loss: 285.1153869628906, Test Loss: 269.96307373046875\n",
            "Epoch 9/100, Loss: 272.05364990234375, Test Loss: 269.87677001953125\n",
            "Epoch 10/100, Loss: 275.3558654785156, Test Loss: 269.789794921875\n",
            "Epoch 11/100, Loss: 282.9493713378906, Test Loss: 269.70306396484375\n",
            "Epoch 12/100, Loss: 269.0690612792969, Test Loss: 269.61663818359375\n",
            "Epoch 13/100, Loss: 274.9710998535156, Test Loss: 269.5309143066406\n",
            "Epoch 14/100, Loss: 273.33746337890625, Test Loss: 269.44549560546875\n",
            "Epoch 15/100, Loss: 277.9626159667969, Test Loss: 269.3600158691406\n",
            "Epoch 16/100, Loss: 281.09674072265625, Test Loss: 269.2753601074219\n",
            "Epoch 17/100, Loss: 275.1030578613281, Test Loss: 269.1885986328125\n",
            "Epoch 18/100, Loss: 280.7702331542969, Test Loss: 269.10205078125\n",
            "Epoch 19/100, Loss: 273.1988525390625, Test Loss: 269.0167541503906\n",
            "Epoch 20/100, Loss: 273.05609130859375, Test Loss: 268.9307861328125\n",
            "Epoch 21/100, Loss: 274.12115478515625, Test Loss: 268.84454345703125\n",
            "Epoch 22/100, Loss: 275.83367919921875, Test Loss: 268.7586364746094\n",
            "Epoch 23/100, Loss: 281.7217712402344, Test Loss: 268.6725769042969\n",
            "Epoch 24/100, Loss: 282.1999816894531, Test Loss: 268.5863952636719\n",
            "Epoch 25/100, Loss: 265.15106201171875, Test Loss: 268.4991149902344\n",
            "Epoch 26/100, Loss: 277.3968200683594, Test Loss: 268.4129943847656\n",
            "Epoch 27/100, Loss: 277.8751220703125, Test Loss: 268.3263854980469\n",
            "Epoch 28/100, Loss: 275.8407897949219, Test Loss: 268.2403869628906\n",
            "Epoch 29/100, Loss: 275.1798095703125, Test Loss: 268.1547546386719\n",
            "Epoch 30/100, Loss: 279.7027893066406, Test Loss: 268.0674133300781\n",
            "Epoch 31/100, Loss: 279.525390625, Test Loss: 267.98126220703125\n",
            "Epoch 32/100, Loss: 276.8642578125, Test Loss: 267.8943786621094\n",
            "Epoch 33/100, Loss: 281.2786865234375, Test Loss: 267.80670166015625\n",
            "Epoch 34/100, Loss: 278.98419189453125, Test Loss: 267.71954345703125\n",
            "Epoch 35/100, Loss: 273.77252197265625, Test Loss: 267.6321105957031\n",
            "Epoch 36/100, Loss: 273.36981201171875, Test Loss: 267.5444641113281\n",
            "Epoch 37/100, Loss: 280.3175354003906, Test Loss: 267.4565734863281\n",
            "Epoch 38/100, Loss: 264.4035339355469, Test Loss: 267.36822509765625\n",
            "Epoch 39/100, Loss: 269.5523986816406, Test Loss: 267.281494140625\n",
            "Epoch 40/100, Loss: 267.8833312988281, Test Loss: 267.19244384765625\n",
            "Epoch 41/100, Loss: 273.4884338378906, Test Loss: 267.102783203125\n",
            "Epoch 42/100, Loss: 278.81585693359375, Test Loss: 267.0149230957031\n",
            "Epoch 43/100, Loss: 268.05072021484375, Test Loss: 266.9268798828125\n",
            "Epoch 44/100, Loss: 264.8693542480469, Test Loss: 266.83819580078125\n",
            "Epoch 45/100, Loss: 280.43426513671875, Test Loss: 266.7491455078125\n",
            "Epoch 46/100, Loss: 268.6852722167969, Test Loss: 266.65850830078125\n",
            "Epoch 47/100, Loss: 274.24261474609375, Test Loss: 266.5686340332031\n",
            "Epoch 48/100, Loss: 273.88226318359375, Test Loss: 266.4774169921875\n",
            "Epoch 49/100, Loss: 267.17120361328125, Test Loss: 266.38653564453125\n",
            "Epoch 50/100, Loss: 267.80517578125, Test Loss: 266.29522705078125\n",
            "Epoch 51/100, Loss: 273.69921875, Test Loss: 266.20489501953125\n",
            "Epoch 52/100, Loss: 272.9397277832031, Test Loss: 266.1129455566406\n",
            "Epoch 53/100, Loss: 272.55859375, Test Loss: 266.0210876464844\n",
            "Epoch 54/100, Loss: 266.6596374511719, Test Loss: 265.9283142089844\n",
            "Epoch 55/100, Loss: 273.81695556640625, Test Loss: 265.8342590332031\n",
            "Epoch 56/100, Loss: 266.3660888671875, Test Loss: 265.74188232421875\n",
            "Epoch 57/100, Loss: 264.5943908691406, Test Loss: 265.64923095703125\n",
            "Epoch 58/100, Loss: 272.4366760253906, Test Loss: 265.5552062988281\n",
            "Epoch 59/100, Loss: 273.3391418457031, Test Loss: 265.46044921875\n",
            "Epoch 60/100, Loss: 266.29754638671875, Test Loss: 265.3667297363281\n",
            "Epoch 61/100, Loss: 274.6649169921875, Test Loss: 265.2719421386719\n",
            "Epoch 62/100, Loss: 274.23663330078125, Test Loss: 265.1765441894531\n",
            "Epoch 63/100, Loss: 271.1251525878906, Test Loss: 265.080078125\n",
            "Epoch 64/100, Loss: 261.97747802734375, Test Loss: 264.9822998046875\n",
            "Epoch 65/100, Loss: 267.0216369628906, Test Loss: 264.884765625\n",
            "Epoch 66/100, Loss: 273.4002685546875, Test Loss: 264.7860107421875\n",
            "Epoch 67/100, Loss: 265.10223388671875, Test Loss: 264.6880798339844\n",
            "Epoch 68/100, Loss: 270.67852783203125, Test Loss: 264.58856201171875\n",
            "Epoch 69/100, Loss: 272.9481506347656, Test Loss: 264.4898681640625\n",
            "Epoch 70/100, Loss: 274.5204162597656, Test Loss: 264.38922119140625\n",
            "Epoch 71/100, Loss: 270.7413024902344, Test Loss: 264.28729248046875\n",
            "Epoch 72/100, Loss: 268.6200866699219, Test Loss: 264.1879577636719\n",
            "Epoch 73/100, Loss: 269.4488220214844, Test Loss: 264.0860900878906\n",
            "Epoch 74/100, Loss: 261.7801208496094, Test Loss: 263.983642578125\n",
            "Epoch 75/100, Loss: 269.3453674316406, Test Loss: 263.8782043457031\n",
            "Epoch 76/100, Loss: 271.9961853027344, Test Loss: 263.7748718261719\n",
            "Epoch 77/100, Loss: 274.0118713378906, Test Loss: 263.6714172363281\n",
            "Epoch 78/100, Loss: 268.7303161621094, Test Loss: 263.565185546875\n",
            "Epoch 79/100, Loss: 268.3852844238281, Test Loss: 263.4580078125\n",
            "Epoch 80/100, Loss: 267.6124267578125, Test Loss: 263.351806640625\n",
            "Epoch 81/100, Loss: 268.6689758300781, Test Loss: 263.2456970214844\n",
            "Epoch 82/100, Loss: 266.7492980957031, Test Loss: 263.1356201171875\n",
            "Epoch 83/100, Loss: 270.71636962890625, Test Loss: 263.0258483886719\n",
            "Epoch 84/100, Loss: 266.5668640136719, Test Loss: 262.9163513183594\n",
            "Epoch 85/100, Loss: 268.37896728515625, Test Loss: 262.80560302734375\n",
            "Epoch 86/100, Loss: 267.1659851074219, Test Loss: 262.6924133300781\n",
            "Epoch 87/100, Loss: 281.3775939941406, Test Loss: 262.5792236328125\n",
            "Epoch 88/100, Loss: 264.9996643066406, Test Loss: 262.46612548828125\n",
            "Epoch 89/100, Loss: 263.95892333984375, Test Loss: 262.3503112792969\n",
            "Epoch 90/100, Loss: 267.11151123046875, Test Loss: 262.23388671875\n",
            "Epoch 91/100, Loss: 273.8994445800781, Test Loss: 262.1163635253906\n",
            "Epoch 92/100, Loss: 260.54974365234375, Test Loss: 262.0012512207031\n",
            "Epoch 93/100, Loss: 281.5418701171875, Test Loss: 261.8837890625\n",
            "Epoch 94/100, Loss: 272.6452331542969, Test Loss: 261.7651672363281\n",
            "Epoch 95/100, Loss: 262.35552978515625, Test Loss: 261.6429138183594\n",
            "Epoch 96/100, Loss: 272.7769775390625, Test Loss: 261.5206298828125\n",
            "Epoch 97/100, Loss: 259.5611572265625, Test Loss: 261.3971252441406\n",
            "Epoch 98/100, Loss: 260.1924743652344, Test Loss: 261.2737731933594\n",
            "Epoch 99/100, Loss: 264.1597900390625, Test Loss: 261.1482849121094\n",
            "Epoch 100/100, Loss: 266.3133850097656, Test Loss: 261.021728515625\n",
            "Final MSE: 261.0217590332031\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=64, epochs=250\n",
            "Epoch 1/250, Loss: 274.9331970214844, Test Loss: 264.6578674316406\n",
            "Epoch 2/250, Loss: 266.0390319824219, Test Loss: 264.6000061035156\n",
            "Epoch 3/250, Loss: 282.96136474609375, Test Loss: 264.54345703125\n",
            "Epoch 4/250, Loss: 270.4083251953125, Test Loss: 264.4863586425781\n",
            "Epoch 5/250, Loss: 265.0860900878906, Test Loss: 264.429931640625\n",
            "Epoch 6/250, Loss: 280.04193115234375, Test Loss: 264.3733215332031\n",
            "Epoch 7/250, Loss: 279.3729248046875, Test Loss: 264.31689453125\n",
            "Epoch 8/250, Loss: 271.4582214355469, Test Loss: 264.26055908203125\n",
            "Epoch 9/250, Loss: 260.9328918457031, Test Loss: 264.20477294921875\n",
            "Epoch 10/250, Loss: 268.19696044921875, Test Loss: 264.148681640625\n",
            "Epoch 11/250, Loss: 275.67034912109375, Test Loss: 264.0931396484375\n",
            "Epoch 12/250, Loss: 271.0284118652344, Test Loss: 264.0372619628906\n",
            "Epoch 13/250, Loss: 261.8115539550781, Test Loss: 263.9815979003906\n",
            "Epoch 14/250, Loss: 271.6525573730469, Test Loss: 263.9258728027344\n",
            "Epoch 15/250, Loss: 274.4066162109375, Test Loss: 263.8698425292969\n",
            "Epoch 16/250, Loss: 270.2792053222656, Test Loss: 263.81451416015625\n",
            "Epoch 17/250, Loss: 275.2677001953125, Test Loss: 263.7594299316406\n",
            "Epoch 18/250, Loss: 259.4208984375, Test Loss: 263.7046203613281\n",
            "Epoch 19/250, Loss: 266.5093078613281, Test Loss: 263.6495361328125\n",
            "Epoch 20/250, Loss: 274.53826904296875, Test Loss: 263.5943298339844\n",
            "Epoch 21/250, Loss: 276.9713439941406, Test Loss: 263.5397033691406\n",
            "Epoch 22/250, Loss: 266.6773681640625, Test Loss: 263.4853210449219\n",
            "Epoch 23/250, Loss: 272.59185791015625, Test Loss: 263.4303894042969\n",
            "Epoch 24/250, Loss: 272.7356262207031, Test Loss: 263.37591552734375\n",
            "Epoch 25/250, Loss: 270.2620849609375, Test Loss: 263.3215637207031\n",
            "Epoch 26/250, Loss: 273.1857604980469, Test Loss: 263.2677307128906\n",
            "Epoch 27/250, Loss: 278.4144592285156, Test Loss: 263.2136535644531\n",
            "Epoch 28/250, Loss: 261.71771240234375, Test Loss: 263.1591796875\n",
            "Epoch 29/250, Loss: 268.774169921875, Test Loss: 263.1050109863281\n",
            "Epoch 30/250, Loss: 265.12481689453125, Test Loss: 263.05096435546875\n",
            "Epoch 31/250, Loss: 261.4766845703125, Test Loss: 262.9969787597656\n",
            "Epoch 32/250, Loss: 261.7100524902344, Test Loss: 262.9429626464844\n",
            "Epoch 33/250, Loss: 259.2620849609375, Test Loss: 262.8888854980469\n",
            "Epoch 34/250, Loss: 268.83758544921875, Test Loss: 262.83502197265625\n",
            "Epoch 35/250, Loss: 265.8226623535156, Test Loss: 262.78070068359375\n",
            "Epoch 36/250, Loss: 265.03326416015625, Test Loss: 262.7276916503906\n",
            "Epoch 37/250, Loss: 265.0218200683594, Test Loss: 262.6744689941406\n",
            "Epoch 38/250, Loss: 269.4783935546875, Test Loss: 262.61981201171875\n",
            "Epoch 39/250, Loss: 266.8556213378906, Test Loss: 262.5662841796875\n",
            "Epoch 40/250, Loss: 265.37884521484375, Test Loss: 262.51251220703125\n",
            "Epoch 41/250, Loss: 267.4505920410156, Test Loss: 262.4585266113281\n",
            "Epoch 42/250, Loss: 272.9659423828125, Test Loss: 262.40557861328125\n",
            "Epoch 43/250, Loss: 270.9127502441406, Test Loss: 262.351806640625\n",
            "Epoch 44/250, Loss: 267.7862548828125, Test Loss: 262.2979431152344\n",
            "Epoch 45/250, Loss: 266.21905517578125, Test Loss: 262.24444580078125\n",
            "Epoch 46/250, Loss: 263.30291748046875, Test Loss: 262.1907043457031\n",
            "Epoch 47/250, Loss: 268.2337646484375, Test Loss: 262.13653564453125\n",
            "Epoch 48/250, Loss: 260.6969909667969, Test Loss: 262.082763671875\n",
            "Epoch 49/250, Loss: 272.57452392578125, Test Loss: 262.02886962890625\n",
            "Epoch 50/250, Loss: 273.8794860839844, Test Loss: 261.9754333496094\n",
            "Epoch 51/250, Loss: 270.9186706542969, Test Loss: 261.9216613769531\n",
            "Epoch 52/250, Loss: 265.5843200683594, Test Loss: 261.8676452636719\n",
            "Epoch 53/250, Loss: 267.78253173828125, Test Loss: 261.8130187988281\n",
            "Epoch 54/250, Loss: 276.7657775878906, Test Loss: 261.7586669921875\n",
            "Epoch 55/250, Loss: 268.8252868652344, Test Loss: 261.704833984375\n",
            "Epoch 56/250, Loss: 274.1976013183594, Test Loss: 261.65032958984375\n",
            "Epoch 57/250, Loss: 270.8473815917969, Test Loss: 261.59521484375\n",
            "Epoch 58/250, Loss: 271.9233703613281, Test Loss: 261.54083251953125\n",
            "Epoch 59/250, Loss: 266.1617126464844, Test Loss: 261.4864501953125\n",
            "Epoch 60/250, Loss: 271.08319091796875, Test Loss: 261.4318542480469\n",
            "Epoch 61/250, Loss: 268.239990234375, Test Loss: 261.3771667480469\n",
            "Epoch 62/250, Loss: 257.48370361328125, Test Loss: 261.3220520019531\n",
            "Epoch 63/250, Loss: 270.4313049316406, Test Loss: 261.2665710449219\n",
            "Epoch 64/250, Loss: 269.6160583496094, Test Loss: 261.21063232421875\n",
            "Epoch 65/250, Loss: 266.07623291015625, Test Loss: 261.1551513671875\n",
            "Epoch 66/250, Loss: 253.76065063476562, Test Loss: 261.0992431640625\n",
            "Epoch 67/250, Loss: 265.0804748535156, Test Loss: 261.0431213378906\n",
            "Epoch 68/250, Loss: 277.3211975097656, Test Loss: 260.98638916015625\n",
            "Epoch 69/250, Loss: 272.0897521972656, Test Loss: 260.93023681640625\n",
            "Epoch 70/250, Loss: 272.39337158203125, Test Loss: 260.87353515625\n",
            "Epoch 71/250, Loss: 263.6881408691406, Test Loss: 260.816162109375\n",
            "Epoch 72/250, Loss: 272.48724365234375, Test Loss: 260.7594909667969\n",
            "Epoch 73/250, Loss: 269.020263671875, Test Loss: 260.70196533203125\n",
            "Epoch 74/250, Loss: 269.37774658203125, Test Loss: 260.6444091796875\n",
            "Epoch 75/250, Loss: 269.1085510253906, Test Loss: 260.5859680175781\n",
            "Epoch 76/250, Loss: 266.14300537109375, Test Loss: 260.5277099609375\n",
            "Epoch 77/250, Loss: 264.8031005859375, Test Loss: 260.4692077636719\n",
            "Epoch 78/250, Loss: 265.7318115234375, Test Loss: 260.41021728515625\n",
            "Epoch 79/250, Loss: 276.9563903808594, Test Loss: 260.3509216308594\n",
            "Epoch 80/250, Loss: 260.7721252441406, Test Loss: 260.2908630371094\n",
            "Epoch 81/250, Loss: 268.1676330566406, Test Loss: 260.2318420410156\n",
            "Epoch 82/250, Loss: 265.3462219238281, Test Loss: 260.1719970703125\n",
            "Epoch 83/250, Loss: 262.7750549316406, Test Loss: 260.1107482910156\n",
            "Epoch 84/250, Loss: 275.541015625, Test Loss: 260.05035400390625\n",
            "Epoch 85/250, Loss: 257.4373474121094, Test Loss: 259.9888610839844\n",
            "Epoch 86/250, Loss: 265.36614990234375, Test Loss: 259.9269104003906\n",
            "Epoch 87/250, Loss: 262.9944763183594, Test Loss: 259.8653564453125\n",
            "Epoch 88/250, Loss: 263.9227294921875, Test Loss: 259.8024597167969\n",
            "Epoch 89/250, Loss: 265.0801696777344, Test Loss: 259.74029541015625\n",
            "Epoch 90/250, Loss: 276.9945983886719, Test Loss: 259.6778564453125\n",
            "Epoch 91/250, Loss: 269.440185546875, Test Loss: 259.6139831542969\n",
            "Epoch 92/250, Loss: 273.6136779785156, Test Loss: 259.5494384765625\n",
            "Epoch 93/250, Loss: 262.88482666015625, Test Loss: 259.484375\n",
            "Epoch 94/250, Loss: 264.75054931640625, Test Loss: 259.42010498046875\n",
            "Epoch 95/250, Loss: 275.8101806640625, Test Loss: 259.3541564941406\n",
            "Epoch 96/250, Loss: 262.33135986328125, Test Loss: 259.28857421875\n",
            "Epoch 97/250, Loss: 263.7518005371094, Test Loss: 259.2228088378906\n",
            "Epoch 98/250, Loss: 261.62164306640625, Test Loss: 259.15631103515625\n",
            "Epoch 99/250, Loss: 269.5965576171875, Test Loss: 259.0887451171875\n",
            "Epoch 100/250, Loss: 260.1927795410156, Test Loss: 259.02117919921875\n",
            "Epoch 101/250, Loss: 258.1127014160156, Test Loss: 258.9516296386719\n",
            "Epoch 102/250, Loss: 262.33892822265625, Test Loss: 258.8829650878906\n",
            "Epoch 103/250, Loss: 270.0484619140625, Test Loss: 258.81353759765625\n",
            "Epoch 104/250, Loss: 269.16400146484375, Test Loss: 258.7428894042969\n",
            "Epoch 105/250, Loss: 265.720703125, Test Loss: 258.6716003417969\n",
            "Epoch 106/250, Loss: 267.17169189453125, Test Loss: 258.5987243652344\n",
            "Epoch 107/250, Loss: 270.34332275390625, Test Loss: 258.5265808105469\n",
            "Epoch 108/250, Loss: 266.58319091796875, Test Loss: 258.454345703125\n",
            "Epoch 109/250, Loss: 258.0301208496094, Test Loss: 258.38092041015625\n",
            "Epoch 110/250, Loss: 267.5640869140625, Test Loss: 258.3050231933594\n",
            "Epoch 111/250, Loss: 262.3901062011719, Test Loss: 258.2300109863281\n",
            "Epoch 112/250, Loss: 265.8558349609375, Test Loss: 258.1542663574219\n",
            "Epoch 113/250, Loss: 261.53985595703125, Test Loss: 258.0773010253906\n",
            "Epoch 114/250, Loss: 264.5380554199219, Test Loss: 257.9992980957031\n",
            "Epoch 115/250, Loss: 257.4913330078125, Test Loss: 257.92108154296875\n",
            "Epoch 116/250, Loss: 263.67254638671875, Test Loss: 257.8429870605469\n",
            "Epoch 117/250, Loss: 269.4443664550781, Test Loss: 257.7624206542969\n",
            "Epoch 118/250, Loss: 268.0146179199219, Test Loss: 257.68170166015625\n",
            "Epoch 119/250, Loss: 266.22296142578125, Test Loss: 257.59954833984375\n",
            "Epoch 120/250, Loss: 262.9135437011719, Test Loss: 257.5166015625\n",
            "Epoch 121/250, Loss: 266.4685974121094, Test Loss: 257.4324645996094\n",
            "Epoch 122/250, Loss: 264.7535705566406, Test Loss: 257.34771728515625\n",
            "Epoch 123/250, Loss: 266.6663818359375, Test Loss: 257.26190185546875\n",
            "Epoch 124/250, Loss: 258.4406433105469, Test Loss: 257.17498779296875\n",
            "Epoch 125/250, Loss: 270.5680236816406, Test Loss: 257.0868835449219\n",
            "Epoch 126/250, Loss: 262.83697509765625, Test Loss: 256.99774169921875\n",
            "Epoch 127/250, Loss: 263.1545715332031, Test Loss: 256.9089660644531\n",
            "Epoch 128/250, Loss: 260.7679443359375, Test Loss: 256.8183288574219\n",
            "Epoch 129/250, Loss: 267.6584777832031, Test Loss: 256.72528076171875\n",
            "Epoch 130/250, Loss: 263.35888671875, Test Loss: 256.631103515625\n",
            "Epoch 131/250, Loss: 262.0968322753906, Test Loss: 256.53643798828125\n",
            "Epoch 132/250, Loss: 259.63580322265625, Test Loss: 256.4409484863281\n",
            "Epoch 133/250, Loss: 268.8768615722656, Test Loss: 256.3446960449219\n",
            "Epoch 134/250, Loss: 255.40711975097656, Test Loss: 256.2460632324219\n",
            "Epoch 135/250, Loss: 251.08349609375, Test Loss: 256.1461486816406\n",
            "Epoch 136/250, Loss: 263.9308166503906, Test Loss: 256.0446472167969\n",
            "Epoch 137/250, Loss: 258.11993408203125, Test Loss: 255.9412078857422\n",
            "Epoch 138/250, Loss: 253.20521545410156, Test Loss: 255.83529663085938\n",
            "Epoch 139/250, Loss: 265.7569580078125, Test Loss: 255.7307586669922\n",
            "Epoch 140/250, Loss: 262.58111572265625, Test Loss: 255.62388610839844\n",
            "Epoch 141/250, Loss: 254.37115478515625, Test Loss: 255.51466369628906\n",
            "Epoch 142/250, Loss: 261.38299560546875, Test Loss: 255.40541076660156\n",
            "Epoch 143/250, Loss: 259.2566833496094, Test Loss: 255.2944793701172\n",
            "Epoch 144/250, Loss: 259.4434814453125, Test Loss: 255.18275451660156\n",
            "Epoch 145/250, Loss: 252.88951110839844, Test Loss: 255.0667724609375\n",
            "Epoch 146/250, Loss: 259.2953186035156, Test Loss: 254.9509735107422\n",
            "Epoch 147/250, Loss: 253.67428588867188, Test Loss: 254.83485412597656\n",
            "Epoch 148/250, Loss: 255.5969696044922, Test Loss: 254.71524047851562\n",
            "Epoch 149/250, Loss: 270.6995849609375, Test Loss: 254.59490966796875\n",
            "Epoch 150/250, Loss: 257.1677551269531, Test Loss: 254.47470092773438\n",
            "Epoch 151/250, Loss: 251.20228576660156, Test Loss: 254.34983825683594\n",
            "Epoch 152/250, Loss: 262.38818359375, Test Loss: 254.22198486328125\n",
            "Epoch 153/250, Loss: 257.8566589355469, Test Loss: 254.09629821777344\n",
            "Epoch 154/250, Loss: 259.76544189453125, Test Loss: 253.96363830566406\n",
            "Epoch 155/250, Loss: 262.3881530761719, Test Loss: 253.8314666748047\n",
            "Epoch 156/250, Loss: 262.0804748535156, Test Loss: 253.70008850097656\n",
            "Epoch 157/250, Loss: 257.2462158203125, Test Loss: 253.56118774414062\n",
            "Epoch 158/250, Loss: 260.1676330566406, Test Loss: 253.42311096191406\n",
            "Epoch 159/250, Loss: 252.4216766357422, Test Loss: 253.28411865234375\n",
            "Epoch 160/250, Loss: 251.64913940429688, Test Loss: 253.14556884765625\n",
            "Epoch 161/250, Loss: 252.96917724609375, Test Loss: 253.00491333007812\n",
            "Epoch 162/250, Loss: 258.3455810546875, Test Loss: 252.8611602783203\n",
            "Epoch 163/250, Loss: 260.869873046875, Test Loss: 252.71424865722656\n",
            "Epoch 164/250, Loss: 261.9406433105469, Test Loss: 252.56304931640625\n",
            "Epoch 165/250, Loss: 260.7183837890625, Test Loss: 252.4142303466797\n",
            "Epoch 166/250, Loss: 252.22418212890625, Test Loss: 252.2633056640625\n",
            "Epoch 167/250, Loss: 255.25608825683594, Test Loss: 252.1057891845703\n",
            "Epoch 168/250, Loss: 252.34214782714844, Test Loss: 251.9471435546875\n",
            "Epoch 169/250, Loss: 255.13916015625, Test Loss: 251.79281616210938\n",
            "Epoch 170/250, Loss: 255.99049377441406, Test Loss: 251.63336181640625\n",
            "Epoch 171/250, Loss: 243.78587341308594, Test Loss: 251.4668731689453\n",
            "Epoch 172/250, Loss: 251.0535125732422, Test Loss: 251.3021697998047\n",
            "Epoch 173/250, Loss: 253.1969757080078, Test Loss: 251.13720703125\n",
            "Epoch 174/250, Loss: 254.64878845214844, Test Loss: 250.97369384765625\n",
            "Epoch 175/250, Loss: 247.50367736816406, Test Loss: 250.79930114746094\n",
            "Epoch 176/250, Loss: 248.20419311523438, Test Loss: 250.6222381591797\n",
            "Epoch 177/250, Loss: 251.8195343017578, Test Loss: 250.45269775390625\n",
            "Epoch 178/250, Loss: 254.42996215820312, Test Loss: 250.2752685546875\n",
            "Epoch 179/250, Loss: 247.63565063476562, Test Loss: 250.09841918945312\n",
            "Epoch 180/250, Loss: 242.781494140625, Test Loss: 249.91258239746094\n",
            "Epoch 181/250, Loss: 249.77780151367188, Test Loss: 249.7281494140625\n",
            "Epoch 182/250, Loss: 252.5015106201172, Test Loss: 249.53958129882812\n",
            "Epoch 183/250, Loss: 247.9532928466797, Test Loss: 249.3533935546875\n",
            "Epoch 184/250, Loss: 245.70704650878906, Test Loss: 249.1599578857422\n",
            "Epoch 185/250, Loss: 250.94471740722656, Test Loss: 248.96835327148438\n",
            "Epoch 186/250, Loss: 248.22470092773438, Test Loss: 248.77639770507812\n",
            "Epoch 187/250, Loss: 240.10968017578125, Test Loss: 248.57640075683594\n",
            "Epoch 188/250, Loss: 247.22891235351562, Test Loss: 248.37484741210938\n",
            "Epoch 189/250, Loss: 248.07972717285156, Test Loss: 248.17274475097656\n",
            "Epoch 190/250, Loss: 248.0338897705078, Test Loss: 247.97190856933594\n",
            "Epoch 191/250, Loss: 247.1222381591797, Test Loss: 247.76834106445312\n",
            "Epoch 192/250, Loss: 257.60003662109375, Test Loss: 247.5554962158203\n",
            "Epoch 193/250, Loss: 247.68603515625, Test Loss: 247.3394012451172\n",
            "Epoch 194/250, Loss: 239.24501037597656, Test Loss: 247.12208557128906\n",
            "Epoch 195/250, Loss: 245.0364532470703, Test Loss: 246.89903259277344\n",
            "Epoch 196/250, Loss: 241.31419372558594, Test Loss: 246.6799774169922\n",
            "Epoch 197/250, Loss: 242.00927734375, Test Loss: 246.4513397216797\n",
            "Epoch 198/250, Loss: 244.23463439941406, Test Loss: 246.23863220214844\n",
            "Epoch 199/250, Loss: 244.87161254882812, Test Loss: 246.00717163085938\n",
            "Epoch 200/250, Loss: 242.30418395996094, Test Loss: 245.77516174316406\n",
            "Epoch 201/250, Loss: 234.3606414794922, Test Loss: 245.5333251953125\n",
            "Epoch 202/250, Loss: 243.4568634033203, Test Loss: 245.29754638671875\n",
            "Epoch 203/250, Loss: 239.39828491210938, Test Loss: 245.0519256591797\n",
            "Epoch 204/250, Loss: 243.78970336914062, Test Loss: 244.81240844726562\n",
            "Epoch 205/250, Loss: 241.00621032714844, Test Loss: 244.56924438476562\n",
            "Epoch 206/250, Loss: 237.140869140625, Test Loss: 244.3160858154297\n",
            "Epoch 207/250, Loss: 236.52684020996094, Test Loss: 244.06838989257812\n",
            "Epoch 208/250, Loss: 238.01766967773438, Test Loss: 243.80966186523438\n",
            "Epoch 209/250, Loss: 238.48065185546875, Test Loss: 243.5509796142578\n",
            "Epoch 210/250, Loss: 236.53536987304688, Test Loss: 243.2964324951172\n",
            "Epoch 211/250, Loss: 230.22653198242188, Test Loss: 243.0257110595703\n",
            "Epoch 212/250, Loss: 247.72372436523438, Test Loss: 242.7738800048828\n",
            "Epoch 213/250, Loss: 231.6081085205078, Test Loss: 242.49586486816406\n",
            "Epoch 214/250, Loss: 235.94479370117188, Test Loss: 242.22268676757812\n",
            "Epoch 215/250, Loss: 228.6015625, Test Loss: 241.94427490234375\n",
            "Epoch 216/250, Loss: 231.8065643310547, Test Loss: 241.66976928710938\n",
            "Epoch 217/250, Loss: 232.2770233154297, Test Loss: 241.3826904296875\n",
            "Epoch 218/250, Loss: 227.38497924804688, Test Loss: 241.1073455810547\n",
            "Epoch 219/250, Loss: 226.9014129638672, Test Loss: 240.8129425048828\n",
            "Epoch 220/250, Loss: 235.00552368164062, Test Loss: 240.52757263183594\n",
            "Epoch 221/250, Loss: 226.0513458251953, Test Loss: 240.2321319580078\n",
            "Epoch 222/250, Loss: 223.0061798095703, Test Loss: 239.92933654785156\n",
            "Epoch 223/250, Loss: 222.56646728515625, Test Loss: 239.62460327148438\n",
            "Epoch 224/250, Loss: 223.16639709472656, Test Loss: 239.32266235351562\n",
            "Epoch 225/250, Loss: 229.02407836914062, Test Loss: 239.02020263671875\n",
            "Epoch 226/250, Loss: 232.7102813720703, Test Loss: 238.71763610839844\n",
            "Epoch 227/250, Loss: 228.11582946777344, Test Loss: 238.412353515625\n",
            "Epoch 228/250, Loss: 222.69998168945312, Test Loss: 238.09852600097656\n",
            "Epoch 229/250, Loss: 231.1194610595703, Test Loss: 237.7816925048828\n",
            "Epoch 230/250, Loss: 221.36915588378906, Test Loss: 237.44276428222656\n",
            "Epoch 231/250, Loss: 224.69064331054688, Test Loss: 237.09681701660156\n",
            "Epoch 232/250, Loss: 218.77838134765625, Test Loss: 236.77041625976562\n",
            "Epoch 233/250, Loss: 226.45706176757812, Test Loss: 236.43653869628906\n",
            "Epoch 234/250, Loss: 209.33779907226562, Test Loss: 236.07867431640625\n",
            "Epoch 235/250, Loss: 226.74560546875, Test Loss: 235.74195861816406\n",
            "Epoch 236/250, Loss: 221.84945678710938, Test Loss: 235.40328979492188\n",
            "Epoch 237/250, Loss: 223.9641876220703, Test Loss: 235.03648376464844\n",
            "Epoch 238/250, Loss: 207.2544708251953, Test Loss: 234.68722534179688\n",
            "Epoch 239/250, Loss: 228.6970977783203, Test Loss: 234.31982421875\n",
            "Epoch 240/250, Loss: 221.09422302246094, Test Loss: 233.95460510253906\n",
            "Epoch 241/250, Loss: 207.91004943847656, Test Loss: 233.57936096191406\n",
            "Epoch 242/250, Loss: 205.85675048828125, Test Loss: 233.19869995117188\n",
            "Epoch 243/250, Loss: 219.5889434814453, Test Loss: 232.8206024169922\n",
            "Epoch 244/250, Loss: 207.58738708496094, Test Loss: 232.42813110351562\n",
            "Epoch 245/250, Loss: 214.15391540527344, Test Loss: 232.04013061523438\n",
            "Epoch 246/250, Loss: 219.0076446533203, Test Loss: 231.65927124023438\n",
            "Epoch 247/250, Loss: 209.5754852294922, Test Loss: 231.2555389404297\n",
            "Epoch 248/250, Loss: 214.3494110107422, Test Loss: 230.84494018554688\n",
            "Epoch 249/250, Loss: 197.30679321289062, Test Loss: 230.41700744628906\n",
            "Epoch 250/250, Loss: 217.0573272705078, Test Loss: 230.00282287597656\n",
            "Final MSE: 230.00282287597656\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=128, epochs=1\n",
            "Epoch 1/1, Loss: 291.6357116699219, Test Loss: 289.8623046875\n",
            "Final MSE: 289.8623046875\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=128, epochs=10\n",
            "Epoch 1/10, Loss: 272.7716064453125, Test Loss: 265.5764465332031\n",
            "Epoch 2/10, Loss: 274.16766357421875, Test Loss: 265.5464172363281\n",
            "Epoch 3/10, Loss: 280.1867980957031, Test Loss: 265.516357421875\n",
            "Epoch 4/10, Loss: 274.8540344238281, Test Loss: 265.48638916015625\n",
            "Epoch 5/10, Loss: 280.35400390625, Test Loss: 265.4566955566406\n",
            "Epoch 6/10, Loss: 277.7677001953125, Test Loss: 265.4266357421875\n",
            "Epoch 7/10, Loss: 279.3580627441406, Test Loss: 265.3963928222656\n",
            "Epoch 8/10, Loss: 281.0950012207031, Test Loss: 265.3656005859375\n",
            "Epoch 9/10, Loss: 277.3774719238281, Test Loss: 265.3348083496094\n",
            "Epoch 10/10, Loss: 276.7218017578125, Test Loss: 265.3042297363281\n",
            "Final MSE: 265.3042297363281\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=128, epochs=25\n",
            "Epoch 1/25, Loss: 270.6690979003906, Test Loss: 273.240234375\n",
            "Epoch 2/25, Loss: 274.79937744140625, Test Loss: 273.1888427734375\n",
            "Epoch 3/25, Loss: 279.5296630859375, Test Loss: 273.1340637207031\n",
            "Epoch 4/25, Loss: 274.3424377441406, Test Loss: 273.0777587890625\n",
            "Epoch 5/25, Loss: 282.6907653808594, Test Loss: 273.0224304199219\n",
            "Epoch 6/25, Loss: 282.9239501953125, Test Loss: 272.96588134765625\n",
            "Epoch 7/25, Loss: 270.5768127441406, Test Loss: 272.91009521484375\n",
            "Epoch 8/25, Loss: 278.9136047363281, Test Loss: 272.85528564453125\n",
            "Epoch 9/25, Loss: 281.77911376953125, Test Loss: 272.80072021484375\n",
            "Epoch 10/25, Loss: 272.1296081542969, Test Loss: 272.74554443359375\n",
            "Epoch 11/25, Loss: 273.8648986816406, Test Loss: 272.6904602050781\n",
            "Epoch 12/25, Loss: 279.4848327636719, Test Loss: 272.63543701171875\n",
            "Epoch 13/25, Loss: 276.5481872558594, Test Loss: 272.58026123046875\n",
            "Epoch 14/25, Loss: 277.2469482421875, Test Loss: 272.5259704589844\n",
            "Epoch 15/25, Loss: 269.1897888183594, Test Loss: 272.4720458984375\n",
            "Epoch 16/25, Loss: 278.7953796386719, Test Loss: 272.4179382324219\n",
            "Epoch 17/25, Loss: 280.1252746582031, Test Loss: 272.3638000488281\n",
            "Epoch 18/25, Loss: 284.3249816894531, Test Loss: 272.3094482421875\n",
            "Epoch 19/25, Loss: 277.0950927734375, Test Loss: 272.2555847167969\n",
            "Epoch 20/25, Loss: 274.5579833984375, Test Loss: 272.2012023925781\n",
            "Epoch 21/25, Loss: 284.4007263183594, Test Loss: 272.14691162109375\n",
            "Epoch 22/25, Loss: 278.8851623535156, Test Loss: 272.09210205078125\n",
            "Epoch 23/25, Loss: 282.5317077636719, Test Loss: 272.03704833984375\n",
            "Epoch 24/25, Loss: 286.42022705078125, Test Loss: 271.9818420410156\n",
            "Epoch 25/25, Loss: 282.1490783691406, Test Loss: 271.9261169433594\n",
            "Final MSE: 271.9261474609375\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=128, epochs=50\n",
            "Epoch 1/50, Loss: 272.7340393066406, Test Loss: 263.0277099609375\n",
            "Epoch 2/50, Loss: 270.2068786621094, Test Loss: 262.9649353027344\n",
            "Epoch 3/50, Loss: 268.4744567871094, Test Loss: 262.9026794433594\n",
            "Epoch 4/50, Loss: 266.9042053222656, Test Loss: 262.84039306640625\n",
            "Epoch 5/50, Loss: 265.38897705078125, Test Loss: 262.776611328125\n",
            "Epoch 6/50, Loss: 275.17254638671875, Test Loss: 262.7134704589844\n",
            "Epoch 7/50, Loss: 270.68896484375, Test Loss: 262.6482238769531\n",
            "Epoch 8/50, Loss: 263.04290771484375, Test Loss: 262.585205078125\n",
            "Epoch 9/50, Loss: 273.4783020019531, Test Loss: 262.5224609375\n",
            "Epoch 10/50, Loss: 268.4884948730469, Test Loss: 262.4590759277344\n",
            "Epoch 11/50, Loss: 272.59197998046875, Test Loss: 262.3957824707031\n",
            "Epoch 12/50, Loss: 269.1759948730469, Test Loss: 262.3302917480469\n",
            "Epoch 13/50, Loss: 273.35003662109375, Test Loss: 262.2637634277344\n",
            "Epoch 14/50, Loss: 261.7926940917969, Test Loss: 262.1975402832031\n",
            "Epoch 15/50, Loss: 263.0392150878906, Test Loss: 262.1311950683594\n",
            "Epoch 16/50, Loss: 272.0542907714844, Test Loss: 262.06640625\n",
            "Epoch 17/50, Loss: 266.0559997558594, Test Loss: 262.0008850097656\n",
            "Epoch 18/50, Loss: 269.7936096191406, Test Loss: 261.9343566894531\n",
            "Epoch 19/50, Loss: 274.5814514160156, Test Loss: 261.8682861328125\n",
            "Epoch 20/50, Loss: 261.1596984863281, Test Loss: 261.802001953125\n",
            "Epoch 21/50, Loss: 269.1626281738281, Test Loss: 261.7383728027344\n",
            "Epoch 22/50, Loss: 262.7739562988281, Test Loss: 261.6741943359375\n",
            "Epoch 23/50, Loss: 264.00738525390625, Test Loss: 261.6102600097656\n",
            "Epoch 24/50, Loss: 268.7308044433594, Test Loss: 261.5460205078125\n",
            "Epoch 25/50, Loss: 272.1735534667969, Test Loss: 261.4825744628906\n",
            "Epoch 26/50, Loss: 266.1855773925781, Test Loss: 261.4175720214844\n",
            "Epoch 27/50, Loss: 262.7057800292969, Test Loss: 261.3530578613281\n",
            "Epoch 28/50, Loss: 277.8337097167969, Test Loss: 261.28857421875\n",
            "Epoch 29/50, Loss: 269.69635009765625, Test Loss: 261.22314453125\n",
            "Epoch 30/50, Loss: 271.55267333984375, Test Loss: 261.15618896484375\n",
            "Epoch 31/50, Loss: 263.7768859863281, Test Loss: 261.0892639160156\n",
            "Epoch 32/50, Loss: 272.0323791503906, Test Loss: 261.0242614746094\n",
            "Epoch 33/50, Loss: 265.25811767578125, Test Loss: 260.9571533203125\n",
            "Epoch 34/50, Loss: 265.6200256347656, Test Loss: 260.89013671875\n",
            "Epoch 35/50, Loss: 266.4086608886719, Test Loss: 260.8243713378906\n",
            "Epoch 36/50, Loss: 268.9329833984375, Test Loss: 260.7583312988281\n",
            "Epoch 37/50, Loss: 266.0602722167969, Test Loss: 260.6930236816406\n",
            "Epoch 38/50, Loss: 272.5074157714844, Test Loss: 260.62689208984375\n",
            "Epoch 39/50, Loss: 274.6415710449219, Test Loss: 260.5614013671875\n",
            "Epoch 40/50, Loss: 268.62567138671875, Test Loss: 260.4957580566406\n",
            "Epoch 41/50, Loss: 264.74420166015625, Test Loss: 260.430908203125\n",
            "Epoch 42/50, Loss: 267.03118896484375, Test Loss: 260.3671875\n",
            "Epoch 43/50, Loss: 260.8245849609375, Test Loss: 260.3014221191406\n",
            "Epoch 44/50, Loss: 269.4009094238281, Test Loss: 260.2350158691406\n",
            "Epoch 45/50, Loss: 267.39508056640625, Test Loss: 260.1700134277344\n",
            "Epoch 46/50, Loss: 274.44482421875, Test Loss: 260.10498046875\n",
            "Epoch 47/50, Loss: 263.59271240234375, Test Loss: 260.039794921875\n",
            "Epoch 48/50, Loss: 262.0821838378906, Test Loss: 259.97540283203125\n",
            "Epoch 49/50, Loss: 263.8495178222656, Test Loss: 259.9117431640625\n",
            "Epoch 50/50, Loss: 271.60797119140625, Test Loss: 259.8473205566406\n",
            "Final MSE: 259.8473205566406\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=128, epochs=100\n",
            "Epoch 1/100, Loss: 289.9449462890625, Test Loss: 283.3877868652344\n",
            "Epoch 2/100, Loss: 291.34326171875, Test Loss: 283.3493957519531\n",
            "Epoch 3/100, Loss: 291.4925231933594, Test Loss: 283.3102111816406\n",
            "Epoch 4/100, Loss: 293.5654602050781, Test Loss: 283.27166748046875\n",
            "Epoch 5/100, Loss: 292.9820556640625, Test Loss: 283.232421875\n",
            "Epoch 6/100, Loss: 294.58587646484375, Test Loss: 283.194091796875\n",
            "Epoch 7/100, Loss: 299.89642333984375, Test Loss: 283.1548156738281\n",
            "Epoch 8/100, Loss: 286.2590026855469, Test Loss: 283.1153564453125\n",
            "Epoch 9/100, Loss: 290.42431640625, Test Loss: 283.0767822265625\n",
            "Epoch 10/100, Loss: 288.87908935546875, Test Loss: 283.03778076171875\n",
            "Epoch 11/100, Loss: 291.5271301269531, Test Loss: 282.9994201660156\n",
            "Epoch 12/100, Loss: 288.2185363769531, Test Loss: 282.96124267578125\n",
            "Epoch 13/100, Loss: 287.2815246582031, Test Loss: 282.92279052734375\n",
            "Epoch 14/100, Loss: 289.56732177734375, Test Loss: 282.8844299316406\n",
            "Epoch 15/100, Loss: 289.3784484863281, Test Loss: 282.84564208984375\n",
            "Epoch 16/100, Loss: 291.9409484863281, Test Loss: 282.806640625\n",
            "Epoch 17/100, Loss: 293.3515930175781, Test Loss: 282.7680358886719\n",
            "Epoch 18/100, Loss: 288.51800537109375, Test Loss: 282.7294006347656\n",
            "Epoch 19/100, Loss: 291.8125, Test Loss: 282.6910705566406\n",
            "Epoch 20/100, Loss: 284.5225830078125, Test Loss: 282.65234375\n",
            "Epoch 21/100, Loss: 286.8560791015625, Test Loss: 282.61309814453125\n",
            "Epoch 22/100, Loss: 285.41741943359375, Test Loss: 282.5741271972656\n",
            "Epoch 23/100, Loss: 291.0474548339844, Test Loss: 282.53533935546875\n",
            "Epoch 24/100, Loss: 275.33013916015625, Test Loss: 282.49652099609375\n",
            "Epoch 25/100, Loss: 281.1016540527344, Test Loss: 282.4574890136719\n",
            "Epoch 26/100, Loss: 294.6849060058594, Test Loss: 282.41876220703125\n",
            "Epoch 27/100, Loss: 293.48187255859375, Test Loss: 282.38031005859375\n",
            "Epoch 28/100, Loss: 291.6729736328125, Test Loss: 282.34124755859375\n",
            "Epoch 29/100, Loss: 290.6294860839844, Test Loss: 282.30267333984375\n",
            "Epoch 30/100, Loss: 291.41082763671875, Test Loss: 282.2638854980469\n",
            "Epoch 31/100, Loss: 295.0672912597656, Test Loss: 282.2250671386719\n",
            "Epoch 32/100, Loss: 297.3069152832031, Test Loss: 282.1861572265625\n",
            "Epoch 33/100, Loss: 291.08660888671875, Test Loss: 282.1474304199219\n",
            "Epoch 34/100, Loss: 292.3138427734375, Test Loss: 282.10797119140625\n",
            "Epoch 35/100, Loss: 293.2547607421875, Test Loss: 282.0682067871094\n",
            "Epoch 36/100, Loss: 293.34466552734375, Test Loss: 282.0280456542969\n",
            "Epoch 37/100, Loss: 293.67291259765625, Test Loss: 281.98797607421875\n",
            "Epoch 38/100, Loss: 287.0365905761719, Test Loss: 281.9477233886719\n",
            "Epoch 39/100, Loss: 285.9219665527344, Test Loss: 281.9078674316406\n",
            "Epoch 40/100, Loss: 289.9234619140625, Test Loss: 281.86810302734375\n",
            "Epoch 41/100, Loss: 292.9209289550781, Test Loss: 281.82867431640625\n",
            "Epoch 42/100, Loss: 285.35943603515625, Test Loss: 281.7882995605469\n",
            "Epoch 43/100, Loss: 295.6134338378906, Test Loss: 281.7481994628906\n",
            "Epoch 44/100, Loss: 289.00677490234375, Test Loss: 281.708251953125\n",
            "Epoch 45/100, Loss: 291.0608215332031, Test Loss: 281.66790771484375\n",
            "Epoch 46/100, Loss: 289.3293762207031, Test Loss: 281.6272888183594\n",
            "Epoch 47/100, Loss: 287.2113037109375, Test Loss: 281.586669921875\n",
            "Epoch 48/100, Loss: 290.7272033691406, Test Loss: 281.5458068847656\n",
            "Epoch 49/100, Loss: 286.3310852050781, Test Loss: 281.5049743652344\n",
            "Epoch 50/100, Loss: 285.91796875, Test Loss: 281.46368408203125\n",
            "Epoch 51/100, Loss: 289.7794189453125, Test Loss: 281.4228210449219\n",
            "Epoch 52/100, Loss: 290.1824035644531, Test Loss: 281.38116455078125\n",
            "Epoch 53/100, Loss: 288.77301025390625, Test Loss: 281.33984375\n",
            "Epoch 54/100, Loss: 282.1158142089844, Test Loss: 281.29852294921875\n",
            "Epoch 55/100, Loss: 287.9836120605469, Test Loss: 281.25732421875\n",
            "Epoch 56/100, Loss: 292.2868957519531, Test Loss: 281.2159423828125\n",
            "Epoch 57/100, Loss: 277.1032409667969, Test Loss: 281.1739501953125\n",
            "Epoch 58/100, Loss: 296.6662292480469, Test Loss: 281.1319580078125\n",
            "Epoch 59/100, Loss: 296.11749267578125, Test Loss: 281.08978271484375\n",
            "Epoch 60/100, Loss: 291.8348388671875, Test Loss: 281.0469665527344\n",
            "Epoch 61/100, Loss: 292.7550048828125, Test Loss: 281.0042419433594\n",
            "Epoch 62/100, Loss: 290.7535400390625, Test Loss: 280.9620361328125\n",
            "Epoch 63/100, Loss: 288.7252502441406, Test Loss: 280.91961669921875\n",
            "Epoch 64/100, Loss: 291.4621887207031, Test Loss: 280.8766174316406\n",
            "Epoch 65/100, Loss: 291.6361999511719, Test Loss: 280.8336486816406\n",
            "Epoch 66/100, Loss: 290.62408447265625, Test Loss: 280.79071044921875\n",
            "Epoch 67/100, Loss: 277.3092346191406, Test Loss: 280.74761962890625\n",
            "Epoch 68/100, Loss: 287.1989440917969, Test Loss: 280.7039794921875\n",
            "Epoch 69/100, Loss: 295.5119934082031, Test Loss: 280.660400390625\n",
            "Epoch 70/100, Loss: 288.4920349121094, Test Loss: 280.6172790527344\n",
            "Epoch 71/100, Loss: 282.3467712402344, Test Loss: 280.574462890625\n",
            "Epoch 72/100, Loss: 300.8614501953125, Test Loss: 280.53125\n",
            "Epoch 73/100, Loss: 290.43402099609375, Test Loss: 280.4875793457031\n",
            "Epoch 74/100, Loss: 290.35028076171875, Test Loss: 280.44427490234375\n",
            "Epoch 75/100, Loss: 287.6995544433594, Test Loss: 280.3999938964844\n",
            "Epoch 76/100, Loss: 284.1819763183594, Test Loss: 280.3558349609375\n",
            "Epoch 77/100, Loss: 289.25457763671875, Test Loss: 280.3111572265625\n",
            "Epoch 78/100, Loss: 279.00567626953125, Test Loss: 280.266357421875\n",
            "Epoch 79/100, Loss: 289.3382263183594, Test Loss: 280.2216796875\n",
            "Epoch 80/100, Loss: 288.5951843261719, Test Loss: 280.1763916015625\n",
            "Epoch 81/100, Loss: 281.5748291015625, Test Loss: 280.130615234375\n",
            "Epoch 82/100, Loss: 288.3684997558594, Test Loss: 280.08416748046875\n",
            "Epoch 83/100, Loss: 285.16217041015625, Test Loss: 280.0382385253906\n",
            "Epoch 84/100, Loss: 288.5751953125, Test Loss: 279.9923400878906\n",
            "Epoch 85/100, Loss: 300.39337158203125, Test Loss: 279.9458923339844\n",
            "Epoch 86/100, Loss: 293.5641174316406, Test Loss: 279.8995361328125\n",
            "Epoch 87/100, Loss: 281.4100646972656, Test Loss: 279.8531494140625\n",
            "Epoch 88/100, Loss: 290.39617919921875, Test Loss: 279.80657958984375\n",
            "Epoch 89/100, Loss: 293.1647644042969, Test Loss: 279.75958251953125\n",
            "Epoch 90/100, Loss: 292.2662048339844, Test Loss: 279.7135314941406\n",
            "Epoch 91/100, Loss: 291.3486328125, Test Loss: 279.6672668457031\n",
            "Epoch 92/100, Loss: 279.6661071777344, Test Loss: 279.6207275390625\n",
            "Epoch 93/100, Loss: 282.7322998046875, Test Loss: 279.5752868652344\n",
            "Epoch 94/100, Loss: 274.19830322265625, Test Loss: 279.52923583984375\n",
            "Epoch 95/100, Loss: 288.0375061035156, Test Loss: 279.4835205078125\n",
            "Epoch 96/100, Loss: 292.7269592285156, Test Loss: 279.437744140625\n",
            "Epoch 97/100, Loss: 278.9076232910156, Test Loss: 279.3902587890625\n",
            "Epoch 98/100, Loss: 282.03253173828125, Test Loss: 279.343505859375\n",
            "Epoch 99/100, Loss: 289.79571533203125, Test Loss: 279.2958679199219\n",
            "Epoch 100/100, Loss: 280.9880676269531, Test Loss: 279.2483825683594\n",
            "Final MSE: 279.2483825683594\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=128, epochs=250\n",
            "Epoch 1/250, Loss: 278.6233825683594, Test Loss: 279.2973937988281\n",
            "Epoch 2/250, Loss: 292.8835754394531, Test Loss: 279.2456359863281\n",
            "Epoch 3/250, Loss: 288.2818908691406, Test Loss: 279.19256591796875\n",
            "Epoch 4/250, Loss: 286.7716979980469, Test Loss: 279.140869140625\n",
            "Epoch 5/250, Loss: 288.1114196777344, Test Loss: 279.08856201171875\n",
            "Epoch 6/250, Loss: 282.82879638671875, Test Loss: 279.0360107421875\n",
            "Epoch 7/250, Loss: 294.1563720703125, Test Loss: 278.9839172363281\n",
            "Epoch 8/250, Loss: 280.2680358886719, Test Loss: 278.93157958984375\n",
            "Epoch 9/250, Loss: 292.541015625, Test Loss: 278.8793029785156\n",
            "Epoch 10/250, Loss: 271.70452880859375, Test Loss: 278.8267517089844\n",
            "Epoch 11/250, Loss: 287.662109375, Test Loss: 278.77447509765625\n",
            "Epoch 12/250, Loss: 280.78717041015625, Test Loss: 278.722900390625\n",
            "Epoch 13/250, Loss: 291.8384704589844, Test Loss: 278.67108154296875\n",
            "Epoch 14/250, Loss: 283.626953125, Test Loss: 278.61907958984375\n",
            "Epoch 15/250, Loss: 293.8217468261719, Test Loss: 278.5671081542969\n",
            "Epoch 16/250, Loss: 294.05194091796875, Test Loss: 278.5151672363281\n",
            "Epoch 17/250, Loss: 292.39312744140625, Test Loss: 278.4635009765625\n",
            "Epoch 18/250, Loss: 286.8783264160156, Test Loss: 278.4113464355469\n",
            "Epoch 19/250, Loss: 287.4930114746094, Test Loss: 278.3594055175781\n",
            "Epoch 20/250, Loss: 284.8224792480469, Test Loss: 278.3071594238281\n",
            "Epoch 21/250, Loss: 283.6944885253906, Test Loss: 278.255859375\n",
            "Epoch 22/250, Loss: 280.0569763183594, Test Loss: 278.2041015625\n",
            "Epoch 23/250, Loss: 282.5431213378906, Test Loss: 278.1529235839844\n",
            "Epoch 24/250, Loss: 287.2550048828125, Test Loss: 278.1019592285156\n",
            "Epoch 25/250, Loss: 284.92730712890625, Test Loss: 278.05126953125\n",
            "Epoch 26/250, Loss: 289.0351257324219, Test Loss: 278.00054931640625\n",
            "Epoch 27/250, Loss: 291.9507751464844, Test Loss: 277.9501647949219\n",
            "Epoch 28/250, Loss: 292.4669189453125, Test Loss: 277.8998718261719\n",
            "Epoch 29/250, Loss: 279.0074157714844, Test Loss: 277.84881591796875\n",
            "Epoch 30/250, Loss: 288.23699951171875, Test Loss: 277.7980041503906\n",
            "Epoch 31/250, Loss: 284.1562805175781, Test Loss: 277.7477722167969\n",
            "Epoch 32/250, Loss: 288.5635681152344, Test Loss: 277.6976013183594\n",
            "Epoch 33/250, Loss: 280.7411804199219, Test Loss: 277.6474304199219\n",
            "Epoch 34/250, Loss: 281.1119079589844, Test Loss: 277.5975036621094\n",
            "Epoch 35/250, Loss: 287.1845703125, Test Loss: 277.5472412109375\n",
            "Epoch 36/250, Loss: 288.2895812988281, Test Loss: 277.4980163574219\n",
            "Epoch 37/250, Loss: 276.0159912109375, Test Loss: 277.4486083984375\n",
            "Epoch 38/250, Loss: 278.5217590332031, Test Loss: 277.399169921875\n",
            "Epoch 39/250, Loss: 290.84136962890625, Test Loss: 277.35009765625\n",
            "Epoch 40/250, Loss: 285.3216247558594, Test Loss: 277.3000793457031\n",
            "Epoch 41/250, Loss: 277.95208740234375, Test Loss: 277.249755859375\n",
            "Epoch 42/250, Loss: 282.31817626953125, Test Loss: 277.19970703125\n",
            "Epoch 43/250, Loss: 292.3335876464844, Test Loss: 277.1497802734375\n",
            "Epoch 44/250, Loss: 278.08282470703125, Test Loss: 277.1001281738281\n",
            "Epoch 45/250, Loss: 281.2471618652344, Test Loss: 277.0499267578125\n",
            "Epoch 46/250, Loss: 280.1202087402344, Test Loss: 277.0003356933594\n",
            "Epoch 47/250, Loss: 284.87469482421875, Test Loss: 276.9508056640625\n",
            "Epoch 48/250, Loss: 283.4735412597656, Test Loss: 276.90118408203125\n",
            "Epoch 49/250, Loss: 287.2752990722656, Test Loss: 276.8516540527344\n",
            "Epoch 50/250, Loss: 274.3199462890625, Test Loss: 276.8020324707031\n",
            "Epoch 51/250, Loss: 283.4131164550781, Test Loss: 276.75244140625\n",
            "Epoch 52/250, Loss: 282.80035400390625, Test Loss: 276.7027587890625\n",
            "Epoch 53/250, Loss: 275.57574462890625, Test Loss: 276.6535339355469\n",
            "Epoch 54/250, Loss: 278.9559020996094, Test Loss: 276.6039733886719\n",
            "Epoch 55/250, Loss: 285.3445739746094, Test Loss: 276.5544738769531\n",
            "Epoch 56/250, Loss: 282.6412658691406, Test Loss: 276.5044860839844\n",
            "Epoch 57/250, Loss: 276.3549499511719, Test Loss: 276.4538879394531\n",
            "Epoch 58/250, Loss: 283.8788146972656, Test Loss: 276.4034423828125\n",
            "Epoch 59/250, Loss: 289.6258239746094, Test Loss: 276.3533935546875\n",
            "Epoch 60/250, Loss: 281.3719787597656, Test Loss: 276.3036193847656\n",
            "Epoch 61/250, Loss: 289.6737060546875, Test Loss: 276.25445556640625\n",
            "Epoch 62/250, Loss: 284.2855529785156, Test Loss: 276.20513916015625\n",
            "Epoch 63/250, Loss: 287.6032409667969, Test Loss: 276.155517578125\n",
            "Epoch 64/250, Loss: 290.49090576171875, Test Loss: 276.1059265136719\n",
            "Epoch 65/250, Loss: 276.49310302734375, Test Loss: 276.05657958984375\n",
            "Epoch 66/250, Loss: 288.61322021484375, Test Loss: 276.00689697265625\n",
            "Epoch 67/250, Loss: 282.4656982421875, Test Loss: 275.9574890136719\n",
            "Epoch 68/250, Loss: 276.52960205078125, Test Loss: 275.90814208984375\n",
            "Epoch 69/250, Loss: 283.5131530761719, Test Loss: 275.859375\n",
            "Epoch 70/250, Loss: 288.0884704589844, Test Loss: 275.8099670410156\n",
            "Epoch 71/250, Loss: 285.7448425292969, Test Loss: 275.7604675292969\n",
            "Epoch 72/250, Loss: 282.2640686035156, Test Loss: 275.7110595703125\n",
            "Epoch 73/250, Loss: 281.95574951171875, Test Loss: 275.6616516113281\n",
            "Epoch 74/250, Loss: 276.0720520019531, Test Loss: 275.61199951171875\n",
            "Epoch 75/250, Loss: 288.5811462402344, Test Loss: 275.5616149902344\n",
            "Epoch 76/250, Loss: 280.6304626464844, Test Loss: 275.51177978515625\n",
            "Epoch 77/250, Loss: 286.2852478027344, Test Loss: 275.46234130859375\n",
            "Epoch 78/250, Loss: 288.54864501953125, Test Loss: 275.412841796875\n",
            "Epoch 79/250, Loss: 287.99224853515625, Test Loss: 275.36328125\n",
            "Epoch 80/250, Loss: 277.3070983886719, Test Loss: 275.3135986328125\n",
            "Epoch 81/250, Loss: 283.0591125488281, Test Loss: 275.263916015625\n",
            "Epoch 82/250, Loss: 285.8772277832031, Test Loss: 275.21392822265625\n",
            "Epoch 83/250, Loss: 275.4759216308594, Test Loss: 275.16400146484375\n",
            "Epoch 84/250, Loss: 284.19140625, Test Loss: 275.1138610839844\n",
            "Epoch 85/250, Loss: 285.76446533203125, Test Loss: 275.0638427734375\n",
            "Epoch 86/250, Loss: 282.84906005859375, Test Loss: 275.0141906738281\n",
            "Epoch 87/250, Loss: 278.7373046875, Test Loss: 274.9635925292969\n",
            "Epoch 88/250, Loss: 287.2544860839844, Test Loss: 274.91314697265625\n",
            "Epoch 89/250, Loss: 288.1880798339844, Test Loss: 274.8623352050781\n",
            "Epoch 90/250, Loss: 288.0533752441406, Test Loss: 274.8114318847656\n",
            "Epoch 91/250, Loss: 281.9919128417969, Test Loss: 274.7607727050781\n",
            "Epoch 92/250, Loss: 276.1380615234375, Test Loss: 274.7099914550781\n",
            "Epoch 93/250, Loss: 284.8736877441406, Test Loss: 274.6592102050781\n",
            "Epoch 94/250, Loss: 280.04925537109375, Test Loss: 274.6081237792969\n",
            "Epoch 95/250, Loss: 288.7921447753906, Test Loss: 274.5576171875\n",
            "Epoch 96/250, Loss: 276.41534423828125, Test Loss: 274.50689697265625\n",
            "Epoch 97/250, Loss: 277.3551940917969, Test Loss: 274.4559631347656\n",
            "Epoch 98/250, Loss: 284.4911193847656, Test Loss: 274.4047546386719\n",
            "Epoch 99/250, Loss: 282.9749450683594, Test Loss: 274.3538513183594\n",
            "Epoch 100/250, Loss: 286.70068359375, Test Loss: 274.3028564453125\n",
            "Epoch 101/250, Loss: 284.3993225097656, Test Loss: 274.25164794921875\n",
            "Epoch 102/250, Loss: 273.45196533203125, Test Loss: 274.2006530761719\n",
            "Epoch 103/250, Loss: 280.71160888671875, Test Loss: 274.15032958984375\n",
            "Epoch 104/250, Loss: 278.3613586425781, Test Loss: 274.0999450683594\n",
            "Epoch 105/250, Loss: 282.00177001953125, Test Loss: 274.04888916015625\n",
            "Epoch 106/250, Loss: 291.1371765136719, Test Loss: 273.997802734375\n",
            "Epoch 107/250, Loss: 278.4178771972656, Test Loss: 273.9463195800781\n",
            "Epoch 108/250, Loss: 285.71978759765625, Test Loss: 273.89471435546875\n",
            "Epoch 109/250, Loss: 282.66546630859375, Test Loss: 273.8422546386719\n",
            "Epoch 110/250, Loss: 280.1064758300781, Test Loss: 273.78997802734375\n",
            "Epoch 111/250, Loss: 276.79071044921875, Test Loss: 273.7379150390625\n",
            "Epoch 112/250, Loss: 275.00701904296875, Test Loss: 273.6869201660156\n",
            "Epoch 113/250, Loss: 280.1337890625, Test Loss: 273.6354675292969\n",
            "Epoch 114/250, Loss: 272.3447265625, Test Loss: 273.5838928222656\n",
            "Epoch 115/250, Loss: 287.9894714355469, Test Loss: 273.5318298339844\n",
            "Epoch 116/250, Loss: 286.6559143066406, Test Loss: 273.479736328125\n",
            "Epoch 117/250, Loss: 276.2147216796875, Test Loss: 273.4272155761719\n",
            "Epoch 118/250, Loss: 285.3812561035156, Test Loss: 273.37481689453125\n",
            "Epoch 119/250, Loss: 276.48095703125, Test Loss: 273.3223876953125\n",
            "Epoch 120/250, Loss: 274.3494873046875, Test Loss: 273.26971435546875\n",
            "Epoch 121/250, Loss: 273.0654296875, Test Loss: 273.21697998046875\n",
            "Epoch 122/250, Loss: 287.43780517578125, Test Loss: 273.1640319824219\n",
            "Epoch 123/250, Loss: 283.39105224609375, Test Loss: 273.111572265625\n",
            "Epoch 124/250, Loss: 275.7316589355469, Test Loss: 273.059326171875\n",
            "Epoch 125/250, Loss: 274.5921936035156, Test Loss: 273.007568359375\n",
            "Epoch 126/250, Loss: 273.1487731933594, Test Loss: 272.95526123046875\n",
            "Epoch 127/250, Loss: 282.03582763671875, Test Loss: 272.9034423828125\n",
            "Epoch 128/250, Loss: 286.72308349609375, Test Loss: 272.8514099121094\n",
            "Epoch 129/250, Loss: 279.05035400390625, Test Loss: 272.79901123046875\n",
            "Epoch 130/250, Loss: 277.0361328125, Test Loss: 272.74627685546875\n",
            "Epoch 131/250, Loss: 281.2405700683594, Test Loss: 272.69354248046875\n",
            "Epoch 132/250, Loss: 279.9676513671875, Test Loss: 272.64068603515625\n",
            "Epoch 133/250, Loss: 280.0147399902344, Test Loss: 272.58770751953125\n",
            "Epoch 134/250, Loss: 277.8316345214844, Test Loss: 272.5343933105469\n",
            "Epoch 135/250, Loss: 275.2982482910156, Test Loss: 272.4817199707031\n",
            "Epoch 136/250, Loss: 278.5156555175781, Test Loss: 272.4291687011719\n",
            "Epoch 137/250, Loss: 280.32684326171875, Test Loss: 272.3760986328125\n",
            "Epoch 138/250, Loss: 276.9941711425781, Test Loss: 272.32330322265625\n",
            "Epoch 139/250, Loss: 276.58587646484375, Test Loss: 272.2701110839844\n",
            "Epoch 140/250, Loss: 289.55462646484375, Test Loss: 272.21771240234375\n",
            "Epoch 141/250, Loss: 279.1555480957031, Test Loss: 272.1654357910156\n",
            "Epoch 142/250, Loss: 278.89752197265625, Test Loss: 272.1130065917969\n",
            "Epoch 143/250, Loss: 278.2581787109375, Test Loss: 272.0609436035156\n",
            "Epoch 144/250, Loss: 282.3815612792969, Test Loss: 272.0080261230469\n",
            "Epoch 145/250, Loss: 273.3604431152344, Test Loss: 271.95477294921875\n",
            "Epoch 146/250, Loss: 288.7310485839844, Test Loss: 271.9024658203125\n",
            "Epoch 147/250, Loss: 273.0043029785156, Test Loss: 271.84869384765625\n",
            "Epoch 148/250, Loss: 290.40765380859375, Test Loss: 271.7945251464844\n",
            "Epoch 149/250, Loss: 272.24505615234375, Test Loss: 271.7397766113281\n",
            "Epoch 150/250, Loss: 274.9385681152344, Test Loss: 271.6857604980469\n",
            "Epoch 151/250, Loss: 277.6347351074219, Test Loss: 271.6324462890625\n",
            "Epoch 152/250, Loss: 281.7692565917969, Test Loss: 271.5784606933594\n",
            "Epoch 153/250, Loss: 273.1299133300781, Test Loss: 271.5250549316406\n",
            "Epoch 154/250, Loss: 274.4753723144531, Test Loss: 271.47113037109375\n",
            "Epoch 155/250, Loss: 281.0983581542969, Test Loss: 271.41741943359375\n",
            "Epoch 156/250, Loss: 277.4997253417969, Test Loss: 271.36376953125\n",
            "Epoch 157/250, Loss: 288.71142578125, Test Loss: 271.3096923828125\n",
            "Epoch 158/250, Loss: 283.49554443359375, Test Loss: 271.25579833984375\n",
            "Epoch 159/250, Loss: 273.0636291503906, Test Loss: 271.2035827636719\n",
            "Epoch 160/250, Loss: 277.2458801269531, Test Loss: 271.1511535644531\n",
            "Epoch 161/250, Loss: 268.4742431640625, Test Loss: 271.09881591796875\n",
            "Epoch 162/250, Loss: 269.8870544433594, Test Loss: 271.04559326171875\n",
            "Epoch 163/250, Loss: 278.0650634765625, Test Loss: 270.9917297363281\n",
            "Epoch 164/250, Loss: 274.3063659667969, Test Loss: 270.9375\n",
            "Epoch 165/250, Loss: 275.3127746582031, Test Loss: 270.88397216796875\n",
            "Epoch 166/250, Loss: 277.45355224609375, Test Loss: 270.83026123046875\n",
            "Epoch 167/250, Loss: 279.2563781738281, Test Loss: 270.7749938964844\n",
            "Epoch 168/250, Loss: 274.1920471191406, Test Loss: 270.71893310546875\n",
            "Epoch 169/250, Loss: 272.265625, Test Loss: 270.6640625\n",
            "Epoch 170/250, Loss: 269.0775146484375, Test Loss: 270.60931396484375\n",
            "Epoch 171/250, Loss: 274.8253479003906, Test Loss: 270.55389404296875\n",
            "Epoch 172/250, Loss: 286.3828125, Test Loss: 270.49932861328125\n",
            "Epoch 173/250, Loss: 278.91265869140625, Test Loss: 270.4443054199219\n",
            "Epoch 174/250, Loss: 274.7251892089844, Test Loss: 270.3888244628906\n",
            "Epoch 175/250, Loss: 271.7083435058594, Test Loss: 270.33294677734375\n",
            "Epoch 176/250, Loss: 278.3187561035156, Test Loss: 270.27728271484375\n",
            "Epoch 177/250, Loss: 272.7147521972656, Test Loss: 270.2210388183594\n",
            "Epoch 178/250, Loss: 274.40576171875, Test Loss: 270.163818359375\n",
            "Epoch 179/250, Loss: 274.8246765136719, Test Loss: 270.1080322265625\n",
            "Epoch 180/250, Loss: 281.3382873535156, Test Loss: 270.0515441894531\n",
            "Epoch 181/250, Loss: 268.8886413574219, Test Loss: 269.9957275390625\n",
            "Epoch 182/250, Loss: 271.4696960449219, Test Loss: 269.94024658203125\n",
            "Epoch 183/250, Loss: 277.8976135253906, Test Loss: 269.8841857910156\n",
            "Epoch 184/250, Loss: 277.61151123046875, Test Loss: 269.82659912109375\n",
            "Epoch 185/250, Loss: 283.7369689941406, Test Loss: 269.7706604003906\n",
            "Epoch 186/250, Loss: 273.3109436035156, Test Loss: 269.7141418457031\n",
            "Epoch 187/250, Loss: 276.0377502441406, Test Loss: 269.6575622558594\n",
            "Epoch 188/250, Loss: 283.26507568359375, Test Loss: 269.5996398925781\n",
            "Epoch 189/250, Loss: 279.6277160644531, Test Loss: 269.5409851074219\n",
            "Epoch 190/250, Loss: 278.52276611328125, Test Loss: 269.48321533203125\n",
            "Epoch 191/250, Loss: 270.1380920410156, Test Loss: 269.4269104003906\n",
            "Epoch 192/250, Loss: 269.8154602050781, Test Loss: 269.3705139160156\n",
            "Epoch 193/250, Loss: 280.5471496582031, Test Loss: 269.3127136230469\n",
            "Epoch 194/250, Loss: 271.7395324707031, Test Loss: 269.2545471191406\n",
            "Epoch 195/250, Loss: 280.53173828125, Test Loss: 269.19696044921875\n",
            "Epoch 196/250, Loss: 263.5986633300781, Test Loss: 269.138427734375\n",
            "Epoch 197/250, Loss: 271.0040588378906, Test Loss: 269.0794372558594\n",
            "Epoch 198/250, Loss: 270.5771789550781, Test Loss: 269.0202331542969\n",
            "Epoch 199/250, Loss: 270.6908264160156, Test Loss: 268.96124267578125\n",
            "Epoch 200/250, Loss: 271.0376281738281, Test Loss: 268.9024353027344\n",
            "Epoch 201/250, Loss: 273.6324157714844, Test Loss: 268.84515380859375\n",
            "Epoch 202/250, Loss: 280.3904113769531, Test Loss: 268.7877197265625\n",
            "Epoch 203/250, Loss: 275.9827880859375, Test Loss: 268.73052978515625\n",
            "Epoch 204/250, Loss: 273.73486328125, Test Loss: 268.6720886230469\n",
            "Epoch 205/250, Loss: 278.1327209472656, Test Loss: 268.6126403808594\n",
            "Epoch 206/250, Loss: 280.0306396484375, Test Loss: 268.5533752441406\n",
            "Epoch 207/250, Loss: 282.3138122558594, Test Loss: 268.4930114746094\n",
            "Epoch 208/250, Loss: 267.1560974121094, Test Loss: 268.4335021972656\n",
            "Epoch 209/250, Loss: 269.2303771972656, Test Loss: 268.3729553222656\n",
            "Epoch 210/250, Loss: 273.0833435058594, Test Loss: 268.31219482421875\n",
            "Epoch 211/250, Loss: 275.40875244140625, Test Loss: 268.2513427734375\n",
            "Epoch 212/250, Loss: 273.6314392089844, Test Loss: 268.19000244140625\n",
            "Epoch 213/250, Loss: 271.9237365722656, Test Loss: 268.1296691894531\n",
            "Epoch 214/250, Loss: 273.1820983886719, Test Loss: 268.0688781738281\n",
            "Epoch 215/250, Loss: 262.7252197265625, Test Loss: 268.0089111328125\n",
            "Epoch 216/250, Loss: 278.6819763183594, Test Loss: 267.948486328125\n",
            "Epoch 217/250, Loss: 275.08489990234375, Test Loss: 267.88824462890625\n",
            "Epoch 218/250, Loss: 275.4505920410156, Test Loss: 267.829345703125\n",
            "Epoch 219/250, Loss: 277.44451904296875, Test Loss: 267.7702331542969\n",
            "Epoch 220/250, Loss: 277.1163635253906, Test Loss: 267.71087646484375\n",
            "Epoch 221/250, Loss: 261.79656982421875, Test Loss: 267.6514892578125\n",
            "Epoch 222/250, Loss: 276.8818359375, Test Loss: 267.5932922363281\n",
            "Epoch 223/250, Loss: 265.7345886230469, Test Loss: 267.53509521484375\n",
            "Epoch 224/250, Loss: 272.4498596191406, Test Loss: 267.4759826660156\n",
            "Epoch 225/250, Loss: 270.137451171875, Test Loss: 267.4156799316406\n",
            "Epoch 226/250, Loss: 273.1954650878906, Test Loss: 267.35528564453125\n",
            "Epoch 227/250, Loss: 268.95489501953125, Test Loss: 267.2958679199219\n",
            "Epoch 228/250, Loss: 272.9953918457031, Test Loss: 267.23577880859375\n",
            "Epoch 229/250, Loss: 267.58251953125, Test Loss: 267.1749572753906\n",
            "Epoch 230/250, Loss: 276.4029235839844, Test Loss: 267.1126403808594\n",
            "Epoch 231/250, Loss: 271.2633361816406, Test Loss: 267.052001953125\n",
            "Epoch 232/250, Loss: 265.68743896484375, Test Loss: 266.99066162109375\n",
            "Epoch 233/250, Loss: 269.0853576660156, Test Loss: 266.92816162109375\n",
            "Epoch 234/250, Loss: 279.0464782714844, Test Loss: 266.86773681640625\n",
            "Epoch 235/250, Loss: 275.3790588378906, Test Loss: 266.8058776855469\n",
            "Epoch 236/250, Loss: 258.10986328125, Test Loss: 266.7455139160156\n",
            "Epoch 237/250, Loss: 271.5438232421875, Test Loss: 266.68389892578125\n",
            "Epoch 238/250, Loss: 270.62725830078125, Test Loss: 266.62091064453125\n",
            "Epoch 239/250, Loss: 272.0230407714844, Test Loss: 266.55914306640625\n",
            "Epoch 240/250, Loss: 261.85760498046875, Test Loss: 266.4959716796875\n",
            "Epoch 241/250, Loss: 272.2444152832031, Test Loss: 266.4322509765625\n",
            "Epoch 242/250, Loss: 273.849609375, Test Loss: 266.36834716796875\n",
            "Epoch 243/250, Loss: 261.8592834472656, Test Loss: 266.3034362792969\n",
            "Epoch 244/250, Loss: 273.3670349121094, Test Loss: 266.2395324707031\n",
            "Epoch 245/250, Loss: 264.0894775390625, Test Loss: 266.1752014160156\n",
            "Epoch 246/250, Loss: 266.37872314453125, Test Loss: 266.1111755371094\n",
            "Epoch 247/250, Loss: 271.3959045410156, Test Loss: 266.0452880859375\n",
            "Epoch 248/250, Loss: 270.43914794921875, Test Loss: 265.97979736328125\n",
            "Epoch 249/250, Loss: 273.9856262207031, Test Loss: 265.9153137207031\n",
            "Epoch 250/250, Loss: 276.5793762207031, Test Loss: 265.8492736816406\n",
            "Final MSE: 265.8492736816406\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=256, epochs=1\n",
            "Epoch 1/1, Loss: 273.1329345703125, Test Loss: 270.15728759765625\n",
            "Final MSE: 270.15728759765625\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=256, epochs=10\n",
            "Epoch 1/10, Loss: 289.39581298828125, Test Loss: 290.12274169921875\n",
            "Epoch 2/10, Loss: 296.5600891113281, Test Loss: 290.0982360839844\n",
            "Epoch 3/10, Loss: 290.9813232421875, Test Loss: 290.07373046875\n",
            "Epoch 4/10, Loss: 304.39862060546875, Test Loss: 290.0492858886719\n",
            "Epoch 5/10, Loss: 295.6147155761719, Test Loss: 290.0247497558594\n",
            "Epoch 6/10, Loss: 299.56463623046875, Test Loss: 289.99993896484375\n",
            "Epoch 7/10, Loss: 300.70166015625, Test Loss: 289.97515869140625\n",
            "Epoch 8/10, Loss: 293.8283386230469, Test Loss: 289.95050048828125\n",
            "Epoch 9/10, Loss: 294.8389892578125, Test Loss: 289.92596435546875\n",
            "Epoch 10/10, Loss: 290.4337463378906, Test Loss: 289.9012756347656\n",
            "Final MSE: 289.9013366699219\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=256, epochs=25\n",
            "Epoch 1/25, Loss: 281.5643310546875, Test Loss: 274.2026062011719\n",
            "Epoch 2/25, Loss: 282.3204650878906, Test Loss: 274.1539611816406\n",
            "Epoch 3/25, Loss: 282.5123596191406, Test Loss: 274.1039733886719\n",
            "Epoch 4/25, Loss: 285.7334899902344, Test Loss: 274.0531921386719\n",
            "Epoch 5/25, Loss: 283.07080078125, Test Loss: 274.0025939941406\n",
            "Epoch 6/25, Loss: 279.9198913574219, Test Loss: 273.9539489746094\n",
            "Epoch 7/25, Loss: 285.92083740234375, Test Loss: 273.9053955078125\n",
            "Epoch 8/25, Loss: 290.63848876953125, Test Loss: 273.8567199707031\n",
            "Epoch 9/25, Loss: 285.4476013183594, Test Loss: 273.8074951171875\n",
            "Epoch 10/25, Loss: 284.9264831542969, Test Loss: 273.757568359375\n",
            "Epoch 11/25, Loss: 271.8380126953125, Test Loss: 273.7080078125\n",
            "Epoch 12/25, Loss: 275.7986145019531, Test Loss: 273.6584777832031\n",
            "Epoch 13/25, Loss: 278.8609313964844, Test Loss: 273.6086730957031\n",
            "Epoch 14/25, Loss: 294.0547790527344, Test Loss: 273.5589599609375\n",
            "Epoch 15/25, Loss: 285.67376708984375, Test Loss: 273.5088195800781\n",
            "Epoch 16/25, Loss: 268.40106201171875, Test Loss: 273.45916748046875\n",
            "Epoch 17/25, Loss: 285.6881408691406, Test Loss: 273.41015625\n",
            "Epoch 18/25, Loss: 290.0873718261719, Test Loss: 273.3608703613281\n",
            "Epoch 19/25, Loss: 280.3730773925781, Test Loss: 273.3110046386719\n",
            "Epoch 20/25, Loss: 290.9688720703125, Test Loss: 273.2611389160156\n",
            "Epoch 21/25, Loss: 286.4551086425781, Test Loss: 273.2117919921875\n",
            "Epoch 22/25, Loss: 284.7745666503906, Test Loss: 273.1622314453125\n",
            "Epoch 23/25, Loss: 284.0697326660156, Test Loss: 273.1126708984375\n",
            "Epoch 24/25, Loss: 291.3760070800781, Test Loss: 273.0631408691406\n",
            "Epoch 25/25, Loss: 275.33782958984375, Test Loss: 273.0134582519531\n",
            "Final MSE: 273.01348876953125\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=256, epochs=50\n",
            "Epoch 1/50, Loss: 282.9201354980469, Test Loss: 273.5669250488281\n",
            "Epoch 2/50, Loss: 282.73284912109375, Test Loss: 273.5469665527344\n",
            "Epoch 3/50, Loss: 281.0287170410156, Test Loss: 273.5265197753906\n",
            "Epoch 4/50, Loss: 281.7236633300781, Test Loss: 273.5061950683594\n",
            "Epoch 5/50, Loss: 278.6920471191406, Test Loss: 273.4859924316406\n",
            "Epoch 6/50, Loss: 281.8992004394531, Test Loss: 273.46588134765625\n",
            "Epoch 7/50, Loss: 275.2789306640625, Test Loss: 273.44580078125\n",
            "Epoch 8/50, Loss: 284.6822509765625, Test Loss: 273.4255676269531\n",
            "Epoch 9/50, Loss: 286.34466552734375, Test Loss: 273.4051513671875\n",
            "Epoch 10/50, Loss: 292.3877868652344, Test Loss: 273.3848876953125\n",
            "Epoch 11/50, Loss: 272.32586669921875, Test Loss: 273.36468505859375\n",
            "Epoch 12/50, Loss: 285.1773681640625, Test Loss: 273.34466552734375\n",
            "Epoch 13/50, Loss: 280.6270446777344, Test Loss: 273.3247375488281\n",
            "Epoch 14/50, Loss: 279.5716857910156, Test Loss: 273.30487060546875\n",
            "Epoch 15/50, Loss: 283.7777099609375, Test Loss: 273.2851257324219\n",
            "Epoch 16/50, Loss: 277.68536376953125, Test Loss: 273.2654724121094\n",
            "Epoch 17/50, Loss: 274.76116943359375, Test Loss: 273.2455749511719\n",
            "Epoch 18/50, Loss: 280.4139709472656, Test Loss: 273.2257080078125\n",
            "Epoch 19/50, Loss: 285.45025634765625, Test Loss: 273.2058410644531\n",
            "Epoch 20/50, Loss: 290.4562072753906, Test Loss: 273.18597412109375\n",
            "Epoch 21/50, Loss: 278.30999755859375, Test Loss: 273.1660461425781\n",
            "Epoch 22/50, Loss: 278.52349853515625, Test Loss: 273.1462707519531\n",
            "Epoch 23/50, Loss: 284.8695068359375, Test Loss: 273.1264953613281\n",
            "Epoch 24/50, Loss: 290.86102294921875, Test Loss: 273.1067199707031\n",
            "Epoch 25/50, Loss: 285.45068359375, Test Loss: 273.0871887207031\n",
            "Epoch 26/50, Loss: 278.9696350097656, Test Loss: 273.0679016113281\n",
            "Epoch 27/50, Loss: 285.6550598144531, Test Loss: 273.048583984375\n",
            "Epoch 28/50, Loss: 282.1256408691406, Test Loss: 273.0292053222656\n",
            "Epoch 29/50, Loss: 276.49761962890625, Test Loss: 273.0098571777344\n",
            "Epoch 30/50, Loss: 277.98828125, Test Loss: 272.9905700683594\n",
            "Epoch 31/50, Loss: 282.7950439453125, Test Loss: 272.97149658203125\n",
            "Epoch 32/50, Loss: 275.5602111816406, Test Loss: 272.9524841308594\n",
            "Epoch 33/50, Loss: 286.8559265136719, Test Loss: 272.933349609375\n",
            "Epoch 34/50, Loss: 281.23760986328125, Test Loss: 272.91400146484375\n",
            "Epoch 35/50, Loss: 280.2593994140625, Test Loss: 272.89453125\n",
            "Epoch 36/50, Loss: 281.3182373046875, Test Loss: 272.87493896484375\n",
            "Epoch 37/50, Loss: 279.2406005859375, Test Loss: 272.85528564453125\n",
            "Epoch 38/50, Loss: 278.15234375, Test Loss: 272.8356628417969\n",
            "Epoch 39/50, Loss: 289.5272216796875, Test Loss: 272.8161926269531\n",
            "Epoch 40/50, Loss: 276.8692932128906, Test Loss: 272.79681396484375\n",
            "Epoch 41/50, Loss: 278.2248840332031, Test Loss: 272.7774353027344\n",
            "Epoch 42/50, Loss: 272.8454895019531, Test Loss: 272.758056640625\n",
            "Epoch 43/50, Loss: 281.36956787109375, Test Loss: 272.7386169433594\n",
            "Epoch 44/50, Loss: 286.1075744628906, Test Loss: 272.7190856933594\n",
            "Epoch 45/50, Loss: 285.3018798828125, Test Loss: 272.69952392578125\n",
            "Epoch 46/50, Loss: 286.2647399902344, Test Loss: 272.6799621582031\n",
            "Epoch 47/50, Loss: 284.49993896484375, Test Loss: 272.66033935546875\n",
            "Epoch 48/50, Loss: 272.31646728515625, Test Loss: 272.64080810546875\n",
            "Epoch 49/50, Loss: 285.9761657714844, Test Loss: 272.6215515136719\n",
            "Epoch 50/50, Loss: 285.4564514160156, Test Loss: 272.60247802734375\n",
            "Final MSE: 272.6024475097656\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=256, epochs=100\n",
            "Epoch 1/100, Loss: 268.4260559082031, Test Loss: 265.6163330078125\n",
            "Epoch 2/100, Loss: 276.1546325683594, Test Loss: 265.5843505859375\n",
            "Epoch 3/100, Loss: 275.9930419921875, Test Loss: 265.5522766113281\n",
            "Epoch 4/100, Loss: 269.75286865234375, Test Loss: 265.51995849609375\n",
            "Epoch 5/100, Loss: 267.41943359375, Test Loss: 265.4878845214844\n",
            "Epoch 6/100, Loss: 273.2897644042969, Test Loss: 265.4558410644531\n",
            "Epoch 7/100, Loss: 269.66107177734375, Test Loss: 265.4236755371094\n",
            "Epoch 8/100, Loss: 265.50225830078125, Test Loss: 265.3913269042969\n",
            "Epoch 9/100, Loss: 280.40234375, Test Loss: 265.35870361328125\n",
            "Epoch 10/100, Loss: 274.77581787109375, Test Loss: 265.3260192871094\n",
            "Epoch 11/100, Loss: 264.3004150390625, Test Loss: 265.2932434082031\n",
            "Epoch 12/100, Loss: 270.25421142578125, Test Loss: 265.2604675292969\n",
            "Epoch 13/100, Loss: 274.26123046875, Test Loss: 265.2278137207031\n",
            "Epoch 14/100, Loss: 268.55914306640625, Test Loss: 265.1950378417969\n",
            "Epoch 15/100, Loss: 270.11773681640625, Test Loss: 265.1621398925781\n",
            "Epoch 16/100, Loss: 269.0794982910156, Test Loss: 265.1289978027344\n",
            "Epoch 17/100, Loss: 283.3748779296875, Test Loss: 265.095703125\n",
            "Epoch 18/100, Loss: 270.00921630859375, Test Loss: 265.0623474121094\n",
            "Epoch 19/100, Loss: 268.1428527832031, Test Loss: 265.0290222167969\n",
            "Epoch 20/100, Loss: 266.8080139160156, Test Loss: 264.99591064453125\n",
            "Epoch 21/100, Loss: 278.9525451660156, Test Loss: 264.96270751953125\n",
            "Epoch 22/100, Loss: 272.846435546875, Test Loss: 264.9293518066406\n",
            "Epoch 23/100, Loss: 283.08331298828125, Test Loss: 264.895751953125\n",
            "Epoch 24/100, Loss: 267.1224365234375, Test Loss: 264.8618469238281\n",
            "Epoch 25/100, Loss: 276.9346618652344, Test Loss: 264.827880859375\n",
            "Epoch 26/100, Loss: 263.21148681640625, Test Loss: 264.7939758300781\n",
            "Epoch 27/100, Loss: 278.2257080078125, Test Loss: 264.760009765625\n",
            "Epoch 28/100, Loss: 274.5185546875, Test Loss: 264.7259521484375\n",
            "Epoch 29/100, Loss: 272.4060363769531, Test Loss: 264.69189453125\n",
            "Epoch 30/100, Loss: 270.5553894042969, Test Loss: 264.6577453613281\n",
            "Epoch 31/100, Loss: 277.9781494140625, Test Loss: 264.6233215332031\n",
            "Epoch 32/100, Loss: 276.5153503417969, Test Loss: 264.5887145996094\n",
            "Epoch 33/100, Loss: 274.1387634277344, Test Loss: 264.5541076660156\n",
            "Epoch 34/100, Loss: 274.63787841796875, Test Loss: 264.51959228515625\n",
            "Epoch 35/100, Loss: 267.5992126464844, Test Loss: 264.4850769042969\n",
            "Epoch 36/100, Loss: 260.0922546386719, Test Loss: 264.4505310058594\n",
            "Epoch 37/100, Loss: 273.126953125, Test Loss: 264.4160461425781\n",
            "Epoch 38/100, Loss: 274.3539123535156, Test Loss: 264.38140869140625\n",
            "Epoch 39/100, Loss: 267.2703857421875, Test Loss: 264.3467712402344\n",
            "Epoch 40/100, Loss: 274.87249755859375, Test Loss: 264.3119201660156\n",
            "Epoch 41/100, Loss: 267.6351318359375, Test Loss: 264.2769775390625\n",
            "Epoch 42/100, Loss: 271.578125, Test Loss: 264.24188232421875\n",
            "Epoch 43/100, Loss: 276.55517578125, Test Loss: 264.20672607421875\n",
            "Epoch 44/100, Loss: 272.6807861328125, Test Loss: 264.1715087890625\n",
            "Epoch 45/100, Loss: 266.9424133300781, Test Loss: 264.1361999511719\n",
            "Epoch 46/100, Loss: 266.1628723144531, Test Loss: 264.100830078125\n",
            "Epoch 47/100, Loss: 269.8347473144531, Test Loss: 264.0652160644531\n",
            "Epoch 48/100, Loss: 274.62115478515625, Test Loss: 264.0293884277344\n",
            "Epoch 49/100, Loss: 272.9752197265625, Test Loss: 263.9936218261719\n",
            "Epoch 50/100, Loss: 267.8847961425781, Test Loss: 263.9579162597656\n",
            "Epoch 51/100, Loss: 269.083251953125, Test Loss: 263.9222412109375\n",
            "Epoch 52/100, Loss: 273.6932373046875, Test Loss: 263.8865661621094\n",
            "Epoch 53/100, Loss: 271.4097595214844, Test Loss: 263.8508605957031\n",
            "Epoch 54/100, Loss: 275.9371643066406, Test Loss: 263.8150329589844\n",
            "Epoch 55/100, Loss: 274.75048828125, Test Loss: 263.779052734375\n",
            "Epoch 56/100, Loss: 270.5570373535156, Test Loss: 263.74273681640625\n",
            "Epoch 57/100, Loss: 279.8982238769531, Test Loss: 263.7065734863281\n",
            "Epoch 58/100, Loss: 276.0314636230469, Test Loss: 263.6702880859375\n",
            "Epoch 59/100, Loss: 274.9752502441406, Test Loss: 263.63385009765625\n",
            "Epoch 60/100, Loss: 270.3070373535156, Test Loss: 263.5973815917969\n",
            "Epoch 61/100, Loss: 265.3599853515625, Test Loss: 263.5607604980469\n",
            "Epoch 62/100, Loss: 268.52142333984375, Test Loss: 263.5237121582031\n",
            "Epoch 63/100, Loss: 264.36688232421875, Test Loss: 263.48651123046875\n",
            "Epoch 64/100, Loss: 269.3697204589844, Test Loss: 263.44921875\n",
            "Epoch 65/100, Loss: 272.68841552734375, Test Loss: 263.4119873046875\n",
            "Epoch 66/100, Loss: 272.53082275390625, Test Loss: 263.3747253417969\n",
            "Epoch 67/100, Loss: 270.134033203125, Test Loss: 263.337646484375\n",
            "Epoch 68/100, Loss: 279.7926330566406, Test Loss: 263.3002014160156\n",
            "Epoch 69/100, Loss: 269.32537841796875, Test Loss: 263.2625732421875\n",
            "Epoch 70/100, Loss: 268.9573669433594, Test Loss: 263.22503662109375\n",
            "Epoch 71/100, Loss: 268.8397521972656, Test Loss: 263.18743896484375\n",
            "Epoch 72/100, Loss: 262.3775939941406, Test Loss: 263.14971923828125\n",
            "Epoch 73/100, Loss: 262.4132995605469, Test Loss: 263.1119079589844\n",
            "Epoch 74/100, Loss: 275.4610290527344, Test Loss: 263.0741271972656\n",
            "Epoch 75/100, Loss: 273.32861328125, Test Loss: 263.0359802246094\n",
            "Epoch 76/100, Loss: 267.5671691894531, Test Loss: 262.9978332519531\n",
            "Epoch 77/100, Loss: 276.3949279785156, Test Loss: 262.9598388671875\n",
            "Epoch 78/100, Loss: 274.1422424316406, Test Loss: 262.9216613769531\n",
            "Epoch 79/100, Loss: 272.558349609375, Test Loss: 262.88360595703125\n",
            "Epoch 80/100, Loss: 270.0779724121094, Test Loss: 262.8455505371094\n",
            "Epoch 81/100, Loss: 275.40057373046875, Test Loss: 262.8070983886719\n",
            "Epoch 82/100, Loss: 272.9357604980469, Test Loss: 262.7686767578125\n",
            "Epoch 83/100, Loss: 267.3857727050781, Test Loss: 262.73028564453125\n",
            "Epoch 84/100, Loss: 272.9427490234375, Test Loss: 262.6915283203125\n",
            "Epoch 85/100, Loss: 271.3340148925781, Test Loss: 262.65240478515625\n",
            "Epoch 86/100, Loss: 268.3039245605469, Test Loss: 262.6126708984375\n",
            "Epoch 87/100, Loss: 265.93878173828125, Test Loss: 262.57293701171875\n",
            "Epoch 88/100, Loss: 271.24334716796875, Test Loss: 262.53338623046875\n",
            "Epoch 89/100, Loss: 267.0242614746094, Test Loss: 262.49334716796875\n",
            "Epoch 90/100, Loss: 273.595703125, Test Loss: 262.45281982421875\n",
            "Epoch 91/100, Loss: 269.5097351074219, Test Loss: 262.4122314453125\n",
            "Epoch 92/100, Loss: 270.4495544433594, Test Loss: 262.3714294433594\n",
            "Epoch 93/100, Loss: 266.5320129394531, Test Loss: 262.3309326171875\n",
            "Epoch 94/100, Loss: 262.1753234863281, Test Loss: 262.2905578613281\n",
            "Epoch 95/100, Loss: 264.5821228027344, Test Loss: 262.2500305175781\n",
            "Epoch 96/100, Loss: 275.4067077636719, Test Loss: 262.20953369140625\n",
            "Epoch 97/100, Loss: 273.25286865234375, Test Loss: 262.1684265136719\n",
            "Epoch 98/100, Loss: 273.1094970703125, Test Loss: 262.1266784667969\n",
            "Epoch 99/100, Loss: 264.1246643066406, Test Loss: 262.0850524902344\n",
            "Epoch 100/100, Loss: 273.7297668457031, Test Loss: 262.0438232421875\n",
            "Final MSE: 262.0438232421875\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=256, epochs=250\n",
            "Epoch 1/250, Loss: 290.5851745605469, Test Loss: 283.28338623046875\n",
            "Epoch 2/250, Loss: 283.58294677734375, Test Loss: 283.2514343261719\n",
            "Epoch 3/250, Loss: 279.00921630859375, Test Loss: 283.2188415527344\n",
            "Epoch 4/250, Loss: 290.0823974609375, Test Loss: 283.1858825683594\n",
            "Epoch 5/250, Loss: 293.6285705566406, Test Loss: 283.152587890625\n",
            "Epoch 6/250, Loss: 289.7162170410156, Test Loss: 283.1193542480469\n",
            "Epoch 7/250, Loss: 286.1639099121094, Test Loss: 283.08685302734375\n",
            "Epoch 8/250, Loss: 285.40838623046875, Test Loss: 283.0543212890625\n",
            "Epoch 9/250, Loss: 279.43316650390625, Test Loss: 283.0217590332031\n",
            "Epoch 10/250, Loss: 294.09954833984375, Test Loss: 282.98956298828125\n",
            "Epoch 11/250, Loss: 294.52099609375, Test Loss: 282.95758056640625\n",
            "Epoch 12/250, Loss: 297.6642150878906, Test Loss: 282.9254455566406\n",
            "Epoch 13/250, Loss: 289.98858642578125, Test Loss: 282.8929748535156\n",
            "Epoch 14/250, Loss: 292.5973205566406, Test Loss: 282.8601989746094\n",
            "Epoch 15/250, Loss: 287.6649169921875, Test Loss: 282.82708740234375\n",
            "Epoch 16/250, Loss: 285.4447021484375, Test Loss: 282.7942199707031\n",
            "Epoch 17/250, Loss: 285.0937805175781, Test Loss: 282.7615661621094\n",
            "Epoch 18/250, Loss: 295.6714172363281, Test Loss: 282.7294921875\n",
            "Epoch 19/250, Loss: 295.7472839355469, Test Loss: 282.6977844238281\n",
            "Epoch 20/250, Loss: 285.99127197265625, Test Loss: 282.6656494140625\n",
            "Epoch 21/250, Loss: 286.6199951171875, Test Loss: 282.6329650878906\n",
            "Epoch 22/250, Loss: 293.3343200683594, Test Loss: 282.6001281738281\n",
            "Epoch 23/250, Loss: 290.4899597167969, Test Loss: 282.56744384765625\n",
            "Epoch 24/250, Loss: 283.06536865234375, Test Loss: 282.5347595214844\n",
            "Epoch 25/250, Loss: 295.04833984375, Test Loss: 282.50189208984375\n",
            "Epoch 26/250, Loss: 293.85736083984375, Test Loss: 282.4692687988281\n",
            "Epoch 27/250, Loss: 290.4444885253906, Test Loss: 282.4371643066406\n",
            "Epoch 28/250, Loss: 274.4089050292969, Test Loss: 282.4055480957031\n",
            "Epoch 29/250, Loss: 281.9906921386719, Test Loss: 282.3741455078125\n",
            "Epoch 30/250, Loss: 278.76983642578125, Test Loss: 282.3423767089844\n",
            "Epoch 31/250, Loss: 295.7231750488281, Test Loss: 282.3105163574219\n",
            "Epoch 32/250, Loss: 285.6003723144531, Test Loss: 282.2790222167969\n",
            "Epoch 33/250, Loss: 286.7674865722656, Test Loss: 282.24737548828125\n",
            "Epoch 34/250, Loss: 284.2176513671875, Test Loss: 282.21533203125\n",
            "Epoch 35/250, Loss: 279.19805908203125, Test Loss: 282.1836242675781\n",
            "Epoch 36/250, Loss: 301.3428039550781, Test Loss: 282.1518859863281\n",
            "Epoch 37/250, Loss: 282.76885986328125, Test Loss: 282.1199951171875\n",
            "Epoch 38/250, Loss: 289.29510498046875, Test Loss: 282.087890625\n",
            "Epoch 39/250, Loss: 281.29766845703125, Test Loss: 282.0558776855469\n",
            "Epoch 40/250, Loss: 286.65203857421875, Test Loss: 282.02435302734375\n",
            "Epoch 41/250, Loss: 298.40802001953125, Test Loss: 281.9928894042969\n",
            "Epoch 42/250, Loss: 285.4796447753906, Test Loss: 281.9613952636719\n",
            "Epoch 43/250, Loss: 287.0625915527344, Test Loss: 281.9300231933594\n",
            "Epoch 44/250, Loss: 287.1244201660156, Test Loss: 281.8980712890625\n",
            "Epoch 45/250, Loss: 285.62274169921875, Test Loss: 281.8652648925781\n",
            "Epoch 46/250, Loss: 298.3539733886719, Test Loss: 281.8318786621094\n",
            "Epoch 47/250, Loss: 289.8767395019531, Test Loss: 281.79888916015625\n",
            "Epoch 48/250, Loss: 293.18560791015625, Test Loss: 281.766845703125\n",
            "Epoch 49/250, Loss: 284.7379455566406, Test Loss: 281.73516845703125\n",
            "Epoch 50/250, Loss: 283.5480041503906, Test Loss: 281.7032165527344\n",
            "Epoch 51/250, Loss: 289.13287353515625, Test Loss: 281.6714782714844\n",
            "Epoch 52/250, Loss: 289.29364013671875, Test Loss: 281.63983154296875\n",
            "Epoch 53/250, Loss: 288.32757568359375, Test Loss: 281.6081237792969\n",
            "Epoch 54/250, Loss: 294.7244873046875, Test Loss: 281.57659912109375\n",
            "Epoch 55/250, Loss: 288.8056945800781, Test Loss: 281.5450744628906\n",
            "Epoch 56/250, Loss: 285.09356689453125, Test Loss: 281.513427734375\n",
            "Epoch 57/250, Loss: 292.3559875488281, Test Loss: 281.481689453125\n",
            "Epoch 58/250, Loss: 294.2384033203125, Test Loss: 281.4500732421875\n",
            "Epoch 59/250, Loss: 280.1942138671875, Test Loss: 281.4183044433594\n",
            "Epoch 60/250, Loss: 286.26727294921875, Test Loss: 281.38653564453125\n",
            "Epoch 61/250, Loss: 289.9826965332031, Test Loss: 281.3550720214844\n",
            "Epoch 62/250, Loss: 286.07421875, Test Loss: 281.32366943359375\n",
            "Epoch 63/250, Loss: 282.61968994140625, Test Loss: 281.292236328125\n",
            "Epoch 64/250, Loss: 293.55908203125, Test Loss: 281.2613220214844\n",
            "Epoch 65/250, Loss: 288.7525634765625, Test Loss: 281.23052978515625\n",
            "Epoch 66/250, Loss: 296.3714294433594, Test Loss: 281.199462890625\n",
            "Epoch 67/250, Loss: 284.8833923339844, Test Loss: 281.1680603027344\n",
            "Epoch 68/250, Loss: 295.7711486816406, Test Loss: 281.13616943359375\n",
            "Epoch 69/250, Loss: 289.52197265625, Test Loss: 281.1042785644531\n",
            "Epoch 70/250, Loss: 287.7419128417969, Test Loss: 281.07244873046875\n",
            "Epoch 71/250, Loss: 292.7459411621094, Test Loss: 281.0406799316406\n",
            "Epoch 72/250, Loss: 292.09857177734375, Test Loss: 281.0089416503906\n",
            "Epoch 73/250, Loss: 283.2979736328125, Test Loss: 280.97705078125\n",
            "Epoch 74/250, Loss: 284.4807434082031, Test Loss: 280.94525146484375\n",
            "Epoch 75/250, Loss: 287.32916259765625, Test Loss: 280.9137878417969\n",
            "Epoch 76/250, Loss: 283.0501403808594, Test Loss: 280.8824462890625\n",
            "Epoch 77/250, Loss: 292.68267822265625, Test Loss: 280.85137939453125\n",
            "Epoch 78/250, Loss: 284.9143981933594, Test Loss: 280.82049560546875\n",
            "Epoch 79/250, Loss: 285.2606506347656, Test Loss: 280.7900390625\n",
            "Epoch 80/250, Loss: 278.78729248046875, Test Loss: 280.75970458984375\n",
            "Epoch 81/250, Loss: 296.5479736328125, Test Loss: 280.7291259765625\n",
            "Epoch 82/250, Loss: 276.9248352050781, Test Loss: 280.69854736328125\n",
            "Epoch 83/250, Loss: 294.05303955078125, Test Loss: 280.6682434082031\n",
            "Epoch 84/250, Loss: 288.8624572753906, Test Loss: 280.6383361816406\n",
            "Epoch 85/250, Loss: 289.2027587890625, Test Loss: 280.60858154296875\n",
            "Epoch 86/250, Loss: 285.71429443359375, Test Loss: 280.5788269042969\n",
            "Epoch 87/250, Loss: 282.7645263671875, Test Loss: 280.54852294921875\n",
            "Epoch 88/250, Loss: 280.5007019042969, Test Loss: 280.517822265625\n",
            "Epoch 89/250, Loss: 282.85089111328125, Test Loss: 280.48724365234375\n",
            "Epoch 90/250, Loss: 284.0155944824219, Test Loss: 280.45697021484375\n",
            "Epoch 91/250, Loss: 294.5147705078125, Test Loss: 280.4273986816406\n",
            "Epoch 92/250, Loss: 282.2246398925781, Test Loss: 280.3981628417969\n",
            "Epoch 93/250, Loss: 284.0205383300781, Test Loss: 280.3685607910156\n",
            "Epoch 94/250, Loss: 300.95697021484375, Test Loss: 280.3381652832031\n",
            "Epoch 95/250, Loss: 278.6578369140625, Test Loss: 280.3072509765625\n",
            "Epoch 96/250, Loss: 289.7464904785156, Test Loss: 280.2759704589844\n",
            "Epoch 97/250, Loss: 282.2485656738281, Test Loss: 280.24420166015625\n",
            "Epoch 98/250, Loss: 282.20489501953125, Test Loss: 280.2121887207031\n",
            "Epoch 99/250, Loss: 289.6354064941406, Test Loss: 280.18023681640625\n",
            "Epoch 100/250, Loss: 289.24835205078125, Test Loss: 280.1487731933594\n",
            "Epoch 101/250, Loss: 288.0870056152344, Test Loss: 280.1168518066406\n",
            "Epoch 102/250, Loss: 291.73858642578125, Test Loss: 280.0846252441406\n",
            "Epoch 103/250, Loss: 283.6060485839844, Test Loss: 280.0522766113281\n",
            "Epoch 104/250, Loss: 283.60614013671875, Test Loss: 280.0204162597656\n",
            "Epoch 105/250, Loss: 283.95635986328125, Test Loss: 279.98846435546875\n",
            "Epoch 106/250, Loss: 284.15216064453125, Test Loss: 279.956298828125\n",
            "Epoch 107/250, Loss: 287.48486328125, Test Loss: 279.924072265625\n",
            "Epoch 108/250, Loss: 287.6596984863281, Test Loss: 279.8917541503906\n",
            "Epoch 109/250, Loss: 290.0667724609375, Test Loss: 279.859130859375\n",
            "Epoch 110/250, Loss: 290.5428771972656, Test Loss: 279.8269958496094\n",
            "Epoch 111/250, Loss: 278.5221252441406, Test Loss: 279.7951354980469\n",
            "Epoch 112/250, Loss: 281.94854736328125, Test Loss: 279.7633972167969\n",
            "Epoch 113/250, Loss: 282.97283935546875, Test Loss: 279.73236083984375\n",
            "Epoch 114/250, Loss: 289.8470458984375, Test Loss: 279.7016296386719\n",
            "Epoch 115/250, Loss: 284.22021484375, Test Loss: 279.6706237792969\n",
            "Epoch 116/250, Loss: 290.39349365234375, Test Loss: 279.6393127441406\n",
            "Epoch 117/250, Loss: 287.6675720214844, Test Loss: 279.60797119140625\n",
            "Epoch 118/250, Loss: 290.7591247558594, Test Loss: 279.57672119140625\n",
            "Epoch 119/250, Loss: 272.376220703125, Test Loss: 279.5452575683594\n",
            "Epoch 120/250, Loss: 287.8089294433594, Test Loss: 279.513671875\n",
            "Epoch 121/250, Loss: 288.1543273925781, Test Loss: 279.4816589355469\n",
            "Epoch 122/250, Loss: 291.92205810546875, Test Loss: 279.4492492675781\n",
            "Epoch 123/250, Loss: 298.6939392089844, Test Loss: 279.4169921875\n",
            "Epoch 124/250, Loss: 281.2596130371094, Test Loss: 279.3846435546875\n",
            "Epoch 125/250, Loss: 283.88507080078125, Test Loss: 279.3521728515625\n",
            "Epoch 126/250, Loss: 279.0581970214844, Test Loss: 279.3194580078125\n",
            "Epoch 127/250, Loss: 281.0993957519531, Test Loss: 279.2861633300781\n",
            "Epoch 128/250, Loss: 291.779296875, Test Loss: 279.2532043457031\n",
            "Epoch 129/250, Loss: 289.4233703613281, Test Loss: 279.2207336425781\n",
            "Epoch 130/250, Loss: 281.4442138671875, Test Loss: 279.1881408691406\n",
            "Epoch 131/250, Loss: 283.19073486328125, Test Loss: 279.1553649902344\n",
            "Epoch 132/250, Loss: 278.58251953125, Test Loss: 279.12261962890625\n",
            "Epoch 133/250, Loss: 280.20556640625, Test Loss: 279.0905456542969\n",
            "Epoch 134/250, Loss: 275.94403076171875, Test Loss: 279.0586242675781\n",
            "Epoch 135/250, Loss: 277.5152282714844, Test Loss: 279.0266418457031\n",
            "Epoch 136/250, Loss: 283.6793518066406, Test Loss: 278.99432373046875\n",
            "Epoch 137/250, Loss: 291.13226318359375, Test Loss: 278.9615173339844\n",
            "Epoch 138/250, Loss: 284.23773193359375, Test Loss: 278.92840576171875\n",
            "Epoch 139/250, Loss: 279.8553771972656, Test Loss: 278.8952941894531\n",
            "Epoch 140/250, Loss: 281.02899169921875, Test Loss: 278.8621520996094\n",
            "Epoch 141/250, Loss: 280.03582763671875, Test Loss: 278.8292541503906\n",
            "Epoch 142/250, Loss: 285.5693359375, Test Loss: 278.7973327636719\n",
            "Epoch 143/250, Loss: 289.46002197265625, Test Loss: 278.7657775878906\n",
            "Epoch 144/250, Loss: 283.5100402832031, Test Loss: 278.7336730957031\n",
            "Epoch 145/250, Loss: 288.3111877441406, Test Loss: 278.70159912109375\n",
            "Epoch 146/250, Loss: 276.9336853027344, Test Loss: 278.6693420410156\n",
            "Epoch 147/250, Loss: 283.4022216796875, Test Loss: 278.6363830566406\n",
            "Epoch 148/250, Loss: 283.9359436035156, Test Loss: 278.6033020019531\n",
            "Epoch 149/250, Loss: 282.1634826660156, Test Loss: 278.57110595703125\n",
            "Epoch 150/250, Loss: 286.5824890136719, Test Loss: 278.5396423339844\n",
            "Epoch 151/250, Loss: 285.9624938964844, Test Loss: 278.5082092285156\n",
            "Epoch 152/250, Loss: 291.1712341308594, Test Loss: 278.4761047363281\n",
            "Epoch 153/250, Loss: 286.0211486816406, Test Loss: 278.4441223144531\n",
            "Epoch 154/250, Loss: 285.92791748046875, Test Loss: 278.41217041015625\n",
            "Epoch 155/250, Loss: 288.30126953125, Test Loss: 278.3802490234375\n",
            "Epoch 156/250, Loss: 287.8869323730469, Test Loss: 278.3483581542969\n",
            "Epoch 157/250, Loss: 278.1065979003906, Test Loss: 278.316650390625\n",
            "Epoch 158/250, Loss: 278.1957092285156, Test Loss: 278.2850341796875\n",
            "Epoch 159/250, Loss: 279.3646545410156, Test Loss: 278.2529296875\n",
            "Epoch 160/250, Loss: 282.5277404785156, Test Loss: 278.22064208984375\n",
            "Epoch 161/250, Loss: 285.4135437011719, Test Loss: 278.1884765625\n",
            "Epoch 162/250, Loss: 290.6451721191406, Test Loss: 278.1568908691406\n",
            "Epoch 163/250, Loss: 275.1903991699219, Test Loss: 278.12548828125\n",
            "Epoch 164/250, Loss: 286.9407653808594, Test Loss: 278.09381103515625\n",
            "Epoch 165/250, Loss: 281.931640625, Test Loss: 278.06207275390625\n",
            "Epoch 166/250, Loss: 281.9070739746094, Test Loss: 278.0299987792969\n",
            "Epoch 167/250, Loss: 284.39630126953125, Test Loss: 277.9974670410156\n",
            "Epoch 168/250, Loss: 281.6723327636719, Test Loss: 277.9645080566406\n",
            "Epoch 169/250, Loss: 281.8437805175781, Test Loss: 277.93170166015625\n",
            "Epoch 170/250, Loss: 291.5417175292969, Test Loss: 277.89935302734375\n",
            "Epoch 171/250, Loss: 282.6377868652344, Test Loss: 277.8671875\n",
            "Epoch 172/250, Loss: 278.8879089355469, Test Loss: 277.8355407714844\n",
            "Epoch 173/250, Loss: 286.4814147949219, Test Loss: 277.8031921386719\n",
            "Epoch 174/250, Loss: 287.0947265625, Test Loss: 277.7699279785156\n",
            "Epoch 175/250, Loss: 283.6966857910156, Test Loss: 277.7367248535156\n",
            "Epoch 176/250, Loss: 280.51678466796875, Test Loss: 277.70391845703125\n",
            "Epoch 177/250, Loss: 282.7669677734375, Test Loss: 277.6705627441406\n",
            "Epoch 178/250, Loss: 286.8660583496094, Test Loss: 277.6360778808594\n",
            "Epoch 179/250, Loss: 275.67218017578125, Test Loss: 277.6014709472656\n",
            "Epoch 180/250, Loss: 282.3719177246094, Test Loss: 277.5660095214844\n",
            "Epoch 181/250, Loss: 280.46820068359375, Test Loss: 277.5299377441406\n",
            "Epoch 182/250, Loss: 291.2028503417969, Test Loss: 277.49346923828125\n",
            "Epoch 183/250, Loss: 273.8684387207031, Test Loss: 277.45709228515625\n",
            "Epoch 184/250, Loss: 287.93267822265625, Test Loss: 277.42156982421875\n",
            "Epoch 185/250, Loss: 282.11834716796875, Test Loss: 277.38665771484375\n",
            "Epoch 186/250, Loss: 288.7981262207031, Test Loss: 277.3524475097656\n",
            "Epoch 187/250, Loss: 286.25433349609375, Test Loss: 277.3182678222656\n",
            "Epoch 188/250, Loss: 279.3363342285156, Test Loss: 277.2846374511719\n",
            "Epoch 189/250, Loss: 274.39483642578125, Test Loss: 277.2509765625\n",
            "Epoch 190/250, Loss: 283.66058349609375, Test Loss: 277.21722412109375\n",
            "Epoch 191/250, Loss: 289.9669494628906, Test Loss: 277.1830139160156\n",
            "Epoch 192/250, Loss: 285.7238464355469, Test Loss: 277.1482849121094\n",
            "Epoch 193/250, Loss: 276.3662109375, Test Loss: 277.1136169433594\n",
            "Epoch 194/250, Loss: 287.4482727050781, Test Loss: 277.0791015625\n",
            "Epoch 195/250, Loss: 280.23052978515625, Test Loss: 277.0443420410156\n",
            "Epoch 196/250, Loss: 281.06488037109375, Test Loss: 277.0093688964844\n",
            "Epoch 197/250, Loss: 279.4684143066406, Test Loss: 276.9743347167969\n",
            "Epoch 198/250, Loss: 282.9374084472656, Test Loss: 276.9390869140625\n",
            "Epoch 199/250, Loss: 287.8429870605469, Test Loss: 276.903076171875\n",
            "Epoch 200/250, Loss: 279.8283996582031, Test Loss: 276.86614990234375\n",
            "Epoch 201/250, Loss: 273.8233337402344, Test Loss: 276.82861328125\n",
            "Epoch 202/250, Loss: 281.23065185546875, Test Loss: 276.7911682128906\n",
            "Epoch 203/250, Loss: 282.94439697265625, Test Loss: 276.75299072265625\n",
            "Epoch 204/250, Loss: 276.92681884765625, Test Loss: 276.7143859863281\n",
            "Epoch 205/250, Loss: 285.2202453613281, Test Loss: 276.6759948730469\n",
            "Epoch 206/250, Loss: 285.16876220703125, Test Loss: 276.63726806640625\n",
            "Epoch 207/250, Loss: 283.3966064453125, Test Loss: 276.5985412597656\n",
            "Epoch 208/250, Loss: 283.8406677246094, Test Loss: 276.56011962890625\n",
            "Epoch 209/250, Loss: 282.8311767578125, Test Loss: 276.5219421386719\n",
            "Epoch 210/250, Loss: 291.6933898925781, Test Loss: 276.484619140625\n",
            "Epoch 211/250, Loss: 280.8767395019531, Test Loss: 276.4469299316406\n",
            "Epoch 212/250, Loss: 276.1638488769531, Test Loss: 276.4090576171875\n",
            "Epoch 213/250, Loss: 289.99163818359375, Test Loss: 276.3697814941406\n",
            "Epoch 214/250, Loss: 269.24456787109375, Test Loss: 276.33026123046875\n",
            "Epoch 215/250, Loss: 282.88385009765625, Test Loss: 276.29083251953125\n",
            "Epoch 216/250, Loss: 278.06951904296875, Test Loss: 276.2513122558594\n",
            "Epoch 217/250, Loss: 286.0921936035156, Test Loss: 276.21148681640625\n",
            "Epoch 218/250, Loss: 284.0442199707031, Test Loss: 276.1716003417969\n",
            "Epoch 219/250, Loss: 281.6868896484375, Test Loss: 276.1318359375\n",
            "Epoch 220/250, Loss: 277.8262023925781, Test Loss: 276.091796875\n",
            "Epoch 221/250, Loss: 280.3052062988281, Test Loss: 276.0523376464844\n",
            "Epoch 222/250, Loss: 278.1885681152344, Test Loss: 276.0133361816406\n",
            "Epoch 223/250, Loss: 278.5595703125, Test Loss: 275.9741516113281\n",
            "Epoch 224/250, Loss: 279.43597412109375, Test Loss: 275.93548583984375\n",
            "Epoch 225/250, Loss: 283.37176513671875, Test Loss: 275.8970642089844\n",
            "Epoch 226/250, Loss: 276.771484375, Test Loss: 275.85888671875\n",
            "Epoch 227/250, Loss: 285.47540283203125, Test Loss: 275.8214111328125\n",
            "Epoch 228/250, Loss: 277.6944274902344, Test Loss: 275.7836608886719\n",
            "Epoch 229/250, Loss: 279.9591979980469, Test Loss: 275.7453918457031\n",
            "Epoch 230/250, Loss: 280.3500671386719, Test Loss: 275.7076416015625\n",
            "Epoch 231/250, Loss: 271.6748352050781, Test Loss: 275.67010498046875\n",
            "Epoch 232/250, Loss: 274.6170959472656, Test Loss: 275.6337890625\n",
            "Epoch 233/250, Loss: 284.9463806152344, Test Loss: 275.5982360839844\n",
            "Epoch 234/250, Loss: 282.6249694824219, Test Loss: 275.56304931640625\n",
            "Epoch 235/250, Loss: 284.1058349609375, Test Loss: 275.52825927734375\n",
            "Epoch 236/250, Loss: 285.9930725097656, Test Loss: 275.4940490722656\n",
            "Epoch 237/250, Loss: 279.74017333984375, Test Loss: 275.4585266113281\n",
            "Epoch 238/250, Loss: 270.6779479980469, Test Loss: 275.4208984375\n",
            "Epoch 239/250, Loss: 283.4819641113281, Test Loss: 275.3822326660156\n",
            "Epoch 240/250, Loss: 288.94573974609375, Test Loss: 275.3426513671875\n",
            "Epoch 241/250, Loss: 274.5873718261719, Test Loss: 275.3019104003906\n",
            "Epoch 242/250, Loss: 276.8705749511719, Test Loss: 275.2604064941406\n",
            "Epoch 243/250, Loss: 280.3402099609375, Test Loss: 275.2196044921875\n",
            "Epoch 244/250, Loss: 277.1834411621094, Test Loss: 275.1795959472656\n",
            "Epoch 245/250, Loss: 279.0382385253906, Test Loss: 275.1405334472656\n",
            "Epoch 246/250, Loss: 278.08392333984375, Test Loss: 275.1009826660156\n",
            "Epoch 247/250, Loss: 283.1700439453125, Test Loss: 275.06072998046875\n",
            "Epoch 248/250, Loss: 282.8625793457031, Test Loss: 275.02093505859375\n",
            "Epoch 249/250, Loss: 284.2489013671875, Test Loss: 274.9817199707031\n",
            "Epoch 250/250, Loss: 275.57916259765625, Test Loss: 274.9422912597656\n",
            "Final MSE: 274.9422607421875\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=512, epochs=1\n",
            "Epoch 1/1, Loss: 282.3912658691406, Test Loss: 274.1238098144531\n",
            "Final MSE: 274.1237487792969\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=512, epochs=10\n",
            "Epoch 1/10, Loss: 301.2248229980469, Test Loss: 293.0992736816406\n",
            "Epoch 2/10, Loss: 301.1946105957031, Test Loss: 293.0841979980469\n",
            "Epoch 3/10, Loss: 301.1643981933594, Test Loss: 293.069091796875\n",
            "Epoch 4/10, Loss: 301.1341857910156, Test Loss: 293.0540466308594\n",
            "Epoch 5/10, Loss: 301.1039733886719, Test Loss: 293.0389709472656\n",
            "Epoch 6/10, Loss: 301.07379150390625, Test Loss: 293.0238952636719\n",
            "Epoch 7/10, Loss: 301.0436096191406, Test Loss: 293.0088806152344\n",
            "Epoch 8/10, Loss: 301.0134582519531, Test Loss: 292.9938049316406\n",
            "Epoch 9/10, Loss: 300.9832763671875, Test Loss: 292.9787292480469\n",
            "Epoch 10/10, Loss: 300.953125, Test Loss: 292.9637451171875\n",
            "Final MSE: 292.9637145996094\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=512, epochs=25\n",
            "Epoch 1/25, Loss: 269.5766906738281, Test Loss: 261.98004150390625\n",
            "Epoch 2/25, Loss: 269.5559387207031, Test Loss: 261.96636962890625\n",
            "Epoch 3/25, Loss: 269.53515625, Test Loss: 261.9527587890625\n",
            "Epoch 4/25, Loss: 269.514404296875, Test Loss: 261.93914794921875\n",
            "Epoch 5/25, Loss: 269.4936828613281, Test Loss: 261.92547607421875\n",
            "Epoch 6/25, Loss: 269.472900390625, Test Loss: 261.9118347167969\n",
            "Epoch 7/25, Loss: 269.4521484375, Test Loss: 261.898193359375\n",
            "Epoch 8/25, Loss: 269.4314270019531, Test Loss: 261.8845520019531\n",
            "Epoch 9/25, Loss: 269.4106750488281, Test Loss: 261.8708801269531\n",
            "Epoch 10/25, Loss: 269.3899230957031, Test Loss: 261.85723876953125\n",
            "Epoch 11/25, Loss: 269.3692321777344, Test Loss: 261.84356689453125\n",
            "Epoch 12/25, Loss: 269.3485412597656, Test Loss: 261.82989501953125\n",
            "Epoch 13/25, Loss: 269.32781982421875, Test Loss: 261.8162536621094\n",
            "Epoch 14/25, Loss: 269.3070983886719, Test Loss: 261.8025817871094\n",
            "Epoch 15/25, Loss: 269.28643798828125, Test Loss: 261.78887939453125\n",
            "Epoch 16/25, Loss: 269.2657775878906, Test Loss: 261.7752685546875\n",
            "Epoch 17/25, Loss: 269.24505615234375, Test Loss: 261.7616271972656\n",
            "Epoch 18/25, Loss: 269.2243957519531, Test Loss: 261.74798583984375\n",
            "Epoch 19/25, Loss: 269.2037048339844, Test Loss: 261.7342834472656\n",
            "Epoch 20/25, Loss: 269.1830749511719, Test Loss: 261.7206115722656\n",
            "Epoch 21/25, Loss: 269.16241455078125, Test Loss: 261.70697021484375\n",
            "Epoch 22/25, Loss: 269.1417541503906, Test Loss: 261.6933288574219\n",
            "Epoch 23/25, Loss: 269.1211242675781, Test Loss: 261.6796875\n",
            "Epoch 24/25, Loss: 269.1004638671875, Test Loss: 261.66607666015625\n",
            "Epoch 25/25, Loss: 269.079833984375, Test Loss: 261.65240478515625\n",
            "Final MSE: 261.65240478515625\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=512, epochs=50\n",
            "Epoch 1/50, Loss: 265.0400390625, Test Loss: 257.21453857421875\n",
            "Epoch 2/50, Loss: 265.013916015625, Test Loss: 257.1996154785156\n",
            "Epoch 3/50, Loss: 264.98779296875, Test Loss: 257.1846923828125\n",
            "Epoch 4/50, Loss: 264.9617004394531, Test Loss: 257.1697692871094\n",
            "Epoch 5/50, Loss: 264.93560791015625, Test Loss: 257.1549377441406\n",
            "Epoch 6/50, Loss: 264.9095764160156, Test Loss: 257.1401062011719\n",
            "Epoch 7/50, Loss: 264.8834533691406, Test Loss: 257.1253356933594\n",
            "Epoch 8/50, Loss: 264.8574523925781, Test Loss: 257.1105651855469\n",
            "Epoch 9/50, Loss: 264.8313903808594, Test Loss: 257.0958251953125\n",
            "Epoch 10/50, Loss: 264.8053894042969, Test Loss: 257.0810546875\n",
            "Epoch 11/50, Loss: 264.77935791015625, Test Loss: 257.06634521484375\n",
            "Epoch 12/50, Loss: 264.75335693359375, Test Loss: 257.0516052246094\n",
            "Epoch 13/50, Loss: 264.7273864746094, Test Loss: 257.0368347167969\n",
            "Epoch 14/50, Loss: 264.7013854980469, Test Loss: 257.02203369140625\n",
            "Epoch 15/50, Loss: 264.6754455566406, Test Loss: 257.0072326660156\n",
            "Epoch 16/50, Loss: 264.64947509765625, Test Loss: 256.9924011230469\n",
            "Epoch 17/50, Loss: 264.6235656738281, Test Loss: 256.9775390625\n",
            "Epoch 18/50, Loss: 264.5976257324219, Test Loss: 256.962646484375\n",
            "Epoch 19/50, Loss: 264.5717468261719, Test Loss: 256.9477233886719\n",
            "Epoch 20/50, Loss: 264.54583740234375, Test Loss: 256.9327697753906\n",
            "Epoch 21/50, Loss: 264.51995849609375, Test Loss: 256.91778564453125\n",
            "Epoch 22/50, Loss: 264.4941101074219, Test Loss: 256.90283203125\n",
            "Epoch 23/50, Loss: 264.46826171875, Test Loss: 256.8878173828125\n",
            "Epoch 24/50, Loss: 264.4424133300781, Test Loss: 256.8728332519531\n",
            "Epoch 25/50, Loss: 264.4165954589844, Test Loss: 256.8577880859375\n",
            "Epoch 26/50, Loss: 264.39080810546875, Test Loss: 256.8427734375\n",
            "Epoch 27/50, Loss: 264.364990234375, Test Loss: 256.8277587890625\n",
            "Epoch 28/50, Loss: 264.33917236328125, Test Loss: 256.8127136230469\n",
            "Epoch 29/50, Loss: 264.31341552734375, Test Loss: 256.7977294921875\n",
            "Epoch 30/50, Loss: 264.2876281738281, Test Loss: 256.7826843261719\n",
            "Epoch 31/50, Loss: 264.26190185546875, Test Loss: 256.76763916015625\n",
            "Epoch 32/50, Loss: 264.2361755371094, Test Loss: 256.75262451171875\n",
            "Epoch 33/50, Loss: 264.21044921875, Test Loss: 256.7375793457031\n",
            "Epoch 34/50, Loss: 264.1847229003906, Test Loss: 256.7225341796875\n",
            "Epoch 35/50, Loss: 264.1590576171875, Test Loss: 256.7074890136719\n",
            "Epoch 36/50, Loss: 264.1333312988281, Test Loss: 256.6923828125\n",
            "Epoch 37/50, Loss: 264.1076354980469, Test Loss: 256.67730712890625\n",
            "Epoch 38/50, Loss: 264.08197021484375, Test Loss: 256.66217041015625\n",
            "Epoch 39/50, Loss: 264.0563049316406, Test Loss: 256.6470642089844\n",
            "Epoch 40/50, Loss: 264.03070068359375, Test Loss: 256.6319274902344\n",
            "Epoch 41/50, Loss: 264.0050354003906, Test Loss: 256.61676025390625\n",
            "Epoch 42/50, Loss: 263.97943115234375, Test Loss: 256.60162353515625\n",
            "Epoch 43/50, Loss: 263.9538269042969, Test Loss: 256.5863952636719\n",
            "Epoch 44/50, Loss: 263.92822265625, Test Loss: 256.57122802734375\n",
            "Epoch 45/50, Loss: 263.9026184082031, Test Loss: 256.5560302734375\n",
            "Epoch 46/50, Loss: 263.8770446777344, Test Loss: 256.5408020019531\n",
            "Epoch 47/50, Loss: 263.85150146484375, Test Loss: 256.52557373046875\n",
            "Epoch 48/50, Loss: 263.8258972167969, Test Loss: 256.5104064941406\n",
            "Epoch 49/50, Loss: 263.8003845214844, Test Loss: 256.4951477050781\n",
            "Epoch 50/50, Loss: 263.7748107910156, Test Loss: 256.47991943359375\n",
            "Final MSE: 256.4799499511719\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=512, epochs=100\n",
            "Epoch 1/100, Loss: 272.62896728515625, Test Loss: 265.28338623046875\n",
            "Epoch 2/100, Loss: 272.6095275878906, Test Loss: 265.2758483886719\n",
            "Epoch 3/100, Loss: 272.5900573730469, Test Loss: 265.2682189941406\n",
            "Epoch 4/100, Loss: 272.5706481933594, Test Loss: 265.26055908203125\n",
            "Epoch 5/100, Loss: 272.5512390136719, Test Loss: 265.2528076171875\n",
            "Epoch 6/100, Loss: 272.53179931640625, Test Loss: 265.2450256347656\n",
            "Epoch 7/100, Loss: 272.5123291015625, Test Loss: 265.2372741699219\n",
            "Epoch 8/100, Loss: 272.4928894042969, Test Loss: 265.2294921875\n",
            "Epoch 9/100, Loss: 272.47344970703125, Test Loss: 265.2217102050781\n",
            "Epoch 10/100, Loss: 272.4539794921875, Test Loss: 265.21392822265625\n",
            "Epoch 11/100, Loss: 272.43450927734375, Test Loss: 265.2061462402344\n",
            "Epoch 12/100, Loss: 272.4150390625, Test Loss: 265.1983642578125\n",
            "Epoch 13/100, Loss: 272.3955383300781, Test Loss: 265.19061279296875\n",
            "Epoch 14/100, Loss: 272.3760986328125, Test Loss: 265.1828918457031\n",
            "Epoch 15/100, Loss: 272.3565979003906, Test Loss: 265.1752014160156\n",
            "Epoch 16/100, Loss: 272.3370666503906, Test Loss: 265.1675109863281\n",
            "Epoch 17/100, Loss: 272.3175964355469, Test Loss: 265.15985107421875\n",
            "Epoch 18/100, Loss: 272.298095703125, Test Loss: 265.1521911621094\n",
            "Epoch 19/100, Loss: 272.2785949707031, Test Loss: 265.1445617675781\n",
            "Epoch 20/100, Loss: 272.25909423828125, Test Loss: 265.1369323730469\n",
            "Epoch 21/100, Loss: 272.2395324707031, Test Loss: 265.1293640136719\n",
            "Epoch 22/100, Loss: 272.2200012207031, Test Loss: 265.1217956542969\n",
            "Epoch 23/100, Loss: 272.2004699707031, Test Loss: 265.11419677734375\n",
            "Epoch 24/100, Loss: 272.1809387207031, Test Loss: 265.1066589355469\n",
            "Epoch 25/100, Loss: 272.1614074707031, Test Loss: 265.0990905761719\n",
            "Epoch 26/100, Loss: 272.1418151855469, Test Loss: 265.0915222167969\n",
            "Epoch 27/100, Loss: 272.12225341796875, Test Loss: 265.0840148925781\n",
            "Epoch 28/100, Loss: 272.1026611328125, Test Loss: 265.076416015625\n",
            "Epoch 29/100, Loss: 272.0830993652344, Test Loss: 265.06890869140625\n",
            "Epoch 30/100, Loss: 272.0634765625, Test Loss: 265.0613708496094\n",
            "Epoch 31/100, Loss: 272.04388427734375, Test Loss: 265.0538330078125\n",
            "Epoch 32/100, Loss: 272.0242919921875, Test Loss: 265.0462951660156\n",
            "Epoch 33/100, Loss: 272.004638671875, Test Loss: 265.0387878417969\n",
            "Epoch 34/100, Loss: 271.9850158691406, Test Loss: 265.0312805175781\n",
            "Epoch 35/100, Loss: 271.96539306640625, Test Loss: 265.0237731933594\n",
            "Epoch 36/100, Loss: 271.94573974609375, Test Loss: 265.0163269042969\n",
            "Epoch 37/100, Loss: 271.92608642578125, Test Loss: 265.0087890625\n",
            "Epoch 38/100, Loss: 271.9064025878906, Test Loss: 265.0013732910156\n",
            "Epoch 39/100, Loss: 271.88671875, Test Loss: 264.99395751953125\n",
            "Epoch 40/100, Loss: 271.8670349121094, Test Loss: 264.98651123046875\n",
            "Epoch 41/100, Loss: 271.8473205566406, Test Loss: 264.9790954589844\n",
            "Epoch 42/100, Loss: 271.82757568359375, Test Loss: 264.9717102050781\n",
            "Epoch 43/100, Loss: 271.807861328125, Test Loss: 264.9643249511719\n",
            "Epoch 44/100, Loss: 271.7880859375, Test Loss: 264.95697021484375\n",
            "Epoch 45/100, Loss: 271.7683410644531, Test Loss: 264.9496154785156\n",
            "Epoch 46/100, Loss: 271.7485656738281, Test Loss: 264.9422607421875\n",
            "Epoch 47/100, Loss: 271.7287902832031, Test Loss: 264.9349060058594\n",
            "Epoch 48/100, Loss: 271.7089538574219, Test Loss: 264.92755126953125\n",
            "Epoch 49/100, Loss: 271.6891784667969, Test Loss: 264.92022705078125\n",
            "Epoch 50/100, Loss: 271.6693420410156, Test Loss: 264.91290283203125\n",
            "Epoch 51/100, Loss: 271.6495056152344, Test Loss: 264.9056091308594\n",
            "Epoch 52/100, Loss: 271.629638671875, Test Loss: 264.8982849121094\n",
            "Epoch 53/100, Loss: 271.6097717285156, Test Loss: 264.8909912109375\n",
            "Epoch 54/100, Loss: 271.5898742675781, Test Loss: 264.88372802734375\n",
            "Epoch 55/100, Loss: 271.5699768066406, Test Loss: 264.8764953613281\n",
            "Epoch 56/100, Loss: 271.5500793457031, Test Loss: 264.86920166015625\n",
            "Epoch 57/100, Loss: 271.5301208496094, Test Loss: 264.8619689941406\n",
            "Epoch 58/100, Loss: 271.5101623535156, Test Loss: 264.8547668457031\n",
            "Epoch 59/100, Loss: 271.49017333984375, Test Loss: 264.8475341796875\n",
            "Epoch 60/100, Loss: 271.47021484375, Test Loss: 264.84033203125\n",
            "Epoch 61/100, Loss: 271.4501953125, Test Loss: 264.8331604003906\n",
            "Epoch 62/100, Loss: 271.43017578125, Test Loss: 264.8260192871094\n",
            "Epoch 63/100, Loss: 271.41009521484375, Test Loss: 264.81884765625\n",
            "Epoch 64/100, Loss: 271.3900451660156, Test Loss: 264.8117370605469\n",
            "Epoch 65/100, Loss: 271.3699645996094, Test Loss: 264.8045959472656\n",
            "Epoch 66/100, Loss: 271.3498840332031, Test Loss: 264.7974853515625\n",
            "Epoch 67/100, Loss: 271.3297119140625, Test Loss: 264.7903747558594\n",
            "Epoch 68/100, Loss: 271.3095703125, Test Loss: 264.78326416015625\n",
            "Epoch 69/100, Loss: 271.2893981933594, Test Loss: 264.7762451171875\n",
            "Epoch 70/100, Loss: 271.26922607421875, Test Loss: 264.7691955566406\n",
            "Epoch 71/100, Loss: 271.2490234375, Test Loss: 264.7621765136719\n",
            "Epoch 72/100, Loss: 271.228759765625, Test Loss: 264.7551574707031\n",
            "Epoch 73/100, Loss: 271.20849609375, Test Loss: 264.7481384277344\n",
            "Epoch 74/100, Loss: 271.188232421875, Test Loss: 264.74114990234375\n",
            "Epoch 75/100, Loss: 271.16790771484375, Test Loss: 264.7342224121094\n",
            "Epoch 76/100, Loss: 271.1475524902344, Test Loss: 264.727294921875\n",
            "Epoch 77/100, Loss: 271.1272277832031, Test Loss: 264.7203674316406\n",
            "Epoch 78/100, Loss: 271.1068420410156, Test Loss: 264.7135009765625\n",
            "Epoch 79/100, Loss: 271.08642578125, Test Loss: 264.70660400390625\n",
            "Epoch 80/100, Loss: 271.0660095214844, Test Loss: 264.6997985839844\n",
            "Epoch 81/100, Loss: 271.0455627441406, Test Loss: 264.69293212890625\n",
            "Epoch 82/100, Loss: 271.0250549316406, Test Loss: 264.6861572265625\n",
            "Epoch 83/100, Loss: 271.0045166015625, Test Loss: 264.67938232421875\n",
            "Epoch 84/100, Loss: 270.9840087890625, Test Loss: 264.672607421875\n",
            "Epoch 85/100, Loss: 270.96343994140625, Test Loss: 264.6659240722656\n",
            "Epoch 86/100, Loss: 270.9428405761719, Test Loss: 264.6592102050781\n",
            "Epoch 87/100, Loss: 270.9222106933594, Test Loss: 264.65252685546875\n",
            "Epoch 88/100, Loss: 270.90155029296875, Test Loss: 264.64593505859375\n",
            "Epoch 89/100, Loss: 270.880859375, Test Loss: 264.6393127441406\n",
            "Epoch 90/100, Loss: 270.86016845703125, Test Loss: 264.63275146484375\n",
            "Epoch 91/100, Loss: 270.8394470214844, Test Loss: 264.626220703125\n",
            "Epoch 92/100, Loss: 270.8186340332031, Test Loss: 264.6197204589844\n",
            "Epoch 93/100, Loss: 270.7978515625, Test Loss: 264.61322021484375\n",
            "Epoch 94/100, Loss: 270.7770080566406, Test Loss: 264.60675048828125\n",
            "Epoch 95/100, Loss: 270.7561340332031, Test Loss: 264.600341796875\n",
            "Epoch 96/100, Loss: 270.7352294921875, Test Loss: 264.59393310546875\n",
            "Epoch 97/100, Loss: 270.71429443359375, Test Loss: 264.5876770019531\n",
            "Epoch 98/100, Loss: 270.6933288574219, Test Loss: 264.5813293457031\n",
            "Epoch 99/100, Loss: 270.6723327636719, Test Loss: 264.5750427246094\n",
            "Epoch 100/100, Loss: 270.65130615234375, Test Loss: 264.56878662109375\n",
            "Final MSE: 264.56878662109375\n",
            "Training with [16, 8, 4], linear, lr=0.0001, batch_size=512, epochs=250\n",
            "Epoch 1/250, Loss: 263.73736572265625, Test Loss: 255.7542724609375\n",
            "Epoch 2/250, Loss: 263.7201843261719, Test Loss: 255.74441528320312\n",
            "Epoch 3/250, Loss: 263.7030029296875, Test Loss: 255.7345733642578\n",
            "Epoch 4/250, Loss: 263.6858825683594, Test Loss: 255.72470092773438\n",
            "Epoch 5/250, Loss: 263.6686706542969, Test Loss: 255.71484375\n",
            "Epoch 6/250, Loss: 263.6514892578125, Test Loss: 255.70498657226562\n",
            "Epoch 7/250, Loss: 263.6343688964844, Test Loss: 255.6951446533203\n",
            "Epoch 8/250, Loss: 263.61724853515625, Test Loss: 255.68527221679688\n",
            "Epoch 9/250, Loss: 263.6000671386719, Test Loss: 255.67543029785156\n",
            "Epoch 10/250, Loss: 263.5829162597656, Test Loss: 255.66554260253906\n",
            "Epoch 11/250, Loss: 263.5657958984375, Test Loss: 255.6556854248047\n",
            "Epoch 12/250, Loss: 263.54864501953125, Test Loss: 255.64581298828125\n",
            "Epoch 13/250, Loss: 263.53155517578125, Test Loss: 255.63597106933594\n",
            "Epoch 14/250, Loss: 263.5143737792969, Test Loss: 255.62606811523438\n",
            "Epoch 15/250, Loss: 263.49725341796875, Test Loss: 255.61618041992188\n",
            "Epoch 16/250, Loss: 263.48016357421875, Test Loss: 255.60630798339844\n",
            "Epoch 17/250, Loss: 263.4630432128906, Test Loss: 255.5963897705078\n",
            "Epoch 18/250, Loss: 263.4458923339844, Test Loss: 255.58653259277344\n",
            "Epoch 19/250, Loss: 263.4288024902344, Test Loss: 255.5766143798828\n",
            "Epoch 20/250, Loss: 263.41168212890625, Test Loss: 255.5667266845703\n",
            "Epoch 21/250, Loss: 263.39459228515625, Test Loss: 255.55679321289062\n",
            "Epoch 22/250, Loss: 263.37744140625, Test Loss: 255.54690551757812\n",
            "Epoch 23/250, Loss: 263.3603515625, Test Loss: 255.5369415283203\n",
            "Epoch 24/250, Loss: 263.34326171875, Test Loss: 255.5270233154297\n",
            "Epoch 25/250, Loss: 263.32611083984375, Test Loss: 255.51710510253906\n",
            "Epoch 26/250, Loss: 263.30902099609375, Test Loss: 255.50717163085938\n",
            "Epoch 27/250, Loss: 263.29193115234375, Test Loss: 255.49717712402344\n",
            "Epoch 28/250, Loss: 263.2748107910156, Test Loss: 255.48721313476562\n",
            "Epoch 29/250, Loss: 263.2576904296875, Test Loss: 255.47723388671875\n",
            "Epoch 30/250, Loss: 263.2405700683594, Test Loss: 255.46726989746094\n",
            "Epoch 31/250, Loss: 263.22344970703125, Test Loss: 255.457275390625\n",
            "Epoch 32/250, Loss: 263.2063293457031, Test Loss: 255.447265625\n",
            "Epoch 33/250, Loss: 263.1891784667969, Test Loss: 255.437255859375\n",
            "Epoch 34/250, Loss: 263.1720275878906, Test Loss: 255.42724609375\n",
            "Epoch 35/250, Loss: 263.1549072265625, Test Loss: 255.41720581054688\n",
            "Epoch 36/250, Loss: 263.1377258300781, Test Loss: 255.40716552734375\n",
            "Epoch 37/250, Loss: 263.12054443359375, Test Loss: 255.39715576171875\n",
            "Epoch 38/250, Loss: 263.1033935546875, Test Loss: 255.3870849609375\n",
            "Epoch 39/250, Loss: 263.086181640625, Test Loss: 255.3770294189453\n",
            "Epoch 40/250, Loss: 263.06903076171875, Test Loss: 255.3669891357422\n",
            "Epoch 41/250, Loss: 263.05181884765625, Test Loss: 255.35690307617188\n",
            "Epoch 42/250, Loss: 263.0345764160156, Test Loss: 255.34681701660156\n",
            "Epoch 43/250, Loss: 263.017333984375, Test Loss: 255.3367462158203\n",
            "Epoch 44/250, Loss: 263.0000915527344, Test Loss: 255.32664489746094\n",
            "Epoch 45/250, Loss: 262.98284912109375, Test Loss: 255.3165740966797\n",
            "Epoch 46/250, Loss: 262.9655456542969, Test Loss: 255.30648803710938\n",
            "Epoch 47/250, Loss: 262.9482727050781, Test Loss: 255.29638671875\n",
            "Epoch 48/250, Loss: 262.9309997558594, Test Loss: 255.28627014160156\n",
            "Epoch 49/250, Loss: 262.9136962890625, Test Loss: 255.2761993408203\n",
            "Epoch 50/250, Loss: 262.8963317871094, Test Loss: 255.26609802246094\n",
            "Epoch 51/250, Loss: 262.87896728515625, Test Loss: 255.25595092773438\n",
            "Epoch 52/250, Loss: 262.8616638183594, Test Loss: 255.245849609375\n",
            "Epoch 53/250, Loss: 262.8442687988281, Test Loss: 255.2357635498047\n",
            "Epoch 54/250, Loss: 262.8268737792969, Test Loss: 255.22564697265625\n",
            "Epoch 55/250, Loss: 262.8094787597656, Test Loss: 255.21551513671875\n",
            "Epoch 56/250, Loss: 262.79205322265625, Test Loss: 255.2053985595703\n",
            "Epoch 57/250, Loss: 262.77459716796875, Test Loss: 255.19528198242188\n",
            "Epoch 58/250, Loss: 262.75714111328125, Test Loss: 255.1851806640625\n",
            "Epoch 59/250, Loss: 262.7396545410156, Test Loss: 255.17506408691406\n",
            "Epoch 60/250, Loss: 262.7221984863281, Test Loss: 255.16493225097656\n",
            "Epoch 61/250, Loss: 262.7046813964844, Test Loss: 255.15484619140625\n",
            "Epoch 62/250, Loss: 262.6871337890625, Test Loss: 255.1446990966797\n",
            "Epoch 63/250, Loss: 262.6695861816406, Test Loss: 255.1345977783203\n",
            "Epoch 64/250, Loss: 262.65203857421875, Test Loss: 255.12445068359375\n",
            "Epoch 65/250, Loss: 262.63446044921875, Test Loss: 255.11431884765625\n",
            "Epoch 66/250, Loss: 262.6168518066406, Test Loss: 255.1042022705078\n",
            "Epoch 67/250, Loss: 262.5992431640625, Test Loss: 255.0940704345703\n",
            "Epoch 68/250, Loss: 262.5815734863281, Test Loss: 255.08395385742188\n",
            "Epoch 69/250, Loss: 262.5639343261719, Test Loss: 255.07382202148438\n",
            "Epoch 70/250, Loss: 262.5462951660156, Test Loss: 255.06370544433594\n",
            "Epoch 71/250, Loss: 262.5285949707031, Test Loss: 255.05360412597656\n",
            "Epoch 72/250, Loss: 262.5108642578125, Test Loss: 255.04345703125\n",
            "Epoch 73/250, Loss: 262.4931335449219, Test Loss: 255.03335571289062\n",
            "Epoch 74/250, Loss: 262.475341796875, Test Loss: 255.0232391357422\n",
            "Epoch 75/250, Loss: 262.4576110839844, Test Loss: 255.0131072998047\n",
            "Epoch 76/250, Loss: 262.4397888183594, Test Loss: 255.00299072265625\n",
            "Epoch 77/250, Loss: 262.4219970703125, Test Loss: 254.99285888671875\n",
            "Epoch 78/250, Loss: 262.4041748046875, Test Loss: 254.9827423095703\n",
            "Epoch 79/250, Loss: 262.3862609863281, Test Loss: 254.97262573242188\n",
            "Epoch 80/250, Loss: 262.3684387207031, Test Loss: 254.9625244140625\n",
            "Epoch 81/250, Loss: 262.35052490234375, Test Loss: 254.9524383544922\n",
            "Epoch 82/250, Loss: 262.3326110839844, Test Loss: 254.94229125976562\n",
            "Epoch 83/250, Loss: 262.31463623046875, Test Loss: 254.9322052001953\n",
            "Epoch 84/250, Loss: 262.29669189453125, Test Loss: 254.9220733642578\n",
            "Epoch 85/250, Loss: 262.2787170410156, Test Loss: 254.9119415283203\n",
            "Epoch 86/250, Loss: 262.2607116699219, Test Loss: 254.90182495117188\n",
            "Epoch 87/250, Loss: 262.24267578125, Test Loss: 254.8917694091797\n",
            "Epoch 88/250, Loss: 262.224609375, Test Loss: 254.8816680908203\n",
            "Epoch 89/250, Loss: 262.2065734863281, Test Loss: 254.8715362548828\n",
            "Epoch 90/250, Loss: 262.1884460449219, Test Loss: 254.86141967773438\n",
            "Epoch 91/250, Loss: 262.1703186035156, Test Loss: 254.851318359375\n",
            "Epoch 92/250, Loss: 262.1521911621094, Test Loss: 254.8412322998047\n",
            "Epoch 93/250, Loss: 262.1340637207031, Test Loss: 254.83114624023438\n",
            "Epoch 94/250, Loss: 262.1158142089844, Test Loss: 254.82102966308594\n",
            "Epoch 95/250, Loss: 262.09759521484375, Test Loss: 254.81097412109375\n",
            "Epoch 96/250, Loss: 262.0793762207031, Test Loss: 254.8008270263672\n",
            "Epoch 97/250, Loss: 262.0611267089844, Test Loss: 254.79074096679688\n",
            "Epoch 98/250, Loss: 262.0428466796875, Test Loss: 254.78065490722656\n",
            "Epoch 99/250, Loss: 262.0245361328125, Test Loss: 254.77056884765625\n",
            "Epoch 100/250, Loss: 262.0061950683594, Test Loss: 254.76051330566406\n",
            "Epoch 101/250, Loss: 261.9878234863281, Test Loss: 254.75042724609375\n",
            "Epoch 102/250, Loss: 261.9694519042969, Test Loss: 254.7403106689453\n",
            "Epoch 103/250, Loss: 261.9510498046875, Test Loss: 254.7302703857422\n",
            "Epoch 104/250, Loss: 261.9326171875, Test Loss: 254.72015380859375\n",
            "Epoch 105/250, Loss: 261.91412353515625, Test Loss: 254.7101287841797\n",
            "Epoch 106/250, Loss: 261.8956604003906, Test Loss: 254.70005798339844\n",
            "Epoch 107/250, Loss: 261.87713623046875, Test Loss: 254.69000244140625\n",
            "Epoch 108/250, Loss: 261.85858154296875, Test Loss: 254.679931640625\n",
            "Epoch 109/250, Loss: 261.84002685546875, Test Loss: 254.6698760986328\n",
            "Epoch 110/250, Loss: 261.8214416503906, Test Loss: 254.65980529785156\n",
            "Epoch 111/250, Loss: 261.80279541015625, Test Loss: 254.64974975585938\n",
            "Epoch 112/250, Loss: 261.7841796875, Test Loss: 254.63970947265625\n",
            "Epoch 113/250, Loss: 261.7654724121094, Test Loss: 254.62966918945312\n",
            "Epoch 114/250, Loss: 261.7467956542969, Test Loss: 254.61965942382812\n",
            "Epoch 115/250, Loss: 261.7280578613281, Test Loss: 254.609619140625\n",
            "Epoch 116/250, Loss: 261.7093200683594, Test Loss: 254.59957885742188\n",
            "Epoch 117/250, Loss: 261.6905212402344, Test Loss: 254.5895538330078\n",
            "Epoch 118/250, Loss: 261.67169189453125, Test Loss: 254.5795135498047\n",
            "Epoch 119/250, Loss: 261.65283203125, Test Loss: 254.5695037841797\n",
            "Epoch 120/250, Loss: 261.6340026855469, Test Loss: 254.55947875976562\n",
            "Epoch 121/250, Loss: 261.6150817871094, Test Loss: 254.54949951171875\n",
            "Epoch 122/250, Loss: 261.5961608886719, Test Loss: 254.53948974609375\n",
            "Epoch 123/250, Loss: 261.5771789550781, Test Loss: 254.5294647216797\n",
            "Epoch 124/250, Loss: 261.5581970214844, Test Loss: 254.51950073242188\n",
            "Epoch 125/250, Loss: 261.5391540527344, Test Loss: 254.50949096679688\n",
            "Epoch 126/250, Loss: 261.5201110839844, Test Loss: 254.49948120117188\n",
            "Epoch 127/250, Loss: 261.50103759765625, Test Loss: 254.48956298828125\n",
            "Epoch 128/250, Loss: 261.48193359375, Test Loss: 254.47958374023438\n",
            "Epoch 129/250, Loss: 261.4627685546875, Test Loss: 254.46958923339844\n",
            "Epoch 130/250, Loss: 261.4436340332031, Test Loss: 254.45962524414062\n",
            "Epoch 131/250, Loss: 261.4244079589844, Test Loss: 254.44969177246094\n",
            "Epoch 132/250, Loss: 261.4051818847656, Test Loss: 254.4397430419922\n",
            "Epoch 133/250, Loss: 261.38592529296875, Test Loss: 254.42984008789062\n",
            "Epoch 134/250, Loss: 261.3666076660156, Test Loss: 254.4198760986328\n",
            "Epoch 135/250, Loss: 261.3473205566406, Test Loss: 254.4099578857422\n",
            "Epoch 136/250, Loss: 261.32794189453125, Test Loss: 254.4000244140625\n",
            "Epoch 137/250, Loss: 261.30853271484375, Test Loss: 254.39013671875\n",
            "Epoch 138/250, Loss: 261.28912353515625, Test Loss: 254.38023376464844\n",
            "Epoch 139/250, Loss: 261.2696228027344, Test Loss: 254.37034606933594\n",
            "Epoch 140/250, Loss: 261.2501220703125, Test Loss: 254.36045837402344\n",
            "Epoch 141/250, Loss: 261.2305908203125, Test Loss: 254.35057067871094\n",
            "Epoch 142/250, Loss: 261.2110290527344, Test Loss: 254.34072875976562\n",
            "Epoch 143/250, Loss: 261.1914367675781, Test Loss: 254.33087158203125\n",
            "Epoch 144/250, Loss: 261.17181396484375, Test Loss: 254.3209991455078\n",
            "Epoch 145/250, Loss: 261.15216064453125, Test Loss: 254.31118774414062\n",
            "Epoch 146/250, Loss: 261.1324462890625, Test Loss: 254.3013458251953\n",
            "Epoch 147/250, Loss: 261.1127014160156, Test Loss: 254.29153442382812\n",
            "Epoch 148/250, Loss: 261.0929260253906, Test Loss: 254.28172302246094\n",
            "Epoch 149/250, Loss: 261.0731506347656, Test Loss: 254.2719268798828\n",
            "Epoch 150/250, Loss: 261.05328369140625, Test Loss: 254.26214599609375\n",
            "Epoch 151/250, Loss: 261.03338623046875, Test Loss: 254.2523956298828\n",
            "Epoch 152/250, Loss: 261.01348876953125, Test Loss: 254.24261474609375\n",
            "Epoch 153/250, Loss: 260.9935607910156, Test Loss: 254.23284912109375\n",
            "Epoch 154/250, Loss: 260.97357177734375, Test Loss: 254.2230987548828\n",
            "Epoch 155/250, Loss: 260.95355224609375, Test Loss: 254.21336364746094\n",
            "Epoch 156/250, Loss: 260.9335021972656, Test Loss: 254.20367431640625\n",
            "Epoch 157/250, Loss: 260.9133605957031, Test Loss: 254.19395446777344\n",
            "Epoch 158/250, Loss: 260.8932800292969, Test Loss: 254.18423461914062\n",
            "Epoch 159/250, Loss: 260.8730773925781, Test Loss: 254.17457580566406\n",
            "Epoch 160/250, Loss: 260.8529052734375, Test Loss: 254.16493225097656\n",
            "Epoch 161/250, Loss: 260.8326110839844, Test Loss: 254.15525817871094\n",
            "Epoch 162/250, Loss: 260.8123474121094, Test Loss: 254.14561462402344\n",
            "Epoch 163/250, Loss: 260.7920227050781, Test Loss: 254.13600158691406\n",
            "Epoch 164/250, Loss: 260.77166748046875, Test Loss: 254.1263885498047\n",
            "Epoch 165/250, Loss: 260.7512512207031, Test Loss: 254.11676025390625\n",
            "Epoch 166/250, Loss: 260.73077392578125, Test Loss: 254.10719299316406\n",
            "Epoch 167/250, Loss: 260.7103271484375, Test Loss: 254.0976104736328\n",
            "Epoch 168/250, Loss: 260.6897888183594, Test Loss: 254.08804321289062\n",
            "Epoch 169/250, Loss: 260.6692199707031, Test Loss: 254.07855224609375\n",
            "Epoch 170/250, Loss: 260.6485900878906, Test Loss: 254.06900024414062\n",
            "Epoch 171/250, Loss: 260.6279296875, Test Loss: 254.05953979492188\n",
            "Epoch 172/250, Loss: 260.6072998046875, Test Loss: 254.05001831054688\n",
            "Epoch 173/250, Loss: 260.5865478515625, Test Loss: 254.04054260253906\n",
            "Epoch 174/250, Loss: 260.5657958984375, Test Loss: 254.03109741210938\n",
            "Epoch 175/250, Loss: 260.5449523925781, Test Loss: 254.0216522216797\n",
            "Epoch 176/250, Loss: 260.5241394042969, Test Loss: 254.01226806640625\n",
            "Epoch 177/250, Loss: 260.5032043457031, Test Loss: 254.00283813476562\n",
            "Epoch 178/250, Loss: 260.4822692871094, Test Loss: 253.99346923828125\n",
            "Epoch 179/250, Loss: 260.4612731933594, Test Loss: 253.9840850830078\n",
            "Epoch 180/250, Loss: 260.4402770996094, Test Loss: 253.9747314453125\n",
            "Epoch 181/250, Loss: 260.419189453125, Test Loss: 253.9654083251953\n",
            "Epoch 182/250, Loss: 260.3981018066406, Test Loss: 253.95611572265625\n",
            "Epoch 183/250, Loss: 260.37689208984375, Test Loss: 253.9468231201172\n",
            "Epoch 184/250, Loss: 260.355712890625, Test Loss: 253.9375762939453\n",
            "Epoch 185/250, Loss: 260.33447265625, Test Loss: 253.92832946777344\n",
            "Epoch 186/250, Loss: 260.31317138671875, Test Loss: 253.9191131591797\n",
            "Epoch 187/250, Loss: 260.2918395996094, Test Loss: 253.909912109375\n",
            "Epoch 188/250, Loss: 260.2704772949219, Test Loss: 253.9007110595703\n",
            "Epoch 189/250, Loss: 260.2490234375, Test Loss: 253.89157104492188\n",
            "Epoch 190/250, Loss: 260.2275390625, Test Loss: 253.8824462890625\n",
            "Epoch 191/250, Loss: 260.2060241699219, Test Loss: 253.87332153320312\n",
            "Epoch 192/250, Loss: 260.1844177246094, Test Loss: 253.86422729492188\n",
            "Epoch 193/250, Loss: 260.1628112792969, Test Loss: 253.8551788330078\n",
            "Epoch 194/250, Loss: 260.14117431640625, Test Loss: 253.84609985351562\n",
            "Epoch 195/250, Loss: 260.11944580078125, Test Loss: 253.8370819091797\n",
            "Epoch 196/250, Loss: 260.09765625, Test Loss: 253.82809448242188\n",
            "Epoch 197/250, Loss: 260.07586669921875, Test Loss: 253.81912231445312\n",
            "Epoch 198/250, Loss: 260.0539855957031, Test Loss: 253.8101806640625\n",
            "Epoch 199/250, Loss: 260.0321044921875, Test Loss: 253.8012237548828\n",
            "Epoch 200/250, Loss: 260.0101013183594, Test Loss: 253.79234313964844\n",
            "Epoch 201/250, Loss: 259.98809814453125, Test Loss: 253.78347778320312\n",
            "Epoch 202/250, Loss: 259.966064453125, Test Loss: 253.77462768554688\n",
            "Epoch 203/250, Loss: 259.9439392089844, Test Loss: 253.7658233642578\n",
            "Epoch 204/250, Loss: 259.9217529296875, Test Loss: 253.75706481933594\n",
            "Epoch 205/250, Loss: 259.8995361328125, Test Loss: 253.74827575683594\n",
            "Epoch 206/250, Loss: 259.8772888183594, Test Loss: 253.7395477294922\n",
            "Epoch 207/250, Loss: 259.8549499511719, Test Loss: 253.7308349609375\n",
            "Epoch 208/250, Loss: 259.8326110839844, Test Loss: 253.72216796875\n",
            "Epoch 209/250, Loss: 259.8101501464844, Test Loss: 253.71351623535156\n",
            "Epoch 210/250, Loss: 259.7876892089844, Test Loss: 253.7049102783203\n",
            "Epoch 211/250, Loss: 259.7651672363281, Test Loss: 253.69630432128906\n",
            "Epoch 212/250, Loss: 259.7425537109375, Test Loss: 253.687744140625\n",
            "Epoch 213/250, Loss: 259.71990966796875, Test Loss: 253.67921447753906\n",
            "Epoch 214/250, Loss: 259.69720458984375, Test Loss: 253.67071533203125\n",
            "Epoch 215/250, Loss: 259.6744689941406, Test Loss: 253.66226196289062\n",
            "Epoch 216/250, Loss: 259.6516418457031, Test Loss: 253.65377807617188\n",
            "Epoch 217/250, Loss: 259.6287536621094, Test Loss: 253.64540100097656\n",
            "Epoch 218/250, Loss: 259.6058654785156, Test Loss: 253.6370391845703\n",
            "Epoch 219/250, Loss: 259.5828857421875, Test Loss: 253.628662109375\n",
            "Epoch 220/250, Loss: 259.5598449707031, Test Loss: 253.6204071044922\n",
            "Epoch 221/250, Loss: 259.5367431640625, Test Loss: 253.61212158203125\n",
            "Epoch 222/250, Loss: 259.51361083984375, Test Loss: 253.60386657714844\n",
            "Epoch 223/250, Loss: 259.4903564453125, Test Loss: 253.5956573486328\n",
            "Epoch 224/250, Loss: 259.4670715332031, Test Loss: 253.58749389648438\n",
            "Epoch 225/250, Loss: 259.44378662109375, Test Loss: 253.57933044433594\n",
            "Epoch 226/250, Loss: 259.4203796386719, Test Loss: 253.57125854492188\n",
            "Epoch 227/250, Loss: 259.39691162109375, Test Loss: 253.56317138671875\n",
            "Epoch 228/250, Loss: 259.3734130859375, Test Loss: 253.5551300048828\n",
            "Epoch 229/250, Loss: 259.3498229980469, Test Loss: 253.54714965820312\n",
            "Epoch 230/250, Loss: 259.32623291015625, Test Loss: 253.5391845703125\n",
            "Epoch 231/250, Loss: 259.3025207519531, Test Loss: 253.53125\n",
            "Epoch 232/250, Loss: 259.27874755859375, Test Loss: 253.5233917236328\n",
            "Epoch 233/250, Loss: 259.25494384765625, Test Loss: 253.51553344726562\n",
            "Epoch 234/250, Loss: 259.2310485839844, Test Loss: 253.5077362060547\n",
            "Epoch 235/250, Loss: 259.20709228515625, Test Loss: 253.49996948242188\n",
            "Epoch 236/250, Loss: 259.1830749511719, Test Loss: 253.4922332763672\n",
            "Epoch 237/250, Loss: 259.15899658203125, Test Loss: 253.48452758789062\n",
            "Epoch 238/250, Loss: 259.13482666015625, Test Loss: 253.4768524169922\n",
            "Epoch 239/250, Loss: 259.1106262207031, Test Loss: 253.46926879882812\n",
            "Epoch 240/250, Loss: 259.0863952636719, Test Loss: 253.46168518066406\n",
            "Epoch 241/250, Loss: 259.06201171875, Test Loss: 253.45416259765625\n",
            "Epoch 242/250, Loss: 259.03759765625, Test Loss: 253.44664001464844\n",
            "Epoch 243/250, Loss: 259.0131530761719, Test Loss: 253.43922424316406\n",
            "Epoch 244/250, Loss: 258.98858642578125, Test Loss: 253.43179321289062\n",
            "Epoch 245/250, Loss: 258.9639892578125, Test Loss: 253.42445373535156\n",
            "Epoch 246/250, Loss: 258.93927001953125, Test Loss: 253.41709899902344\n",
            "Epoch 247/250, Loss: 258.9145202636719, Test Loss: 253.4098358154297\n",
            "Epoch 248/250, Loss: 258.88970947265625, Test Loss: 253.40257263183594\n",
            "Epoch 249/250, Loss: 258.86480712890625, Test Loss: 253.39541625976562\n",
            "Epoch 250/250, Loss: 258.83984375, Test Loss: 253.38824462890625\n",
            "Final MSE: 253.3882293701172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to a DataFrame for analysis\n",
        "results_df = pd.DataFrame(results, columns=['Hidden Layers', 'Activation Function', 'Learning Rate', 'Batch Size', 'Epochs', 'MSE'])\n",
        "results_df.to_csv('mlp_regressor_results.csv', index=False)\n",
        "print(results_df.sort_values(by='MSE'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFZN3PRQZ72Y",
        "outputId": "1290275f-09f5-41d9-aaa9-3983ca527281"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Hidden Layers Activation Function  Learning Rate  Batch Size  Epochs  \\\n",
            "928            [4]              linear          1.000         256     100   \n",
            "2044        [8, 4]              linear          0.100         256     100   \n",
            "2099        [8, 4]              linear          0.001          32     250   \n",
            "2002        [8, 4]              linear          1.000         128     100   \n",
            "929            [4]              linear          1.000         256     250   \n",
            "...            ...                 ...            ...         ...     ...   \n",
            "3056    [16, 8, 4]              linear         10.000         512      25   \n",
            "3049    [16, 8, 4]              linear         10.000         256      10   \n",
            "3030    [16, 8, 4]              linear         10.000          32       1   \n",
            "3024    [16, 8, 4]              linear         10.000          16       1   \n",
            "3042    [16, 8, 4]              linear         10.000         128       1   \n",
            "\n",
            "               MSE  \n",
            "928   8.620731e-01  \n",
            "2044  8.698234e-01  \n",
            "2099  8.779467e-01  \n",
            "2002  8.836894e-01  \n",
            "929   8.898076e-01  \n",
            "...            ...  \n",
            "3056  3.040212e+13  \n",
            "3049  4.917704e+13  \n",
            "3030  3.733139e+14  \n",
            "3024  6.036560e+14  \n",
            "3042  1.000988e+15  \n",
            "\n",
            "[3240 rows x 6 columns]\n"
          ]
        }
      ]
    }
  ]
}