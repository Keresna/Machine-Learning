{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-_giRuRk5KW",
        "outputId": "bfa9a378-39b0-4083-c57c-33c9932f08d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "xkn7h8QVkYIJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZeCB_AfkJu1",
        "outputId": "3ad1cb6c-9c2e-4603-85d2-aa25bb6414c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Sizes: [4], Activation: linear, Epochs: 10, Learning Rate: 0.01, Batch Size: 16, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 25, Learning Rate: 0.1, Batch Size: 256, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 50, Learning Rate: 10, Batch Size: 512, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 50, Learning Rate: 0.01, Batch Size: 32, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 50, Learning Rate: 0.01, Batch Size: 128, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 100, Learning Rate: 10, Batch Size: 512, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 100, Learning Rate: 1, Batch Size: 512, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 100, Learning Rate: 0.1, Batch Size: 32, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 100, Learning Rate: 0.01, Batch Size: 128, Accuracy: 1.0000\n",
            "Hidden Sizes: [4], Activation: linear, Epochs: 100, Learning Rate: 0.01, Batch Size: 512, Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Generate a dummy dataset\n",
        "# Explanation: This dataset consists of binary classification data with two features (X) and two labels (y).\n",
        "def create_dummy_dataset():\n",
        "    np.random.seed(42)\n",
        "    X = np.random.rand(1000, 2)  # 1000 samples, 2 features\n",
        "    y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Label is 1 if the sum of features > 1, otherwise 0\n",
        "    return X, y\n",
        "\n",
        "X, y = create_dummy_dataset()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Wrap data into a DataLoader\n",
        "# Explanation: DataLoader allows us to split the dataset into batches for efficient training.\n",
        "def create_dataloader(X, y, batch_size):\n",
        "    dataset = TensorDataset(X, y)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the MLP model\n",
        "# Explanation: This is a modular implementation of a Multi-Layer Perceptron (MLP) with configurable layers and activation functions.\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation_fn):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        current_input_size = input_size\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(current_input_size, hidden_size))\n",
        "            if activation_fn == 'linear':\n",
        "                layers.append(nn.Identity())\n",
        "            elif activation_fn == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "            elif activation_fn == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation_fn == 'softmax':\n",
        "                layers.append(nn.Softmax(dim=1))\n",
        "            elif activation_fn == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            current_input_size = hidden_size\n",
        "        layers.append(nn.Linear(current_input_size, output_size))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Experiment configurations\n",
        "hidden_layer_configs = [[4], [8, 8], [16, 16, 16]]\n",
        "activation_functions = ['linear', 'sigmoid', 'relu', 'softmax', 'tanh']\n",
        "epochs_list = [1, 10, 25, 50, 100, 250]\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
        "\n",
        "# Training function\n",
        "# Explanation: This function trains the model and evaluates its performance on the test set.\n",
        "def train_and_evaluate(hidden_sizes, activation_fn, epochs, learning_rate, batch_size):\n",
        "    train_loader = create_dataloader(X_train_tensor, y_train_tensor, batch_size)\n",
        "    test_loader = create_dataloader(X_test_tensor, y_test_tensor, batch_size)\n",
        "\n",
        "    model = MLP(input_size=2, hidden_sizes=hidden_sizes, output_size=2, activation_fn=activation_fn)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_pred.extend(preds.numpy())\n",
        "            y_true.extend(batch_y.numpy())\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Run experiments and collect results\n",
        "# Explanation: We iterate over all configurations and store the results for comparison.\n",
        "results = []\n",
        "for hidden_sizes in hidden_layer_configs:\n",
        "    for activation_fn in activation_functions:\n",
        "        for epochs in epochs_list:\n",
        "            for learning_rate in learning_rates:\n",
        "                for batch_size in batch_sizes:\n",
        "                    acc = train_and_evaluate(hidden_sizes, activation_fn, epochs, learning_rate, batch_size)\n",
        "                    results.append((hidden_sizes, activation_fn, epochs, learning_rate, batch_size, acc))\n",
        "\n",
        "# Display results\n",
        "# Explanation: Sort and display the configurations and their respective accuracies.\n",
        "results.sort(key=lambda x: x[-1], reverse=True)\n",
        "for res in results[:10]:  # Display top 10 configurations\n",
        "    print(f\"Hidden Sizes: {res[0]}, Activation: {res[1]}, Epochs: {res[2]}, Learning Rate: {res[3]}, Batch Size: {res[4]}, Accuracy: {res[5]:.4f}\")\n"
      ]
    }
  ]
}