{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hnbJxwqyxGKq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_MBf9DIxR-o",
        "outputId": "db2d270e-bc60-4af0-e4e2-32fd6db876e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "# Dataset berupa \"heart.csv\", pastikan file ada di direktori kerja\n",
        "# Dataset ini mengandung informasi mengenai penyakit jantung (biner klasifikasi)\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Dataset/heart.csv\")\n",
        "\n",
        "# Pisahkan fitur (X) dan label (y)\n",
        "X = data.drop(columns=[\"target\"])\n",
        "y = data[\"target\"]\n",
        "\n",
        "# Bagi dataset menjadi data latih dan data uji (80% latih, 20% uji)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Normalisasi data untuk memastikan semua fitur memiliki skala yang sama\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Konversi data ke bentuk tensor PyTorch\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Fungsi untuk membangun MLP model\n",
        "def create_mlp(input_size, hidden_layers, activation_fn):\n",
        "    layers = []\n",
        "    # Tambahkan lapisan input -> hidden pertama\n",
        "    prev_size = input_size\n",
        "    for hidden_size in hidden_layers:\n",
        "        layers.append(nn.Linear(prev_size, hidden_size))\n",
        "        layers.append(activation_fn)\n",
        "        prev_size = hidden_size\n",
        "    # Tambahkan lapisan output (1 unit untuk klasifikasi biner)\n",
        "    layers.append(nn.Linear(prev_size, 2)) # 2 kelas output\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Fungsi untuk pelatihan dan evaluasi model\n",
        "def train_and_evaluate(model, optimizer, criterion, X_train, y_train, X_test, y_test, epochs, batch_size):\n",
        "    # Gunakan DataLoader untuk batch training\n",
        "    dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model ke mode pelatihan\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            optimizer.zero_grad()  # Reset gradien\n",
        "            predictions = model(batch_X)  # Prediksi output\n",
        "            loss = criterion(predictions, batch_y)  # Hitung loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update bobot\n",
        "\n",
        "    # Evaluasi model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test)\n",
        "        y_pred_classes = torch.argmax(y_pred, dim=1)\n",
        "        accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    return accuracy\n",
        "\n",
        "# Hyperparameter yang ingin diuji\n",
        "hidden_layers_list = [[4], [8, 4], [16, 8, 4]]  # Kombinasi hidden layer\n",
        "activation_functions = [nn.Identity(), nn.Sigmoid(), nn.ReLU(), nn.Tanh()]\n",
        "epochs_list = [1, 10, 25, 50, 100, 250]\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "results = []\n",
        "\n",
        "# Lakukan eksperimen dengan semua kombinasi\n",
        "for hidden_layers in hidden_layers_list:\n",
        "    for activation_fn in activation_functions:\n",
        "        for epochs in epochs_list:\n",
        "            for lr in learning_rates:\n",
        "                for batch_size in batch_sizes:\n",
        "                    # Inisialisasi model, optimizer, dan loss function\n",
        "                    model = create_mlp(input_size, hidden_layers, activation_fn)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    # Latih dan evaluasi model\n",
        "                    accuracy = train_and_evaluate(model, optimizer, criterion, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, epochs, batch_size)\n",
        "\n",
        "                    # Simpan hasil\n",
        "                    results.append({\n",
        "                        \"hidden_layers\": hidden_layers,\n",
        "                        \"activation_function\": activation_fn.__class__.__name__,\n",
        "                        \"epochs\": epochs,\n",
        "                        \"learning_rate\": lr,\n",
        "                        \"batch_size\": batch_size,\n",
        "                        \"accuracy\": accuracy\n",
        "                    })\n",
        "\n",
        "# Simpan hasil ke dalam DataFrame untuk analisis lebih lanjut\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.sort_values(by=\"accuracy\", ascending=False).head(10))\n",
        "\n",
        "# Simpan hasil ke file CSV\n",
        "results_df.to_csv(\"mlp_experiments_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uai--RRtxkxA",
        "outputId": "d88b238e-7c01-4b18-9cfa-916c226fce69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     hidden_layers activation_function  epochs  learning_rate  batch_size  \\\n",
            "2364    [16, 8, 4]                ReLU     250          0.001          16   \n",
            "2366    [16, 8, 4]                ReLU     250          0.001          64   \n",
            "2359    [16, 8, 4]                ReLU     250          0.010          32   \n",
            "1279        [8, 4]             Sigmoid     250          0.010          32   \n",
            "2361    [16, 8, 4]                ReLU     250          0.010         128   \n",
            "2362    [16, 8, 4]                ReLU     250          0.010         256   \n",
            "2363    [16, 8, 4]                ReLU     250          0.010         512   \n",
            "2502    [16, 8, 4]                Tanh      50          0.010          16   \n",
            "2365    [16, 8, 4]                ReLU     250          0.001          32   \n",
            "2574    [16, 8, 4]                Tanh     250          0.010          16   \n",
            "\n",
            "      accuracy  \n",
            "2364       1.0  \n",
            "2366       1.0  \n",
            "2359       1.0  \n",
            "1279       1.0  \n",
            "2361       1.0  \n",
            "2362       1.0  \n",
            "2363       1.0  \n",
            "2502       1.0  \n",
            "2365       1.0  \n",
            "2574       1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJtcrcr9MKj5",
        "outputId": "bb5ffa0c-cc49-4063-c694-b8060d23328a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('/content/drive/MyDrive/Dataset/mlp_experiments_results.csv', index=False)"
      ],
      "metadata": {
        "id": "OxGbO8ulMOEJ"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}